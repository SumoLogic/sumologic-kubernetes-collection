---
# Source: sumologic/templates/metrics/collector/otelcol/opentelemetrycollector.yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: RELEASE-NAME-sumologic-metrics
  namespace: sumologic
  labels:
    sumologic.com/app: otelcol
    sumologic.com/component: metrics
    chart: "sumologic-%CURRENT_CHART_VERSION%"
    release: "RELEASE-NAME"
    heritage: "Helm"
    sumologic.com/scrape: "true"

    podLabelKey: podLabelValue

    podKey: podValue
  annotations:
    podAnnotationKey: podAnnotationValue

    annotationKey: annotationValue
spec:
  mode: statefulset
  replicas: 3
  serviceAccount: RELEASE-NAME-sumologic-metrics
  targetAllocator:
    serviceAccount: RELEASE-NAME-sumologic-metrics-targetallocator
    enabled: true
    prometheusCR:
      enabled: true
      serviceMonitorSelector:
        smkey: smvalue
      podMonitorSelector:
        pmkey: pmvalue
  nodeSelector:
    workingGroup: production
  tolerations:
    - effect: NoSchedule
      key: null
      operator: Exists
  priorityClassName: "customPriority"
  autoscaler:
    maxReplicas: 30
    minReplicas: 15
    targetCPUUtilization: 95
    targetMemoryUtilization: 90
  env:
    - name: METADATA_METRICS_SVC
      value: RELEASE-NAME-sumologic-metadata-metrics # no need for remote write proxy here
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
  podSecurityContext:
    fsGroup: 999
  ports:
    - name: pprof
      port: 1777
  resources:
    limits:
      cpu: 3000m
      memory: 2Gi
    requests:
      cpu: 1000m
      memory: 1Gi
  volumes:
    - name: tmp
      emptyDir: {}
    - name: file-storage
      emptyDir: {}
  volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: file-storage
      mountPath: /var/lib/storage/otc
  config: |
    exporters:
      otlphttp:
        endpoint: http://${METADATA_METRICS_SVC}.${NAMESPACE}.svc.cluster.local.:4318
        sending_queue:
          queue_size: 10000
          num_consumers: 10
          storage: file_storage

    extensions:
      health_check: {}
      pprof: {}
      file_storage:
        directory: /var/lib/storage/otc
        timeout: 10s
        compaction:
          on_rebound: true
          directory: /tmp

    # TODO: Delete this once we upgrade operator past https://github.com/open-telemetry/opentelemetry-operator/issues/958
    processors:
      transform/drop_id_name:
        error_mode: ignore
        metric_statements:
          - context: datapoint
            statements:
              - delete_key(attributes, "id")
              - delete_key(attributes, "name")

    receivers:
      prometheus:
        config:
          global:
            scrape_interval: 30s
          scrape_configs:
            ## These scrape configs are for kubelet metrics
            ## Prometheus operator does this by manually maintaining a Service with Endpoints for all Nodes
            ## We don't have that capability, so we need to use a static configuration
            - job_name: kubelet
              scheme: https
              authorization:
                credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: true
              honor_labels: true
              kubernetes_sd_configs:
                - role: node
              metric_relabel_configs:
                - action: keep
                  regex: (?:kubelet_docker_operations_errors(?:|_total)|kubelet_(?:docker|runtime)_operations_duration_seconds_(?:count|sum)|kubelet_running_(?:container|pod)(?:_count|s)|kubelet_(:?docker|runtime)_operations_latency_microseconds(?:|_count|_sum))
                  source_labels: [__name__]
                # TODO: The below can't be used due to https://github.com/open-telemetry/opentelemetry-operator/issues/958
                # - action: labeldrop
                #   regex: id
              relabel_configs: &relabel_configs # partially copied from what operator generates
                - source_labels:
                  - __meta_kubernetes_node_name
                  target_label: node
                - source_labels:
                  - __meta_kubernetes_namespace
                  target_label: namespace
                - source_labels:
                  - __meta_kubernetes_pod_name
                  target_label: pod
                - source_labels:
                  - __meta_kubernetes_pod_container_name
                  target_label: container
                - target_label: endpoint
                  replacement: https-metrics
                - source_labels:
                  - __metrics_path__
                  target_label: metrics_path
                  action: replace
                - source_labels:
                  - __address__
                  target_label: instance
                  action: replace
            - job_name: cadvisor
              scheme: https
              authorization:
                credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
              tls_config:
                ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecure_skip_verify: true
              honor_labels: true
              metrics_path: /metrics/cadvisor
              kubernetes_sd_configs:
                - role: node
              metric_relabel_configs:
                - action: replace
                  regex: .*
                  replacement: kubelet
                  source_labels: [__name__]
                  target_label: job
                - action: keep
                  regex: (?:container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_fs_usage_bytes|container_fs_limit_bytes|container_cpu_cfs_throttled_seconds_total|container_network_receive_bytes_total|container_network_transmit_bytes_total)
                  source_labels: [__name__]
                ## Drop container metrics with container tag set to an empty string:
                ## these are the pod aggregated container metrics which can be aggregated
                ## in Sumo anyway. There's also some cgroup-specific time series we also
                ## do not need.
                - action: drop
                  source_labels: [__name__, container]
                  regex: (?:container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_fs_usage_bytes|container_fs_limit_bytes);$
                - action: labelmap
                  regex: container_name
                  replacement: container
                - action: drop
                  source_labels: [container]
                  regex: POD
                # TODO: The below can't be used due to https://github.com/open-telemetry/opentelemetry-operator/issues/958
                # - action: labeldrop
                #   regex: (id|name)
              relabel_configs: *relabel_configs  # partially copied from what operator generates
        target_allocator:
          endpoint: http://RELEASE-NAME-sumologic-metrics-targetallocator
          interval: 30s
          collector_id: ${POD_NAME}

    service:
      telemetry:
        logs:
          level: info
        metrics:
          address: 0.0.0.0:8888 # this is the default, but setting it explicitly lets the operator add it automatically
      extensions:
        - health_check
        - pprof
        - file_storage
      pipelines:
        metrics:
          exporters: [otlphttp]
          processors: [transform/drop_id_name]
          receivers: [prometheus]
