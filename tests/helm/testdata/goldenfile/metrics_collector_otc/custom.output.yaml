---
# Source: sumologic/templates/metrics/collector/otelcol/opentelemetrycollector.yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: RELEASE-NAME-sumologic-metrics
  namespace: sumologic
  labels:
    sumologic.com/app: otelcol
    sumologic.com/component: metrics
    chart: "sumologic-%CURRENT_CHART_VERSION%"
    release: "RELEASE-NAME"
    heritage: "Helm"
    sumologic.com/scrape: "true"

    podLabelKey: podLabelValue

    podKey: podValue
  annotations:
    podAnnotationKey: podAnnotationValue

    annotationKey: annotationValue
spec:
  mode: statefulset
  replicas: 3
  serviceAccount: RELEASE-NAME-sumologic-metrics
  targetAllocator:
    serviceAccount: RELEASE-NAME-sumologic-metrics-targetallocator
    enabled: true
    prometheusCR:
      enabled: true
      serviceMonitorSelector:
        smkey: smvalue
      podMonitorSelector:
        pmkey: pmvalue
  nodeSelector:
    workingGroup: production
  tolerations:
    - effect: NoSchedule
      key: null
      operator: Exists
  priorityClassName: "customPriority"
  autoscaler:
    maxReplicas: 30
    minReplicas: 15
    targetCPUUtilization: 95
    targetMemoryUtilization: 90
  env:
    - name: METADATA_METRICS_SVC
      value: RELEASE-NAME-sumologic-metadata-metrics # no need for remote write proxy here
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
  podSecurityContext:
    fsGroup: 999
  ports:
    - name: pprof
      port: 1777
  resources:
    limits:
      cpu: 3000m
      memory: 2Gi
    requests:
      cpu: 1000m
      memory: 1Gi
  volumes:
    - name: tmp
      emptyDir: {}
    - name: file-storage
      emptyDir: {}
  volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: file-storage
      mountPath: /var/lib/storage/otc
  config: |
    exporters:
      otlphttp:
        endpoint: http://${METADATA_METRICS_SVC}.${NAMESPACE}.svc.cluster.local.:4318
        compression: zstd
        sending_queue:
          queue_size: 10000
          num_consumers: 10
          storage: file_storage

    extensions:
      health_check: {}
      pprof: {}
      file_storage:
        directory: /var/lib/storage/otc
        timeout: 10s
        compaction:
          on_rebound: true
          directory: /tmp


    processors:
      transform/drop_unnecessary_attributes:
        error_mode: ignore
        metric_statements:
          - context: resource
            statements:
              - delete_key(attributes, "http.scheme")
              - delete_key(attributes, "net.host.name")
              - delete_key(attributes, "net.host.port")
              - delete_key(attributes, "service.instance.id")
              # prometheus receiver adds these automatically
              # we drop them to make the rest of our pipeline easier to reason about
              # after the collector and metadata are merged, consider using them instead of k8sattributes processor
              - delete_matching_keys(attributes, "k8s.*")

      filter/drop_unnecessary_metrics:
        error_mode: ignore
        metrics:
          metric:
            # we let the metrics from annotations ("kubernetes-pods") through as they are
            - resource.attributes["service.name"] != "pod-annotations" and IsMatch(name, "scrape_.*")

    receivers:
      prometheus:
        config:
          global:
            scrape_interval: 30s
          scrape_configs:
            ## scraping metrics basing on annotations:
            ##   - prometheus.io/scrape: true - to scrape metrics from the pod
            ##   - prometheus.io/path: /metrics - path which the metric should be scrape from
            ##   - prometheus.io/port: 9113 - port which the metric should be scrape from
            ## rel: https://github.com/prometheus-operator/kube-prometheus/pull/16#issuecomment-424318647
            - job_name: "pod-annotations"
              kubernetes_sd_configs:
                - role: pod
              relabel_configs:
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                  action: keep
                  regex: true
                - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                  action: replace
                  target_label: __metrics_path__
                  regex: (.+)
                - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                  action: replace
                  regex: ([^:]+)(?::\d+)?;(\d+)
                  replacement: $1:$2
                  target_label: __address__
                - source_labels: [__metrics_path__]
                  separator: ;
                  regex: (.*)
                  target_label: endpoint
                  replacement: $1
                  action: replace
                - source_labels: [__meta_kubernetes_namespace]
                  action: replace
                  target_label: namespace
                - action: labelmap
                  regex: __meta_kubernetes_pod_label_(.+)
                - source_labels: [__meta_kubernetes_pod_name]
                  separator: ;
                  regex: (.*)
                  target_label: pod
                  replacement: $1
                  action: replace
        target_allocator:
          endpoint: http://RELEASE-NAME-sumologic-metrics-targetallocator
          interval: 30s
          collector_id: ${POD_NAME}

    service:
      telemetry:
        logs:
          level: info
        metrics:
          address: 0.0.0.0:8888 # this is the default, but setting it explicitly lets the operator add it automatically
      extensions:
        - health_check
        - pprof
        - file_storage
      pipelines:
        metrics:
          exporters: [otlphttp]
          processors:
            - filter/drop_unnecessary_metrics
            - transform/drop_unnecessary_attributes
          receivers: [prometheus]
