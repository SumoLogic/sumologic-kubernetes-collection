---
# Source: sumologic/templates/metrics/collector/otelcol/opentelemetrycollector.yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: RELEASE-NAME-sumologic-metrics
  namespace: sumologic
  labels:
    sumologic.com/app: otelcol
    sumologic.com/component: metrics
    chart: "sumologic-%CURRENT_CHART_VERSION%"
    release: "RELEASE-NAME"
    heritage: "Helm"
    sumologic.com/scrape: "true"

    podLabelKey: podLabelValue

    podKey: podValue
spec:
  image: "public.ecr.aws/sumologic/sumologic-otel-collector:0.85.0-sumo-0"
  mode: statefulset
  replicas: 1
  serviceAccount: RELEASE-NAME-sumologic-metrics
  targetAllocator:
    allocationStrategy: consistent-hashing
    serviceAccount: RELEASE-NAME-sumologic-metrics-targetallocator
    enabled: true
    filterStrategy: relabel-config
    prometheusCR:
      enabled: true
      scrapeInterval: 60s
      serviceMonitorSelector:
        smkey: smvalue
      podMonitorSelector:
        pmkey: pmvalue
  nodeSelector:
    workingGroup: production
  tolerations:
    - effect: NoSchedule
      key: null
      operator: Exists
  priorityClassName: "customPriority"
  autoscaler:
    maxReplicas: 30
    minReplicas: 15
    targetCPUUtilization: 95
    targetMemoryUtilization: 90
  env:
    - name: METADATA_METRICS_SVC
      valueFrom:
        configMapKeyRef:
          name: sumologic-configmap
          key: metadataMetrics
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
  podAnnotations:
    ## The operator adds this annotation by default, but we use our own ServiceMonitor
    prometheus.io/scrape: "false"
    podAnnotationKey: podAnnotationValue
    annotationKey: annotationValue
  podSecurityContext:
    fsGroup: 999
  ports:
    - name: pprof
      port: 1777
  resources:
    limits:
      cpu: 3000m
      memory: 2Gi
    requests:
      cpu: 1000m
      memory: 1Gi
  volumes:
    - name: tmp
      emptyDir: {}
    - name: file-storage
      emptyDir: {}
  volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: file-storage
      mountPath: /var/lib/storage/otc
  config: |
    exporters:
      otlphttp:
        endpoint: http://${METADATA_METRICS_SVC}.${NAMESPACE}.svc.cluster.local.:4318
        sending_queue:
          queue_size: 10000
          num_consumers: 10
          storage: file_storage
        # this improves load balancing at the cost of more network traffic
        disable_keep_alives: true

    extensions:
      health_check: {}
      pprof: {}
      file_storage:
        directory: /var/lib/storage/otc
        timeout: 10s
        compaction:
          on_rebound: true
          directory: /tmp


    processors:
      batch:
        send_batch_max_size: 2000
        send_batch_size: 1000
        timeout: 1s

      transform/drop_unnecessary_attributes:
        error_mode: ignore
        metric_statements:
          - context: resource
            statements:
              - delete_key(attributes, "http.scheme")
              - delete_key(attributes, "net.host.name")
              - delete_key(attributes, "net.host.port")
              - delete_key(attributes, "service.instance.id")
              # prometheus receiver adds these automatically
              # we drop them to make the rest of our pipeline easier to reason about
              # after the collector and metadata are merged, consider using them instead of k8sattributes processor
              - delete_matching_keys(attributes, "k8s.*")
      filter/drop_unnecessary_metrics:
        error_mode: ignore
        metrics:
          metric:
            # we let the metrics from annotations ("kubernetes-pods") through as they are
            - resource.attributes["service.name"] != "pod-annotations" and IsMatch(name, "scrape_.*")

    receivers:
      prometheus:
        config:
          global:
            scrape_interval: 60s
          scrape_configs: []
        target_allocator:
          endpoint: http://RELEASE-NAME-sumologic-metrics-targetallocator
          interval: 30s
          collector_id: ${POD_NAME}

    service:
      telemetry:
        logs:
          level: info
        metrics:
          address: 0.0.0.0:8888 # this is the default, but setting it explicitly lets the operator add it automatically
      extensions:
        - health_check
        - pprof
        - file_storage
      pipelines:
        metrics:
          exporters: [otlphttp]
          processors:
            - batch
            - filter/drop_unnecessary_metrics
            - transform/drop_unnecessary_attributes
          receivers: [prometheus]
