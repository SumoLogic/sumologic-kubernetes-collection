---
# Source: sumologic/templates/events/fluentd/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-sumologic-fluentd-events
  labels:
    app: RELEASE-NAME-sumologic-fluentd-events
    chart: "sumologic-%CURRENT_CHART_VERSION%"
    release: "RELEASE-NAME"
    heritage: "Helm"
data:
  fluent.conf: |-
    @include events.conf
  events.conf: |
    <source>
      @type events
      deploy_namespace sumologic
      kubernetes_url "https://kubernetes.default.svc:443"
    </source>
    # Prevent fluentd from handling records containing its own logs.
    <match fluentd.**>
      @type null
    </match>
    # Set `cluster` metadata field
    <filter **>
      @type record_modifier
      <record>
        _sumo_metadata ${ {:fields => "cluster=kubernetes"} }
      </record>
    </filter>
    # expose the Fluentd metrics to Prometheus
    <source>
      @type prometheus
      metrics_path /metrics
      port 24231
    </source>
    <source>
      @type prometheus_output_monitor
    </source>
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>
    <match kubernetes.**>
      @type copy
      <store>
        @type sumologic
        @id sumologic.endpoint.events
        sumo_client "k8s_%CURRENT_CHART_VERSION%"
        endpoint "#{ENV['SUMO_ENDPOINT_DEFAULT_EVENTS_SOURCE']}"
        source_name events
        source_category kubernetes/events
        data_type logs
        disable_cookies true
        verify_ssl "true"
        proxy_uri ""
        compress "true"
        compress_encoding "gzip"
        <buffer>
          @type file
          path /var/lib/storage/events
          @include buffer.output.conf
        </buffer>
      </store>
    </match>
    <system>
      log_level info
    </system>
  buffer.output.conf: |
    compress "gzip"
    flush_interval "5s"
    flush_thread_count "8"
    chunk_limit_size "1m"
    total_limit_size "128m"
    queued_chunks_limit_size "128"
    overflow_action drop_oldest_chunk
    retry_max_interval "10m"
    retry_forever "true"
