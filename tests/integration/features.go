package integration

import (
	"context"
	"fmt"
	"strings"
	"testing"
	"time"

	corev1 "k8s.io/api/core/v1"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	log "k8s.io/klog/v2"
	"sigs.k8s.io/e2e-framework/klient/k8s"
	"sigs.k8s.io/e2e-framework/klient/k8s/resources"
	"sigs.k8s.io/e2e-framework/klient/wait"
	"sigs.k8s.io/e2e-framework/klient/wait/conditions"
	"sigs.k8s.io/e2e-framework/pkg/envconf"
	"sigs.k8s.io/e2e-framework/pkg/features"

	terrak8s "github.com/gruntwork-io/terratest/modules/k8s"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/SumoLogic/sumologic-kubernetes-collection/tests/integration/internal"
	"github.com/SumoLogic/sumologic-kubernetes-collection/tests/integration/internal/ctxopts"
	"github.com/SumoLogic/sumologic-kubernetes-collection/tests/integration/internal/stepfuncs"
	strings_internal "github.com/SumoLogic/sumologic-kubernetes-collection/tests/integration/internal/strings"
	"github.com/SumoLogic/sumologic-kubernetes-collection/tests/integration/internal/sumologicmock"
)

type MetricsCollector string

const (
	tickDuration             = 3 * time.Second
	waitDuration             = 2 * time.Minute
	waitBeforeLogsGeneration = 5 * time.Second
	// number determined experimentally
	expectedEventCount uint = 50
	logsGeneratorCount uint = 1000
	// number of log records in single loop with default multiline support only, see: tests/integration/yamls/pod_multiline_long_lines.yaml
	logRecords = 4 + 10
	// number of log records in single loop with multiple multilines support, see: tests/integration/yamls/pod_multiline_long_lines.yaml
	multipleLogRecords = 4 + 4
	// number of loops in which logs are generated, see: tests/integration/yamls/pod_multiline_long_lines.yaml
	logLoops                       = 500
	multilineLogCount         uint = logRecords * logLoops
	multipleMultilineLogCount uint = multipleLogRecords * logLoops
	// number of traces generated per exporter
	tracesPerExporter uint             = 5
	spansPerTrace     uint             = 2
	Prometheus        MetricsCollector = "prometheus"
	Otelcol           MetricsCollector = "otelcol"
	// number of logs generated by tailing sidecar test (50 * 3), see: tests/inegration/yamls/tailing-sidecar-test.yaml
	TailingSidecarCount uint = 150
	// number of logs generated by pod annotations test (50 * 3), see: tests/inegration/yamls/annotations-test.yaml
	AnnotationsLogsCount uint = 150
	// number of logs generated by containers used in namespace annotations test (50 * 3), see: tests/inegration/yamls/namespace-annotations-test.yaml
	NamespaceAnnotationsLogsCount uint = 150
	curlAppMaxWaitTime            uint = 180
	curlAppSleepInterval          uint = 5
)

func GetMetricsK8sattributes(expectedMetrics []string, metricsCollector MetricsCollector) features.Feature {
	return features.New("metrics").
		Assess("expected metrics are present",
			stepfuncs.WaitUntilExpectedMetricsPresent(
				expectedMetrics,
				2*time.Minute, // take longer to account for recording rule metrics
				tickDuration,
			),
		).
		Assess("expected labels are present for container metrics",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				// Get the receiver mock pod as metrics source
				res := envConf.Client().Resources(ctxopts.Namespace(ctx))
				nodeList := corev1.NodeList{}
				podList := corev1.PodList{}
				releaseName := strings_internal.ReleaseNameFromT(t)
				deployment := fmt.Sprintf("%s-sumologic-mock", releaseName)
				require.NoError(t,
					wait.For(
						conditions.New(res).
							ResourceListN(
								&podList,
								1,
								resources.WithLabelSelector(fmt.Sprintf("app=%s", deployment)),
							),
						wait.WithTimeout(waitDuration),
						wait.WithInterval(tickDuration),
					),
				)
				require.NoError(t,
					wait.For(
						conditions.New(res).
							ResourceListN(
								&nodeList,
								1,
								resources.WithLabelSelector(fmt.Sprintf("app=%s", deployment)),
							),
						wait.WithTimeout(waitDuration),
						wait.WithInterval(tickDuration),
					),
				)
				metricFilters := sumologicmock.MetadataFilters{
					"__name__": "container_memory_working_set_bytes",
					"pod":      podList.Items[0].Name,
				}
				namespace := ctxopts.Namespace(ctx)
				expectedLabels := sumologicmock.Labels{
					"cluster":                      "kubernetes",
					"_origin":                      "kubernetes",
					"container":                    "sumologic-mock",
					"deployment":                   deployment,
					"endpoint":                     "https-metrics",
					"image":                        "sumologic/sumologic-mock:.*",
					"job":                          "kubelet",
					"metrics_path":                 "/metrics/cadvisor",
					"namespace":                    ctxopts.Namespace(ctx),
					"node":                         internal.NodeNameRegex,
					"pod_labels_app":               deployment,
					"pod_labels_pod-template-hash": ".+",
					"pod":                          podList.Items[0].Name,
					"replicaset":                   fmt.Sprintf("%s-.*", deployment),
					"service":                      deployment,
					"service.namespace":            ctxopts.Namespace(ctx),
				}
				expectedLabels = addCollectorSpecificMetricLabels(expectedLabels, releaseName, namespace, metricsCollector)

				return stepfuncs.WaitUntilExpectedMetricLabelsPresent(metricFilters, expectedLabels, waitDuration, tickDuration)(ctx, t, envConf)
			},
		).
		Feature()
}

func GetMetricsFeature(expectedMetrics []string, metricsCollector MetricsCollector) features.Feature {
	return features.New("metrics").
		Assess("expected metrics are present",
			stepfuncs.WaitUntilExpectedMetricsPresent(
				expectedMetrics,
				2*time.Minute, // take longer to account for recording rule metrics
				tickDuration,
			),
		).
		Assess("expected labels are present for container metrics",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				// Get the receiver mock pod as metrics source
				res := envConf.Client().Resources(ctxopts.Namespace(ctx))
				podList := corev1.PodList{}
				releaseName := strings_internal.ReleaseNameFromT(t)
				deployment := fmt.Sprintf("%s-sumologic-mock", releaseName)
				require.NoError(t,
					wait.For(
						conditions.New(res).
							ResourceListN(
								&podList,
								1,
								resources.WithLabelSelector(fmt.Sprintf("app=%s", deployment)),
							),
						wait.WithTimeout(waitDuration),
						wait.WithInterval(tickDuration),
					),
				)
				metricFilters := sumologicmock.MetadataFilters{
					"__name__": "container_memory_working_set_bytes",
					"pod":      podList.Items[0].Name,
				}
				namespace := ctxopts.Namespace(ctx)
				expectedLabels := sumologicmock.Labels{
					"cluster":                      "kubernetes",
					"_origin":                      "kubernetes",
					"container":                    "sumologic-mock",
					"deployment":                   deployment,
					"endpoint":                     "https-metrics",
					"image":                        "sumologic/sumologic-mock:.*",
					"job":                          "kubelet",
					"metrics_path":                 "/metrics/cadvisor",
					"namespace":                    ctxopts.Namespace(ctx),
					"node":                         internal.NodeNameRegex,
					"pod_labels_app":               deployment,
					"pod_labels_pod-template-hash": ".+",
					"pod":                          podList.Items[0].Name,
					"replicaset":                   fmt.Sprintf("%s-.*", deployment),
					"service":                      deployment,
				}
				expectedLabels = addCollectorSpecificMetricLabels(expectedLabels, releaseName, namespace, metricsCollector)

				return stepfuncs.WaitUntilExpectedMetricLabelsPresent(metricFilters, expectedLabels, waitDuration, tickDuration)(ctx, t, envConf)
			},
		).
		Assess("expected labels are present for non-pod kube-state-metrics",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				releaseName := strings_internal.ReleaseNameFromT(t)
				deployment := fmt.Sprintf("%s-sumologic-mock", releaseName)
				metricFilters := sumologicmock.MetadataFilters{
					"__name__":   "kube_deployment_spec_replicas",
					"deployment": deployment,
				}
				namespace := ctxopts.Namespace(ctx)
				expectedLabels := sumologicmock.Labels{
					"cluster":    "kubernetes",
					"_origin":    "kubernetes",
					"deployment": deployment,
					"endpoint":   "http",
					"job":        "kube-state-metrics",
					"namespace":  ctxopts.Namespace(ctx),
				}
				expectedLabels = addCollectorSpecificMetricLabels(expectedLabels, releaseName, namespace, metricsCollector)
				// drop some unnecessary labels
				delete(expectedLabels, "prometheus_service")
				return stepfuncs.WaitUntilExpectedMetricLabelsPresent(metricFilters, expectedLabels, waitDuration, tickDuration)(ctx, t, envConf)
			},
		).
		Assess("expected labels are present for pod kube-state-metrics",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				releaseName := strings_internal.ReleaseNameFromT(t)
				deployment := fmt.Sprintf("%s-kube-state-metrics", releaseName)
				metricFilters := sumologicmock.MetadataFilters{
					"__name__":   "kube_pod_status_phase",
					"phase":      "Running",
					"deployment": deployment,
				}
				namespace := ctxopts.Namespace(ctx)
				expectedLabels := sumologicmock.Labels{
					"cluster":                                "kubernetes",
					"_origin":                                "kubernetes",
					"container":                              "kube-state-metrics",
					"deployment":                             deployment,
					"endpoint":                               "http",
					"job":                                    "kube-state-metrics",
					"namespace":                              namespace,
					"node":                                   internal.NodeNameRegex,
					"phase":                                  "Running",
					"pod_labels_app.kubernetes.io/component": "metrics",
					"pod_labels_app.kubernetes.io/instance":  releaseName,
					"pod_labels_app.kubernetes.io/managed-by": "Helm",
					"pod_labels_app.kubernetes.io/name":       "kube-state-metrics",
					"pod_labels_app.kubernetes.io/part-of":    "kube-state-metrics",
					"pod_labels_app.kubernetes.io/version":    "\\d+\\.\\d+\\.\\d+",
					"pod_labels_helm.sh/chart":                "kube-state-metrics-\\d+\\.\\d+\\.\\d+",
					"pod_labels_release":                      releaseName,
					"pod_labels_pod-template-hash":            ".+",
					"pod":                                     fmt.Sprintf("%s-.+", deployment),
					"replicaset":                              fmt.Sprintf("%s-.+", deployment),
					"service":                                 deployment,
					"service_discovery_pod":                   fmt.Sprintf("%s-.+", deployment),
					"uid":                                     ".+",
				}
				expectedLabels = addCollectorSpecificMetricLabels(expectedLabels, releaseName, namespace, metricsCollector)
				// drop some unnecessary labels
				delete(expectedLabels, "prometheus_service")

				return stepfuncs.WaitUntilExpectedMetricLabelsPresent(metricFilters, expectedLabels, waitDuration, tickDuration)(ctx, t, envConf)
			},
		).
		Feature()
}

func GetTelegrafMetricsFeature(expectedMetrics []string, metricsCollector MetricsCollector, errOnExtra bool) features.Feature {
	return features.New("telegraf_metrics").
		Setup(stepfuncs.KubectlApplyFOpt(internal.NginxTelegrafMetricsTest, internal.NginxTelegrafNamespace)).
		Assess("expected metrics are present",
			stepfuncs.WaitUntilExpectedMetricsPresentWithFilters(
				expectedMetrics,
				sumologicmock.MetadataFilters{"job": "pod-annotations"},
				errOnExtra,
				waitDuration*2, // wait longer here, as it can take a bit of time for Nginx to start with the sidecar
				tickDuration,
			),
		).
		Assess("expected labels are present for annotation metrics",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				metricFilters := sumologicmock.MetadataFilters{"__name__": "nginx_accepts", "job": "pod-annotations"}
				releaseName := strings_internal.ReleaseNameFromT(t)
				namespace := ctxopts.Namespace(ctx)
				expectedLabels := sumologicmock.Labels{
					"cluster":                      "kubernetes",
					"_origin":                      "kubernetes",
					"deployment":                   "nginx",
					"endpoint":                     "/metrics",
					"job":                          "pod-annotations",
					"namespace":                    internal.NginxTelegrafNamespace,
					"node":                         internal.NodeNameRegex,
					"pod_labels_app":               "nginx",
					"pod_labels_pod-template-hash": ".+",
					"pod":                          "nginx-.+",
					"replicaset":                   "nginx-.*",
					"service":                      "nginx",
					"app":                          "nginx",
					"host":                         "nginx-.+",
					"port":                         "80",
					"server":                       "localhost",
					"pod_template_hash":            ".+",
				}
				expectedLabels = addCollectorSpecificMetricLabels(expectedLabels, releaseName, namespace, metricsCollector)

				// drop some unnecessary labels
				delete(expectedLabels, "prometheus_service")

				return stepfuncs.WaitUntilExpectedMetricLabelsPresent(metricFilters, expectedLabels, waitDuration, tickDuration)(ctx, t, envConf)
			},
		).
		Teardown(stepfuncs.KubectlDeleteFOpt(internal.NginxTelegrafMetricsTest, internal.NginxTelegrafNamespace)).
		Feature()
}

// addCollectorSpecificMetricLabels adds labels which are present only for the specific metric collector or metadata Service
func addCollectorSpecificMetricLabels(labels sumologicmock.Labels, releaseName string, serviceMonitorNamespace string, collector MetricsCollector) sumologicmock.Labels {
	outputLabels := make(sumologicmock.Labels, len(labels))
	for key, value := range labels {
		outputLabels[key] = value
	}
	prometheusLabels := sumologicmock.Labels{
		"_collector":         "kubernetes",
		"instance":           internal.IpWithPortRegex,
		"prometheus_replica": fmt.Sprintf("prometheus-%s-.*-0", releaseName),
		"prometheus":         fmt.Sprintf("%s/%s-.*-prometheus", serviceMonitorNamespace, releaseName),
		"prometheus_service": fmt.Sprintf("%s-.*-kubelet", releaseName),
	}
	otelcolLabels := sumologicmock.Labels{
		"_collector":     "kubernetes",
		"server.address": ".*",
		"server.port":    ".*",
		"url.scheme":     ".*",
	}

	if collector == Prometheus {
		for key, value := range prometheusLabels {
			outputLabels[key] = value
		}
	} else if collector == Otelcol {
		for key, value := range otelcolLabels {
			outputLabels[key] = value
		}
	}
	return outputLabels
}

func generateDeploymentLogs() features.Func {
	return stepfuncs.GenerateLogs(
		stepfuncs.LogsGeneratorDeployment,
		logsGeneratorCount,
		internal.LogsGeneratorName,
		internal.LogsGeneratorNamespace,
		internal.LogsGeneratorImage,
	)
}

func generateDaemonsetLogs() features.Func {
	return stepfuncs.GenerateLogs(
		stepfuncs.LogsGeneratorDaemonSet,
		logsGeneratorCount,
		internal.LogsGeneratorName,
		internal.LogsGeneratorNamespace,
		internal.LogsGeneratorImage,
	)
}

func waitForDeploymentLogs(count uint, strict bool, waitFunction stepfuncs.WaitForLogs) features.Func {
	return waitFunction(
		count,
		map[string]string{
			"namespace":  internal.LogsGeneratorName,
			"deployment": internal.LogsGeneratorName,
		},
		waitDuration,
		tickDuration,
		strict,
	)
}

func waitForDaemonsetLogs(count uint, strict bool, waitFunction stepfuncs.WaitForLogs) features.Func {
	return waitFunction(
		count,
		map[string]string{
			"namespace": internal.LogsGeneratorName,
			"daemonset": internal.LogsGeneratorName,
		},
		waitDuration,
		tickDuration,
		strict,
	)
}

func checkDeploymentMetadata(count uint, strict bool, waitFunction stepfuncs.WaitForLogs) features.Func {
	return waitFunction(
		count,
		map[string]string{
			"cluster": internal.ClusterName,
			// TODO: uncomment this after v4 release
			// or make it depend on the metadata provider
			// "_collector":     internal.ClusterName,
			"namespace":      internal.LogsGeneratorName,
			"pod_labels_app": internal.LogsGeneratorName,
			"container":      internal.LogsGeneratorName,
			"deployment":     internal.LogsGeneratorName,
			"pod":            fmt.Sprintf("%s%s", internal.LogsGeneratorName, internal.PodDeploymentSuffixRegex),
			"host":           internal.NodeNameRegex,
			"node":           internal.NodeNameRegex,
			"_sourceName": fmt.Sprintf(
				"%s\\.%s%s\\.%s",
				internal.LogsGeneratorNamespace,
				internal.LogsGeneratorName,
				internal.PodDeploymentSuffixRegex,
				internal.LogsGeneratorName,
			),
			"_sourceCategory": fmt.Sprintf(
				"%s/%s/%s", // dashes instead of hyphens due to sourceCategoryReplaceDash
				internal.ClusterName,
				strings.ReplaceAll(internal.LogsGeneratorNamespace, "-", "/"),
				strings.ReplaceAll(internal.LogsGeneratorName, "-", "/"), // this is the pod name prefix, in this case the deployment name
			),
			"_sourceHost": internal.EmptyRegex,
		},
		waitDuration,
		tickDuration,
		strict,
	)
}

func checkDaemonsetMetadata(count uint, strict bool, waitFunction stepfuncs.WaitForLogs) features.Func {
	return waitFunction(
		count,
		map[string]string{
			// TODO: uncomment this after v4 release
			// or make it depend on the metadata provider
			// "_collector":  "kubernetes",
			"namespace":      internal.LogsGeneratorName,
			"pod_labels_app": internal.LogsGeneratorName,
			"container":      internal.LogsGeneratorName,
			"daemonset":      internal.LogsGeneratorName,
			"pod":            fmt.Sprintf("%s%s", internal.LogsGeneratorName, internal.PodDaemonSetSuffixRegex),
			"host":           internal.NodeNameRegex,
			"node":           internal.NodeNameRegex,
			"_sourceName": fmt.Sprintf(
				"%s\\.%s%s\\.%s",
				internal.LogsGeneratorNamespace,
				internal.LogsGeneratorName,
				internal.PodDaemonSetSuffixRegex,
				internal.LogsGeneratorName,
			),
			"_sourceCategory": fmt.Sprintf(
				"%s/%s/%s", // dashes instead of hyphens due to sourceCategoryReplaceDash
				internal.ClusterName,
				strings.ReplaceAll(internal.LogsGeneratorNamespace, "-", "/"),
				strings.ReplaceAll(internal.LogsGeneratorName, "-", "/"), // this is the pod name prefix, in this case the DaemonSet name
			),
			"_sourceHost": internal.EmptyRegex,
		},
		waitDuration,
		tickDuration,
		strict,
	)
}

func checkSystemdLogs(count uint, strict bool, waitFunction stepfuncs.WaitForLogs) features.Func {
	return waitFunction(
		count,
		map[string]string{
			"cluster":         "kubernetes",
			"_sourceName":     internal.NotUndefinedRegex,
			"_sourceCategory": "kubernetes/system",
			"_sourceHost":     internal.NodeNameRegex,
		},
		waitDuration,
		tickDuration,
		strict,
	)
}

func checkKubeletLogs(count uint, strict bool, waitFunction stepfuncs.WaitForLogs) features.Func {
	return waitFunction(
		count,
		map[string]string{
			"cluster":         "kubernetes",
			"_sourceName":     "k8s_kubelet",
			"_sourceCategory": "kubernetes/kubelet",
			"_sourceHost":     internal.NodeNameRegex,
		},
		waitDuration,
		tickDuration,
		strict,
	)
}

func removeLogsDeployment(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
	opts := *ctxopts.KubectlOptions(ctx, envConf)
	opts.Namespace = internal.LogsGeneratorNamespace
	terrak8s.RunKubectl(t, &opts, "delete", "deployment", internal.LogsGeneratorName)
	return ctx
}

func removeLogsDaemonset(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
	opts := *ctxopts.KubectlOptions(ctx, envConf)
	opts.Namespace = internal.LogsGeneratorNamespace
	terrak8s.RunKubectl(t, &opts, "delete", "daemonset", internal.LogsGeneratorName)
	return ctx
}

func GetAllLogsFeature(waitFunction stepfuncs.WaitForLogs, generate bool) features.Feature {
	feature := features.New("logs")
	if generate {
		feature = feature.
			Setup(func(ctx context.Context, t *testing.T, c *envconf.Config) context.Context {
				// Wait before generating logs to help with flakiness
				t.Log("Waiting 5 seconds before generating logs...")
				time.Sleep(waitBeforeLogsGeneration) // Adjust as needed
				return ctx
			}).
			Setup(generateDeploymentLogs()).
			Setup(generateDaemonsetLogs())
	}
	feature = feature.
		Assess("logs from log generator deployment present", waitForDeploymentLogs(
			logsGeneratorCount,
			false,
			waitFunction,
		)).
		Assess("logs from log generator daemonset present", waitForDaemonsetLogs(
			logsGeneratorCount,
			false,
			waitFunction,
		)).
		Assess("expected container log metadata is present for log generator deployment", checkDeploymentMetadata(
			logsGeneratorCount,
			false,
			waitFunction,
		)).
		Assess("expected container log metadata is present for log generator daemonset", checkDaemonsetMetadata(
			logsGeneratorCount,
			false,
			waitFunction,
		)).
		Assess("logs from node systemd present", checkSystemdLogs(
			10, // we don't really control this, just want to check if the logs show up
			false,
			waitFunction,
		)).
		Assess("logs from kubelet present", checkKubeletLogs(
			1, // we don't really control this, just want to check if the logs show up
			false,
			waitFunction,
		))

	if generate {
		feature = feature.
			Teardown(removeLogsDeployment).
			Teardown(removeLogsDaemonset)
	}

	return feature.Feature()
}

func DeployAdditionalSumologicMock() features.Feature {
	return features.New("create additional sumologic mock").
		Setup(func(ctx context.Context, t *testing.T, c *envconf.Config) context.Context {
			namespace := ctxopts.Namespace(ctx)
			return stepfuncs.KubectlApplyFOpt(internal.YamlPathAdditionalSumologicMock, namespace)(ctx, t, c)
		}).
		Assess("additional sumologic mock is ready", stepfuncs.WaitUntilAdditionalSumologicMockAvailable(waitDuration, tickDuration)).
		Feature()
}

func DeleteAdditionalSumologicMock() features.Feature {
	return features.New("delete additional sumologic mock").
		Setup(func(ctx context.Context, t *testing.T, c *envconf.Config) context.Context {
			namespace := ctxopts.Namespace(ctx)
			return stepfuncs.KubectlDeleteFOpt(internal.YamlPathAdditionalSumologicMock, namespace)(ctx, t, c)
		}).
		Feature()
}

func GetAdditionalPartiallyLogsFeature() features.Feature {
	return features.New("additional exporter logs").
		Assess("logs from log generator deployment present", waitForDeploymentLogs(
			logsGeneratorCount,
			false,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Assess("logs from log generator daemonset not present", waitForDaemonsetLogs(
			0,
			true,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Assess("expected container log metadata is present for log generator deployment", checkDeploymentMetadata(
			logsGeneratorCount,
			false,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Assess("logs from node systemd not present", checkSystemdLogs(
			0,
			true,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Assess("logs from kubelet not present", checkKubeletLogs(
			0,
			true,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Feature()
}

func GetPartialLogsFeature() features.Feature {
	return features.New("partial logs").
		Setup(generateDeploymentLogs()).
		Setup(generateDaemonsetLogs()).
		Assess("logs from log generator deployment present", waitForDeploymentLogs(
			0,
			true,
			stepfuncs.WaitUntilExpectedExactLogsPresent,
		)).
		Assess("logs from log generator daemonset present", waitForDaemonsetLogs(
			logsGeneratorCount,
			false,
			stepfuncs.WaitUntilExpectedExactLogsPresent,
		)).
		Assess("expected container log metadata is present for log generator daemonset", checkDaemonsetMetadata(
			logsGeneratorCount,
			false,
			stepfuncs.WaitUntilExpectedExactLogsPresent,
		)).
		Assess("logs from node systemd present", checkSystemdLogs(
			10, // we don't really control this, just want to check if the logs show up
			false,
			stepfuncs.WaitUntilExpectedExactLogsPresent,
		)).
		Assess("logs from kubelet present", checkKubeletLogs(
			1, // we don't really control this, just want to check if the logs show up
			false,
			stepfuncs.WaitUntilExpectedExactLogsPresent,
		)).
		Assess("logs from log generator deployment present", waitForDeploymentLogs(
			logsGeneratorCount,
			false,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Assess("logs from log generator daemonset not present", waitForDaemonsetLogs(
			0,
			true,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Assess("expected container log metadata is present for log generator deployment", checkDeploymentMetadata(
			logsGeneratorCount,
			false,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Assess("logs from node systemd not present", checkSystemdLogs(
			0,
			true,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Assess("logs from kubelet not present", checkKubeletLogs(
			0,
			true,
			stepfuncs.WaitUntilExpectedAdditionalLogsPresent,
		)).
		Teardown(removeLogsDeployment).
		Teardown(removeLogsDaemonset).
		Feature()
}

func GetMultilineLogsFeature() features.Feature {
	return features.New("multiline logs").
		Setup(stepfuncs.KubectlApplyFOpt(internal.MultilineLogsGenerator, internal.MultilineLogsNamespace)).
		Assess("multiline logs present", stepfuncs.WaitUntilExpectedLogsPresent(
			multilineLogCount,
			map[string]string{
				"namespace":          internal.MultilineLogsNamespace,
				"pod_labels_example": internal.MultilineLogsPodName,
			},
			waitDuration,
			tickDuration,
		)).
		Teardown(stepfuncs.KubectlDeleteFOpt(internal.MultilineLogsGenerator, internal.MultilineLogsNamespace)).
		Feature()
}

func GetMultipleMultilineLogsFeature() features.Feature {
	return features.New("multiline logs").
		Setup(stepfuncs.KubectlApplyFOpt(internal.MultilineLogsGenerator, internal.MultilineLogsNamespace)).
		Assess("multiline logs present", stepfuncs.WaitUntilExpectedLogsPresent(
			multipleMultilineLogCount,
			map[string]string{
				"namespace":          internal.MultilineLogsNamespace,
				"pod_labels_example": internal.MultilineLogsPodName,
			},
			waitDuration,
			tickDuration,
		)).
		Teardown(stepfuncs.KubectlDeleteFOpt(internal.MultilineLogsGenerator, internal.MultilineLogsNamespace)).
		Feature()
}

func GetEventsFeature() features.Feature {
	return features.New("events").
		Assess("events present", stepfuncs.WaitUntilExpectedLogsPresent(
			expectedEventCount,
			map[string]string{
				"_sourceName":     "events",
				"_sourceCategory": fmt.Sprintf("%s/events", internal.ClusterName),
				"cluster":         "kubernetes",
			},
			waitDuration,
			tickDuration,
		)).
		Feature()
}

func GetJavaAppFeature() features.Feature {
	return features.New("java-app").
		Setup(stepfuncs.KubectlApplyFOpt(internal.InstrumentationJavaDep, internal.InstrumentationAppsNamespace)).
		Setup(stepfuncs.KubectlApplyFOpt(internal.InstrumentationJavaSvc, internal.InstrumentationAppsNamespace)).
		Assess("java-app deployment is present", stepfuncs.WaitUntilPodsAvailableCustomNS(
			v1.ListOptions{
				LabelSelector: "app=java-app",
			},
			1,
			waitDuration,
			tickDuration,
			internal.InstrumentationAppsNamespace,
		)).
		Assess("java-app svc is present", func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
			res := envConf.Client().Resources(internal.InstrumentationAppsNamespace)
			labelSelector := "app=java-svc"
			sl := corev1.ServiceList{}

			require.NoError(t,
				wait.For(
					conditions.New(res).
						ResourceListN(&sl, 1,
							resources.WithLabelSelector(labelSelector),
						),
					wait.WithTimeout(waitDuration),
					wait.WithInterval(tickDuration),
				),
			)
			return ctx
		}).Feature()
}

func GetNodeJSAppFeature() features.Feature {
	return features.New("nodejs-app").
		Setup(stepfuncs.KubectlApplyFOpt(internal.InstrumentationNodeJSDep, internal.InstrumentationAppsNamespace)).
		Setup(stepfuncs.KubectlApplyFOpt(internal.InstrumentationNodeJSSvc, internal.InstrumentationAppsNamespace)).
		Assess("nodejs-app deployment is present", stepfuncs.WaitUntilPodsAvailableCustomNS(
			v1.ListOptions{
				LabelSelector: "app=nodejs-app",
			},
			1,
			waitDuration,
			tickDuration,
			internal.InstrumentationAppsNamespace,
		)).
		Assess("nodejs-app svc is present", func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
			res := envConf.Client().Resources(internal.InstrumentationAppsNamespace)
			labelSelector := "app=nodejs-svc"
			sl := corev1.ServiceList{}

			require.NoError(t,
				wait.For(
					conditions.New(res).
						ResourceListN(&sl, 1,
							resources.WithLabelSelector(labelSelector),
						),
					wait.WithTimeout(waitDuration),
					wait.WithInterval(tickDuration),
				),
			)
			return ctx
		}).Feature()
}

func GetPythonAppFeature() features.Feature {
	return features.New("python-app").
		Setup(stepfuncs.KubectlApplyFOpt(internal.InstrumentationPythonDep, internal.InstrumentationAppsNamespace)).
		Setup(stepfuncs.KubectlApplyFOpt(internal.InstrumentationPythonSvc, internal.InstrumentationAppsNamespace)).
		Assess("python-app deployment is present", stepfuncs.WaitUntilPodsAvailableCustomNS(
			v1.ListOptions{
				LabelSelector: "app=python-app",
			},
			1,
			waitDuration,
			tickDuration,
			internal.InstrumentationAppsNamespace,
		)).
		Assess("python-app svc is present", func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
			res := envConf.Client().Resources(internal.InstrumentationAppsNamespace)
			labelSelector := "app=python-svc"
			sl := corev1.ServiceList{}

			require.NoError(t,
				wait.For(
					conditions.New(res).
						ResourceListN(&sl, 1,
							resources.WithLabelSelector(labelSelector),
						),
					wait.WithTimeout(waitDuration),
					wait.WithInterval(tickDuration),
				),
			)
			return ctx
		}).Feature()
}

func GetDotnetAppFeature() features.Feature {
	return features.New("dotnet-app").
		Setup(stepfuncs.KubectlApplyFOpt(internal.InstrumentationDotnetDep, internal.InstrumentationAppsNamespace)).
		Setup(stepfuncs.KubectlApplyFOpt(internal.InstrumentationDotnetSvc, internal.InstrumentationAppsNamespace)).
		Assess("dotnet-app deployment is present", stepfuncs.WaitUntilPodsAvailableCustomNS(
			v1.ListOptions{
				LabelSelector: "app=dotnet-app",
			},
			1,
			waitDuration,
			tickDuration,
			internal.InstrumentationAppsNamespace,
		)).
		Assess("dotnet-app svc is present", func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
			res := envConf.Client().Resources(internal.InstrumentationAppsNamespace)
			labelSelector := "app=dotnet-svc"
			sl := corev1.ServiceList{}

			require.NoError(t,
				wait.For(
					conditions.New(res).
						ResourceListN(&sl, 1,
							resources.WithLabelSelector(labelSelector),
						),
					wait.WithTimeout(waitDuration),
					wait.WithInterval(tickDuration),
				),
			)
			return ctx
		}).Feature()
}

func GetCurlAppFeature() features.Feature {
	return features.New("curlapp").WithStep("tst", features.Level(0),
		stepfuncs.MakeCurl(
			curlAppSleepInterval,
			curlAppMaxWaitTime,
			internal.CurlAppName,
			internal.InstrumentationAppsNamespace,
			internal.CurlAppImage,
		)).
		Assess("wait for dotnet traces", stepfuncs.WaitUntilExpectedSpansPresent(
			1,
			map[string]string{
				"application":              "test-apps",
				"service.name":             "dotnet-app",
				"_collector":               "kubernetes",
				"http.request.method":      "GET",
				"url.path":                 "/",
				"k8s.cluster.name":         "kubernetes",
				"k8s.container.name":       "dotnetapp",
				"k8s.deployment.name":      "dotnet-app",
				"k8s.namespace.name":       internal.InstrumentationAppsNamespace,
				"k8s.pod.pod_name":         "dotnet-app",
				"k8s.pod.label.app":        "dotnet-app",
				"_sourceCategory":          "kubernetes/test/apps/dotnet/app",
				"_sourceName":              fmt.Sprintf("%s.dotnet-app.dotnetapp", internal.InstrumentationAppsNamespace),
				"telemetry.distro.version": "1.9.0",
				"telemetry.sdk.language":   "dotnet",
				"telemetry.sdk.version":    "1.9.0",
			},
			waitDuration,
			tickDuration,
		)).
		Assess("wait for nodejs traces", stepfuncs.WaitUntilExpectedSpansPresent(
			1,
			map[string]string{
				"application":            "test-apps",
				"service.name":           "nodejs-app",
				"_collector":             "kubernetes",
				"http.method":            "GET",
				"k8s.cluster.name":       "kubernetes",
				"k8s.container.name":     "nodejsapp",
				"k8s.deployment.name":    "nodejs-app",
				"k8s.namespace.name":     internal.InstrumentationAppsNamespace,
				"k8s.pod.pod_name":       "nodejs-app",
				"k8s.pod.label.app":      "nodejs-app",
				"net.host.port":          "3000",
				"_sourceCategory":        "kubernetes/test/apps/nodejs/app",
				"_sourceName":            fmt.Sprintf("%s.nodejs-app.nodejsapp", internal.InstrumentationAppsNamespace),
				"telemetry.sdk.language": "nodejs",
				"telemetry.sdk.name":     "opentelemetry",
				"telemetry.sdk.version":  "1.27.0",
			},
			waitDuration,
			tickDuration,
		)).
		Assess("wait for java traces", stepfuncs.WaitUntilExpectedSpansPresent(
			1,
			map[string]string{
				"application":               "test-apps",
				"service.name":              "java-app",
				"_collector":                "kubernetes",
				"http.request.method":       "GET",
				"http.route":                "/",
				"http.response.status_code": "200",
				"url.scheme":                "http",
				"url.path":                  "/",
				"k8s.cluster.name":          "kubernetes",
				"k8s.container.name":        "javaapp",
				"k8s.deployment.name":       "java-app",
				"k8s.namespace.name":        internal.InstrumentationAppsNamespace,
				"k8s.pod.pod_name":          "java-app",
				"k8s.pod.label.app":         "java-app",
				"server.port":               "8080",
				"network.protocol.version":  "1.1",
				"_sourceCategory":           "kubernetes/test/apps/java/app",
				"_sourceName":               fmt.Sprintf("%s.java-app.javaapp", internal.InstrumentationAppsNamespace),
				"telemetry.distro.version":  "2.10.0",
				"telemetry.sdk.language":    "java",
				"telemetry.sdk.name":        "opentelemetry",
				"telemetry.sdk.version":     "1.44.1",
			},
			waitDuration,
			tickDuration,
		)).
		Teardown(stepfuncs.KubectlDeleteNamespaceOpt(internal.InstrumentationAppsNamespace, true)).
		Feature()
}

func GetTracesFeature() features.Feature {
	return features.New("traces").
		Setup(stepfuncs.GenerateTraces(
			tracesPerExporter,
			spansPerTrace,
			internal.TracesGeneratorName,
			internal.TracesGeneratorNamespace,
			internal.TracesGeneratorImage,
		)).
		Assess("wait for otlp http traces", stepfuncs.WaitUntilExpectedTracesPresent(
			tracesPerExporter,
			spansPerTrace,
			map[string]string{
				"__name__":            "root-span-otlpHttp",
				"service.name":        "customer-trace-test-service",
				"_collector":          "kubernetes",
				"k8s.cluster.name":    "kubernetes",
				"k8s.container.name":  internal.TracesGeneratorName,
				"k8s.deployment.name": internal.TracesGeneratorName,
				"k8s.namespace.name":  internal.TracesGeneratorNamespace,
				"k8s.pod.pod_name":    internal.TracesGeneratorName,
				"k8s.pod.label.app":   internal.TracesGeneratorName,
				// "_sourceCategory":    "kubernetes/customer/trace/tester/customer/trace/tester",
				"_sourceName": fmt.Sprintf("%s.%s.%s", internal.TracesGeneratorNamespace, internal.TracesGeneratorName, internal.TracesGeneratorName),
			},
			waitDuration,
			tickDuration,
		)).
		Assess("wait for otlp grpc traces", stepfuncs.WaitUntilExpectedTracesPresent(
			tracesPerExporter,
			spansPerTrace,
			map[string]string{
				"__name__":            "root-span-otlpGrpc",
				"service.name":        "customer-trace-test-service",
				"_collector":          "kubernetes",
				"k8s.cluster.name":    "kubernetes",
				"k8s.container.name":  internal.TracesGeneratorName,
				"k8s.deployment.name": internal.TracesGeneratorName,
				"k8s.namespace.name":  internal.TracesGeneratorNamespace,
				"k8s.pod.pod_name":    internal.TracesGeneratorName,
				"k8s.pod.label.app":   internal.TracesGeneratorName,
				// "_sourceCategory":    "kubernetes/customer/trace/tester/customer/trace/tester",
				"_sourceName": fmt.Sprintf("%s.%s.%s", internal.TracesGeneratorNamespace, internal.TracesGeneratorName, internal.TracesGeneratorName),
			},
			waitDuration,
			tickDuration,
		)).
		Assess("wait for zipkin traces", stepfuncs.WaitUntilExpectedTracesPresent(
			tracesPerExporter,
			spansPerTrace,
			map[string]string{
				"__name__":            "root-span-zipkin",
				"service.name":        "customer-trace-test-service",
				"_collector":          "kubernetes",
				"k8s.cluster.name":    "kubernetes",
				"k8s.container.name":  internal.TracesGeneratorName,
				"k8s.deployment.name": internal.TracesGeneratorName,
				"k8s.namespace.name":  internal.TracesGeneratorNamespace,
				"k8s.pod.pod_name":    internal.TracesGeneratorName,
				"k8s.pod.label.app":   internal.TracesGeneratorName,
				// "_sourceCategory":    "kubernetes/customer/trace/tester/customer/trace/tester",
				"_sourceName": fmt.Sprintf("%s.%s.%s", internal.TracesGeneratorNamespace, internal.TracesGeneratorName, internal.TracesGeneratorName),
			},
			waitDuration,
			tickDuration,
		)).
		Assess("wait for jaeger thrift http traces", stepfuncs.WaitUntilExpectedTracesPresent(
			tracesPerExporter,
			spansPerTrace,
			map[string]string{
				"__name__":            "root-span-jaegerThriftHttp",
				"service.name":        "customer-trace-test-service",
				"_collector":          "kubernetes",
				"k8s.cluster.name":    "kubernetes",
				"k8s.container.name":  internal.TracesGeneratorName,
				"k8s.deployment.name": internal.TracesGeneratorName,
				"k8s.namespace.name":  internal.TracesGeneratorNamespace,
				"k8s.pod.pod_name":    internal.TracesGeneratorName,
				"k8s.pod.label.app":   internal.TracesGeneratorName,
				// "_sourceCategory":    "kubernetes/customer/trace/tester/customer/trace/tester",
				"_sourceName":       fmt.Sprintf("%s.%s.%s", internal.TracesGeneratorNamespace, internal.TracesGeneratorName, internal.TracesGeneratorName),
				"otel.library.name": "jaegerThriftHttp",
			},
			waitDuration,
			tickDuration,
		)).
		Assess("wait for all spans", stepfuncs.WaitUntilExpectedSpansPresent(
			4*tracesPerExporter*spansPerTrace, // there are 4 exporters
			map[string]string{},
			waitDuration,
			tickDuration,
		)).
		Teardown(func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
			opts := *ctxopts.KubectlOptions(ctx, envConf)
			opts.Namespace = internal.TracesGeneratorNamespace
			terrak8s.RunKubectl(t, &opts, "delete", "deployment", internal.TracesGeneratorName)
			return ctx
		}).
		Teardown(stepfuncs.KubectlDeleteNamespaceOpt(internal.TracesGeneratorNamespace, true)).
		Feature()
}

func GetTailingSidecarFeature() features.Feature {
	return features.New("tailing sidecar test").
		Setup(stepfuncs.KubectlApplyFOpt(internal.TailingSidecarTest, internal.TailingSidecarTestNamespace)).
		Assess("tailing sidecar test logs present", stepfuncs.WaitUntilExpectedLogsPresent(
			TailingSidecarCount,
			map[string]string{
				"namespace":  internal.TailingSidecarTestNamespace,
				"deployment": internal.TailingSidecarTestDeploymentName,
			},
			waitDuration,
			tickDuration,
		)).
		Teardown(stepfuncs.KubectlDeleteFOpt(internal.TailingSidecarTest, internal.TailingSidecarTestNamespace)).
		Feature()
}

func GetAnnotationsFeature() features.Feature {
	return features.New("annotations test").
		Setup(stepfuncs.KubectlApplyFOpt(internal.AnnotationsTest, internal.AnnotationsTestNamespace)).
		Assess("pod annotations are applied", stepfuncs.WaitUntilExpectedLogsPresent(
			AnnotationsLogsCount,
			map[string]string{
				"namespace":       internal.AnnotationsTestNamespace,
				"_sourceCategory": "pod_Source_Category_Prefixpod_Source_Category",
				"_sourceHost":     "podSourceHost",
				"_sourceName":     "podSourceName",
			},
			waitDuration,
			tickDuration,
		)).
		Assess("container annotation is applied", stepfuncs.WaitUntilExpectedLogsPresent(
			AnnotationsLogsCount,
			map[string]string{
				"namespace":       internal.AnnotationsTestNamespace,
				"_sourceCategory": "containerSourceCategory",
				"_sourceHost":     "podSourceHost",
				"_sourceName":     "podSourceName",
			},
			waitDuration,
			tickDuration,
		)).
		Teardown(stepfuncs.KubectlDeleteFOpt(internal.AnnotationsTest, internal.AnnotationsTestNamespace)).
		Feature()
}

func GetNamespaceAnnotationsFeature() features.Feature {
	return features.New("namespace annotations test").
		Setup(stepfuncs.KubectlApplyFOpt(internal.NamespaceAnnotationsTest, internal.NamespaceAnnotationsTestNamespace)).
		Assess("pod annotations are applied", stepfuncs.WaitUntilExpectedLogsPresent(
			AnnotationsLogsCount,
			map[string]string{
				"namespace":       internal.NamespaceAnnotationsTestNamespace,
				"_sourceCategory": "namespace#Source#Category#PrefixnamespaceSourceCategory",
				"_sourceHost":     "namespaceSourceHost",
				"_sourceName":     "namespaceSourceName",
			},
			waitDuration,
			tickDuration,
		)).
		Assess("container annotation is applied", stepfuncs.WaitUntilExpectedLogsPresent(
			AnnotationsLogsCount,
			map[string]string{
				"namespace":       internal.NamespaceAnnotationsTestNamespace,
				"_sourceCategory": "podSource!Category!PrefixpodSourceCategory",
				"_sourceHost":     "podSourceHost",
				"_sourceName":     "podSourceName",
			},
			waitDuration,
			tickDuration,
		)).
		Teardown(stepfuncs.KubectlDeleteFOpt(internal.NamespaceAnnotationsTest, internal.NamespaceAnnotationsTestNamespace)).
		Feature()
}

type featureCheck func(*features.FeatureBuilder) *features.FeatureBuilder

func GetInstallFeature(installChecks []featureCheck) features.Feature {
	featureBuilder := features.New("installation")
	for _, installCheck := range installChecks {
		featureBuilder = installCheck(featureBuilder)
	}

	return featureBuilder.Feature()
}

func CheckSumologicSecret(endpointCount int) featureCheck {
	return func(builder *features.FeatureBuilder) *features.FeatureBuilder {
		return builder.Assess("sumologic secret is created with endpoints",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				terrak8s.WaitUntilSecretAvailable(t, ctxopts.KubectlOptions(ctx, envConf), "sumologic", 60, tickDuration)
				secret := terrak8s.GetSecret(t, ctxopts.KubectlOptions(ctx, envConf), "sumologic")
				require.Len(t, secret.Data, endpointCount, "Secret has incorrect number of endpoints")
				return ctx
			})
	}
}

func CheckOtelcolMetadataMetricsInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("otelcol metrics statefulset is ready",
			stepfuncs.WaitUntilStatefulSetIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-metrics"),
				),
				stepfuncs.WithLabelsF(
					stepfuncs.LabelFormatterKV{
						K: "app",
						V: stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-metrics"),
					},
				),
			),
		).
		Assess("otelcol metrics buffers PVCs are created and bound",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				res := envConf.Client().Resources(ctxopts.Namespace(ctx))
				pvcs := corev1.PersistentVolumeClaimList{}
				cond := conditions.
					New(res).
					ResourceListMatchN(&pvcs, 1,
						func(object k8s.Object) bool {
							pvc := object.(*corev1.PersistentVolumeClaim)
							if pvc.Status.Phase != corev1.ClaimBound {
								log.V(0).Infof("PVC %q not bound yet", pvc.Name)
								return false
							}
							return true
						},
						resources.WithLabelSelector(
							fmt.Sprintf("app=%s-sumologic-otelcol-metrics", strings_internal.ReleaseNameFromT(t)),
						),
					)
				require.NoError(t,
					wait.For(cond,
						wait.WithTimeout(waitDuration),
						wait.WithInterval(tickDuration),
					),
				)
				return ctx
			})
}

func CheckOtelcolMetricsCollectorInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("otelcol metrics collector statefulset is ready",
			stepfuncs.WaitUntilStatefulSetIsReady(
				waitDuration*2,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-metrics-collector"),
				),
			),
		)
}

func CheckOtelcolMetadataLogsInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("otelcol logs statefulset is ready",
			stepfuncs.WaitUntilStatefulSetIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-logs"),
				),
				stepfuncs.WithLabelsF(
					stepfuncs.LabelFormatterKV{
						K: "app",
						V: stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-logs"),
					},
				),
			),
		).
		Assess("otelcol logs buffers PVCs are created and bound",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				res := envConf.Client().Resources(ctxopts.Namespace(ctx))
				pvcs := corev1.PersistentVolumeClaimList{}
				cond := conditions.
					New(res).
					ResourceListMatchN(&pvcs, 1,
						func(object k8s.Object) bool {
							pvc := object.(*corev1.PersistentVolumeClaim)
							if pvc.Status.Phase != corev1.ClaimBound {
								log.V(0).Infof("PVC %q not bound yet", pvc.Name)
								return false
							}
							return true
						},
						resources.WithLabelSelector(
							fmt.Sprintf("app=%s-sumologic-otelcol-logs", strings_internal.ReleaseNameFromT(t)),
						),
					)
				require.NoError(t,
					wait.For(cond,
						wait.WithTimeout(waitDuration),
						wait.WithInterval(tickDuration),
					),
				)
				return ctx
			})
}

func CheckOtelcolEventsInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("otelcol events statefulset is ready",
			stepfuncs.WaitUntilStatefulSetIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-events"),
				),
				stepfuncs.WithLabelsF(
					stepfuncs.LabelFormatterKV{
						K: "app",
						V: stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-events"),
					},
				),
			),
		).
		Assess("otelcol events buffers PVCs are created",
			func(ctx context.Context, t *testing.T, envConf *envconf.Config) context.Context {
				namespace := ctxopts.Namespace(ctx)
				releaseName := strings_internal.ReleaseNameFromT(t)
				kubectlOptions := ctxopts.KubectlOptions(ctx, envConf)

				t.Logf("kubeconfig: %s", kubectlOptions.ConfigPath)
				cl, err := terrak8s.GetKubernetesClientFromOptionsE(t, kubectlOptions)
				require.NoError(t, err)

				assert.Eventually(t, func() bool {
					pvcs, err := cl.CoreV1().
						PersistentVolumeClaims(namespace).
						List(ctx, v1.ListOptions{
							LabelSelector: fmt.Sprintf("app=%s-sumologic-otelcol-events", releaseName),
						})
					if !assert.NoError(t, err) {
						return false
					}

					return len(pvcs.Items) == 1
				}, waitDuration, tickDuration)
				return ctx
			})
}

func CheckPrometheusInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("prometheus pod is available",
			stepfuncs.WaitUntilPodsAvailable(
				v1.ListOptions{
					LabelSelector: "app.kubernetes.io/name=prometheus",
				},
				1,
				waitDuration,
				tickDuration,
			),
		)
}

func CheckOtelcolLogsCollectorInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("otelcol daemonset is ready",
			stepfuncs.WaitUntilDaemonSetIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-logs-collector"),
				),
				stepfuncs.WithLabelsF(
					stepfuncs.LabelFormatterKV{
						K: "app",
						V: stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-logs-collector"),
					},
				),
			),
		)

}

func CheckTracesInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("traces-sampler deployment is ready",
			stepfuncs.WaitUntilDeploymentIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-traces-sampler"),
				),
				stepfuncs.WithLabelsF(stepfuncs.LabelFormatterKV{
					K: "app",
					V: stepfuncs.ReleaseFormatter("%s-sumologic-traces-sampler"),
				},
				),
			)).
		Assess("otelcol-instrumentation statefulset is ready",
			stepfuncs.WaitUntilStatefulSetIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-instrumentation"),
				),
				stepfuncs.WithLabelsF(
					stepfuncs.LabelFormatterKV{
						K: "app",
						V: stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-instrumentation"),
					},
				),
			),
		).
		Assess("traces-gateway deployment is ready",
			stepfuncs.WaitUntilDeploymentIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-traces-gateway"),
				),
				stepfuncs.WithLabelsF(stepfuncs.LabelFormatterKV{
					K: "app",
					V: stepfuncs.ReleaseFormatter("%s-sumologic-traces-gateway"),
				},
				),
			))

}

func CheckTracesWithoutGatewayInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("traces-sampler deployment is ready",
			stepfuncs.WaitUntilDeploymentIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-traces-sampler"),
				),
				stepfuncs.WithLabelsF(stepfuncs.LabelFormatterKV{
					K: "app",
					V: stepfuncs.ReleaseFormatter("%s-sumologic-traces-sampler"),
				},
				),
			)).
		Assess("otelcol-instrumentation statefulset is ready",
			stepfuncs.WaitUntilStatefulSetIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-instrumentation"),
				),
				stepfuncs.WithLabelsF(
					stepfuncs.LabelFormatterKV{
						K: "app",
						V: stepfuncs.ReleaseFormatter("%s-sumologic-otelcol-instrumentation"),
					},
				),
			),
		)
}

func CheckTailingSidecarOperatorInstall(builder *features.FeatureBuilder) *features.FeatureBuilder {
	return builder.
		Assess("tailing sidecar deployment is ready",
			stepfuncs.WaitUntilDeploymentIsReady(
				waitDuration,
				tickDuration,
				stepfuncs.WithNameF(
					stepfuncs.ReleaseFormatter("%s-tailing-sidecar-operator"),
				),
			),
		)
}
