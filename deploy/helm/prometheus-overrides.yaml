# This file is auto-generated.
## NOTE changing the serviceMonitor scrape interval to be >1m can result in metrics from recording rules to be missing and empty panels in Sumo Logic Kubernetes apps.
kubeApiServer:
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval:
kubelet:
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval:
kubeControllerManager:
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval:
coreDns:
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval:
kubeEtcd:
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval:
kubeScheduler:
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval:
kubeStateMetrics:
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval:
nodeExporter:
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval:
# Ensure we use pre 1.14 recording rules consistently as current content depends on them.
kubeTargetVersionOverride: 1.13.0-0
## Set the enabled flag to false for either of the below two purposes:
## 1. Not install prometheus operator helm chart as a dependency along with this helm chart
## 2. Disable metrics collection altogether
enabled: true
alertmanager:
  enabled: false
grafana:
  enabled: false
  defaultDashboardsEnabled: false
prometheusOperator:
  ## Resource limits for prometheus operator
  resources: {}
  # limits:
  #   cpu: 200m
  #   memory: 200Mi
  # requests:
  #   cpu: 100m
  #   memory: 100Mi
  admissionWebhooks:
    enabled: false
  tlsProxy:
    enabled: false
  ## Resource limits for kube-state-metrics
  kube-state-metrics:
    resources: {}
    # limits:
    #   cpu: 200m
    #   memory: 200Mi
    # requests:
    #   cpu: 200m
    #   memory: 200Mi
  ## Resource limits for prometheus node exporter
  prometheus-node-exporter:
    resources: {}
    # limits:
    #   cpu: 200m
    #   memory: 50Mi
    # requests:
    #   cpu: 100m
    #   memory: 30Mi
prometheus:
  additionalServiceMonitors:
  - name: collection-sumologic-fluentd-logs
    additionalLabels:
      app: collection-sumologic-fluentd-logs
    endpoints:
    - port: metrics
    namespaceSelector:
      matchNames:
      - sumologic
    selector:
      matchLabels:
        app: collection-sumologic-fluentd-logs
  - name: collection-sumologic-fluentd-metrics
    additionalLabels:
      app: collection-sumologic-fluentd-metrics
    endpoints:
    - port: metrics
    namespaceSelector:
      matchNames:
      - sumologic
    selector:
      matchLabels:
        app: collection-sumologic-fluentd-metrics
  - name: collection-sumologic-fluentd-events
    additionalLabels:
      app: collection-sumologic-fluentd-events
    endpoints:
    - port: metrics
    namespaceSelector:
      matchNames:
      - sumologic
    selector:
      matchLabels:
        app: collection-sumologic-fluentd-events
  - name: collection-fluent-bit
    additionalLabels:
      app: collection-fluent-bit
    endpoints:
    - port: metrics
      path: /api/v1/metrics/prometheus
    namespaceSelector:
      matchNames:
      - sumologic
    selector:
      matchLabels:
        app: fluent-bit
  - name: collection-sumologic-otelcol
    additionalLabels:
      app: collection-sumologic-otelcol
    endpoints:
    - port: metrics
    namespaceSelector:
      matchNames:
      - sumologic
    selector:
      matchLabels:
        app: collection-sumologic-otelcol
  prometheusSpec:
    ## Prometheus default scrape interval, default from upstream Prometheus Operator Helm chart
    ## NOTE changing the scrape interval to be >1m can result in metrics from recording rules to be missing and empty panels in Sumo Logic Kubernetes apps.
    scrapeInterval: "30s"
    ## Define resources requests and limits for single Pods.
    resources: {}
    # limits:
    #   cpu: 800m
    #   memory: 800Mi
    # requests:
    #   cpu: 400m
    #   memory: 400Mi
    thanos:
      baseImage: quay.io/thanos/thanos
      version: v0.10.0
    containers:
    - name: "prometheus-config-reloader"
      env:
      - name: CHART
        valueFrom:
          configMapKeyRef:
            name: sumologic-configmap
            key: fluentdMetrics
      - name: NAMESPACE
        valueFrom:
          configMapKeyRef:
            name: sumologic-configmap
            key: fluentdNamespace
    remoteWrite:
    - # kube state metrics
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.state
      writeRelabelConfigs:
      - action: keep
        regex: kube-state-metrics;(?:kube_statefulset_status_observed_generation|kube_statefulset_status_replicas|kube_statefulset_replicas|kube_statefulset_metadata_generation|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_number_unavailable|kube_deployment_spec_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_node_info|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_info|kube_pod_container_resource_requests|kube_pod_container_resource_limits|kube_pod_container_status_ready|kube_pod_container_status_terminated_reason|kube_pod_container_status_waiting_reason|kube_pod_container_status_restarts_total|kube_pod_status_phase)
        sourceLabels: [job, __name__]
    - # controller manager metrics
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.controller-manager
      writeRelabelConfigs:
      - action: keep
        regex: kubelet;cloudprovider_.*_api_request_duration_seconds.*
        sourceLabels: [job, __name__]
    - # scheduler metrics
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.scheduler
      writeRelabelConfigs:
      - action: keep
        regex: kube-scheduler;scheduler_(?:e2e_scheduling|binding|scheduling_algorithm)_latency_microseconds.*
        sourceLabels: [job, __name__]
    - # api server metrics:
      # apiserver_request_count
      # apiserver_request_total
      # apiserver_request_duration_seconds_count
      # apiserver_request_duration_seconds_sum
      # apiserver_request_latencies_count
      # apiserver_request_latencies_sum
      # apiserver_request_latencies_summary
      # apiserver_request_latencies_summary_count
      # apiserver_request_latencies_summary_sum
      # etcd_request_cache_get_duration_seconds_count
      # etcd_request_cache_get_duration_seconds_sum
      # etcd_request_cache_add_duration_seconds_count
      # etcd_request_cache_add_duration_seconds_sum
      # etcd_request_cache_add_latencies_summary_count
      # etcd_request_cache_add_latencies_summary_sum
      # etcd_request_cache_get_latencies_summary_count
      # etcd_request_cache_get_latencies_summary_sum
      # etcd_helper_cache_hit_count
      # etcd_helper_cache_hit_total
      # etcd_helper_cache_miss_count
      # etcd_helper_cache_miss_total
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.apiserver
      writeRelabelConfigs:
      - action: keep
        regex: apiserver;(?:apiserver_request_(?:count|total)|apiserver_request_(?:duration_seconds|latencies)_(?:count|sum)|apiserver_request_latencies_summary(?:|_count|_sum)|etcd_request_cache_(?:add|get)_(?:duration_seconds|latencies_summary)_(?:count|sum)|etcd_helper_cache_(?:hit|miss)_(?:count|total))
        sourceLabels: [job, __name__]
    - # kubelet metrics:
      # kubelet_docker_operations_errors
      # kubelet_docker_operations_errors_total
      # kubelet_docker_operations_duration_seconds_count
      # kubelet_docker_operations_duration_seconds_sum
      # kubelet_runtime_operations_duration_seconds_count
      # kubelet_runtime_operations_duration_seconds_sum
      # kubelet_running_container_count
      # kubelet_running_pod_count
      # kubelet_docker_operations_latency_microseconds
      # kubelet_docker_operations_latency_microseconds_count
      # kubelet_docker_operations_latency_microseconds_sum
      # kubelet_runtime_operations_latency_microseconds
      # kubelet_runtime_operations_latency_microseconds_count
      # kubelet_runtime_operations_latency_microseconds_sum
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.kubelet
      writeRelabelConfigs:
      - action: keep
        regex: kubelet;(?:kubelet_docker_operations_errors(?:|_total)|kubelet_(?:docker|runtime)_operations_duration_seconds_(?:count|sum)|kubelet_running_(?:container|pod)_count|kubelet_(:?docker|runtime)_operations_latency_microseconds(?:|_count|_sum))
        sourceLabels: [job, __name__]
    - # cadvisor container metrics
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.container
      writeRelabelConfigs:
      - action: labelmap
        regex: container_name
        replacement: container
      - action: drop
        regex: POD
        sourceLabels: [container]
      - action: keep
        regex: kubelet;.+;(?:container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_fs_usage_bytes|container_fs_limit_bytes)
        sourceLabels: [job, container, __name__]
    - # cadvisor aggregate container metrics
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.container
      writeRelabelConfigs:
      - action: keep
        regex: kubelet;(?:container_network_receive_bytes_total|container_network_transmit_bytes_total)
        sourceLabels: [job, __name__]
    - # node exporter metrics
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.node
      writeRelabelConfigs:
      - action: keep
        regex: node-exporter;(?:node_load1|node_load5|node_load15|node_cpu_seconds_total)
        sourceLabels: [job, __name__]
    - # prometheus operator rules
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.operator.rule
      writeRelabelConfigs:
      - action: keep
        regex: 'cluster_quantile:apiserver_request_latencies:histogram_quantile|instance:node_filesystem_usage:sum|instance:node_network_receive_bytes:rate:sum|cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile|cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile|cluster_quantile:scheduler_binding_latency:histogram_quantile|node_namespace_pod:kube_pod_info:|:kube_pod_info_node_count:|node:node_num_cpu:sum|:node_cpu_utilisation:avg1m|node:node_cpu_utilisation:avg1m|node:cluster_cpu_utilisation:ratio|:node_cpu_saturation_load1:|node:node_cpu_saturation_load1:|:node_memory_utilisation:|node:node_memory_bytes_total:sum|node:node_memory_utilisation:ratio|node:cluster_memory_utilisation:ratio|:node_memory_swap_io_bytes:sum_rate|node:node_memory_utilisation:|node:node_memory_utilisation_2:|node:node_memory_swap_io_bytes:sum_rate|:node_disk_utilisation:avg_irate|node:node_disk_utilisation:avg_irate|:node_disk_saturation:avg_irate|node:node_disk_saturation:avg_irate|node:node_filesystem_usage:|node:node_filesystem_avail:|:node_net_utilisation:sum_irate|node:node_net_utilisation:sum_irate|:node_net_saturation:sum_irate|node:node_net_saturation:sum_irate|node:node_inodes_total:|node:node_inodes_free:'
        sourceLabels: [__name__]
    - # health
      url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics
      writeRelabelConfigs:
      - action: keep
        regex: (?:up|prometheus_remote_storage_.*|fluentd_.*|fluentbit.*|otelcol.*)
        sourceLabels: [__name__]
