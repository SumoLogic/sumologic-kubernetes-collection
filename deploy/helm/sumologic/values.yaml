image:
  repository: sumologic/kubernetes-fluentd
  tag: 1.1.0-rc.0
  pullPolicy: IfNotPresent

nameOverride: ""

sumologic:
  ## Setup

  # If enabled, a pre-install hook will create Collector and Sources in Sumo Logic
  setupEnabled: true

  # If enabled, accessId and accessKey will be sourced from Secret Name given
  # Be sure to include at least the following env variables in your secret
  # (1) SUMOLOGIC_ACCESSID, (2) SUMOLOGIC_ACCESSKEY
  #envFromSecret: sumo-api-secret

  # Sumo access ID
  #accessId: ""

  # Sumo access key
  #accessKey: ""

  # Sumo API endpoint; Leave blank for automatic endpoint discovery and redirection
  # ref: https://help.sumologic.com/APIs/General-API-Information/Sumo-Logic-Endpoints-and-Firewall-Security
  endpoint: ""

  # proxy urls
  httpProxy: ""
  httpsProxy: ""

  # Collector name
  #collectorName: ""

  # Cluster name: Note spaces are not allowed and will be replaced with dashes.
  clusterName: "kubernetes"

  # Configuration of kubernetes for terraform client
  # https://www.terraform.io/docs/providers/kubernetes/index.html#argument-reference
  # All double quotes should be escaped here regarding terraform syntax
  cluster:
    host: "https://kubernetes.default.svc"
    # username:
    # password:
    # insecure:
    # client_certificate:
    # client_key:
    cluster_ca_certificate: "${file(\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")}"
    # config_path:
    # config_context:
    # config_context_auth_info:
    # config_context_cluster:
    token: "${file(\"/var/run/secrets/kubernetes.io/serviceaccount/token\")}"
    load_config_file: false
    # exec:
    #   api_version:
    #   command:
    #   args: []
    #   env: {}

  #  If you set it to false, it would set EXCLUDE_NAMESPACE=<release-namespace> and not add the fluentD/fluent-bit logs and Prometheus remotestorage metrics.
  collectionMonitoring: true

  setup:
    clusterRole:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "1"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    clusterRoleBinding:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "2"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    configMap:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "2"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    job:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "3"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    serviceAccount:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "0"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded

    ## Configuration of additional collector fields
    ## https://help.sumologic.com/Manage/Fields#http-source-fields
    fields: {}

  # Logs configuration
  ## Set the enabled flag to false for diabling logs ingestion altogether.
  logs:
    enabled: true

  # Metrics configuration
  ## Set the enabled flag to false for diabling metrics ingestion altogether.
  metrics:
    enabled: true

  ## Configuration of http sources
  ## See docs/Terraform.md for more information
  ## name: source name visible in sumologic platform
  ## config-name: This is mostly for backward compatibility
  sources:
    metrics:
      default:
        name: (default-metrics)
        config-name: endpoint-metrics
      apiserver:
        name: apiserver-metrics
        config-name: endpoint-metrics-apiserver
      controller:
        name: kube-controller-manager-metrics
        config-name: endpoint-metrics-kube-controller-manager
      scheduler:
        name: kube-scheduler-metrics
        config-name: endpoint-metrics-kube-scheduler
      state:
        name: kube-state-metrics
        config-name: endpoint-metrics-kube-state
      kubelet:
        name: kubelet-metrics
        config-name: endpoint-metrics-kubelet
      node:
        name: node-exporter-metrics
        config-name: endpoint-metrics-node-exporter
      control-plane:
        name: control-plane-metrics
    logs:
      default:
        name: logs
        config-name: endpoint-logs
    events:
      default:
        name: events
        config-name: endpoint-events
        category: true
    traces:
      default:
        name: traces
        config-name: endpoint-traces
        properties:
          content_type: Zipkin

  # Traces configuration
  # This is experimental feature and may be unavailable for your account
  traces:
    enabled: false
    # Use filter stdout plugin for tracing data
    # Warning: It will produce additional fluentd logs (one per trace)
    fluentd_stdout: false
    # How many spans per request should be send to receiver
    spans_per_request: 100

fluentd:
  additionalPlugins: []

  ## Sets the fluentd log level. The default log level, if not specified, is info.
  ## Sumo will only ingest the error log level and some specific warnings, the info logs can be seen in kubectl logs.
  ## ref: https://docs.fluentd.org/deployment/logging
  logLevel: "info"
  ## to ingest all fluentd logs, turn the logLevelFilter to false
  logLevelFilter: true

  ## Verify SumoLogic HTTPS certificates
  verifySsl: true
  ## Proxy URI for sumologic output plugin
  proxyUri: ""

  securityContext:
    ## The group ID of all processes in the statefulset containers. By default this needs to be fluent(999).
    fsGroup: 999

  ## Persist data to a persistent volume; When enabled, fluentd uses the file buffer instead of memory buffer.
  persistence:
    ## After setting the value to true, run the helm upgrade command with the --force flag.
    enabled: false

    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, Azure & OpenStack)
    ##
    # storageClass: "-"
    # annotations: {}
    accessMode: ReadWriteOnce
    size: 10Gi

  buffer:
    ## Option to specify the Fluentd buffer as file/memory.
    ## If fluentd.persistence.enabled is true, this will be ignored.
    type: "memory"

    ## How frequently to push logs to SumoLogic
    ## ref: https://github.com/SumoLogic/fluentd-kubernetes-sumologic#options
    flushInterval: "5s"
    ## Increase number of http threads to Sumo. May be required in heavy logging/high DPM clusters
    numThreads: 8
    chunkLimitSize: "1m"
    queueChunkLimitSize: 128
    totalLimitSize: "128m"
    retryMaxInterval: "10m"
    retryForever: true

    ## File paths to buffer to, if Fluentd buffer type is specified as file above.
    ## Each sumologic output plugin buffers to its own unique file.
    filePaths:
      logs:
        containers: /fluentd/buffer/logs.containers
        kubelet: /fluentd/buffer/logs.kubelet
        systemd: /fluentd/buffer/logs.systemd
        default: /fluentd/buffer/logs.default
      metrics:
        apiserver: /fluentd/buffer/metrics.apiserver
        kubelet: /fluentd/buffer/metrics.kubelet
        container: /fluentd/buffer/metrics.container
        controller: /fluentd/buffer/metrics.controller
        scheduler: /fluentd/buffer/metrics.scheduler
        state: /fluentd/buffer/metrics.state
        node: /fluentd/buffer/metrics.node
        control-plane: /fluentd/buffer/metrics.control_plane
        default: /fluentd/buffer/metrics.default
      events: /fluentd/buffer/events
      traces: /fluentd/buffer/traces

    ## Additional config for buffer settings
    extraConf: |-

  # configuration of fluentd monitoring metrics
  # input -> fluentd_input_status_num_records_total (~5% DPM increase for empty cluster)
  # output -> fluentd_output_status_num_records_total (~25% DPM increase for empty cluster)
  monitoring:
    input: false
    output: false

  metadata:
    ## Option to control the enabling of metadata filter plugin cache_size.
    ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
    cacheSize: "10000"
    ## Option to control the enabling of metadata filter plugin cache_ttl (in seconds).
    ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
    cacheTtl: "3600"
    ## Option to control the interval at which metadata cache is asynchronously refreshed (in seconds).
    cacheRefresh: "1800"
    ## Option to give plugin specific log level
    pluginLogLevel: "error"
    ## Option to specify K8s API versions
    coreApiVersions:
      - v1
    ## Option to specify K8s API groups
    apiGroups:
      - apps/v1
      - extensions/v1beta1

  logs:
    enabled: true
    statefulset:
      nodeSelector: {}
      tolerations: {}
      affinity: {}
      ## Acceptable values for podAntiAffinity:
      ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
      ## hard: specifies rules that must be met for a pod to be scheduled onto a node
      podAntiAffinity: "soft"
      replicaCount: 3
      resources:
        limits:
          memory: 1Gi
          cpu: 1
        requests:
          memory: 768Mi
          cpu: 0.5

    ## Option to turn autoscaling on for fluentd and specify params for HPA.
    ## Autoscaling needs metrics-server to access cpu metrics.
    autoscaling:
      enabled: false
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilizationPercentage: 50

    rawConfig: |-
      @include common.conf
      @include logs.conf

    ## Configuration for sumologic output plugin
    ## ref: https://github.com/SumoLogic/fluentd-output-sumologic
    ## ref: https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/master/deploy/helm/sumologic/conf/logs/logs.output.conf
    output:
      ## Format to post logs into Sumo: fields, json, json_merge, or text.
      ## NOTE: for logs metadata, fields is required.
      logFormat: fields
      ## Option to control adding timestamp to logs.
      addTimestamp: true
      ## Field name when add_timestamp is on.
      timestampKey: "timestamp"
      ## Option to give plugin specific log level
      pluginLogLevel: "error"
      ## Additional config parameters for sumologic output plugin
      extraConf: |-

    ## Additional config for custom log pipelines
    ## ref: TODO: documentation for custom logs pipelines
    extraLogs: |-

    ## Container log configuration
    containers:
      ## To override the entire contents of logs.source.containers.conf file. Leave empty for the default pipeline
      overrideRawConfig: |-

      outputConf: |-
        @include logs.output.conf

      ## Override output section for container logs. Leave empty for the default output section
      overrideOutputConf: |-

      ## Set the _sourceName metadata field in Sumo Logic.
      sourceName: "%{namespace}.%{pod}.%{container}"
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "%{namespace}/%{pod_name}"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for containers.
      ## Matching containers will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeContainerRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for namespaces.
      ## Matching namespaces will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeNamespaceRegex: ""
      ## A regular expression for pods.
      ## Matching pods will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePodRegex: ""

      ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
      k8sMetadataFilter:
        ## Option to control the enabling of metadata filter plugin watch.
        watch: "true"
        ## path to CA file for Kubernetes server certificate validation
        caFile: ""
        ## Validate SSL certificates
        verifySsl: true
        ## Path to a client cert file to authenticate to the API server
        clientCert: ""
        ## Path to a client key file to authenticate to the API server
        clientKey: ""
        ## Path to a file containing the bearer token to use for authentication
        bearerTokenFile: ""

      ## To use additional filter plugins
      extraFilterPluginConf: |-

    ## Kubelet log configuration
    kubelet:
      enabled: true
      outputConf: |-
        @include logs.output.conf

      ## Override output section for kubelet logs. Leave empty for the default output section.
      overrideOutputConf: |-

      ## Set the _sourceName metadata field in Sumo Logic.
      sourceName: "k8s_kubelet"
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "kubelet"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for facility.
      ## Matching facility will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeFacilityRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for priority.
      ## Matching priority will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePriorityRegex: ""
      ## A regular expression for unit.
      ## Matching unit will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeUnitRegex: ""

    ## Systemd log configuration
    systemd:
      enabled: true
      outputConf: |-
        @include logs.output.conf

      ## Override output section for systemd logs. Leave empty for the default output section.
      overrideOutputConf: |-

      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "system"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for facility.
      ## Matching facility will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeFacilityRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for priority.
      ## Matching priority will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePriorityRegex: ""
      ## A regular expression for unit.
      ## Matching unit will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeUnitRegex: ""

    ## Default log configuration (catch-all)
    default:
      outputConf: |-
        @include logs.output.conf

      ## Override output section for untagged logs. Leave empty for the default output section.
      overrideOutputConf: |-

    # Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #  - name: VALUE_FROM_SECRET
    #    valueFrom:
    #      secretKeyRef:
    #        name: secret_name
    #        key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

  metrics:
    enabled: true
    statefulset:
      nodeSelector: {}
      tolerations: {}
      affinity: {}
      ## Acceptable values for podAntiAffinity:
      ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
      ## hard: specifies rules that must be met for a pod to be scheduled onto a node
      podAntiAffinity: "soft"
      replicaCount: 3
      resources:
        limits:
          memory: 1Gi
          cpu: 1
        requests:
          memory: 768Mi
          cpu: 0.5

    ## Option to turn autoscaling on for fluentd and specify params for HPA.
    ## Autoscaling needs metrics-server to access cpu metrics.
    autoscaling:
      enabled: false
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilizationPercentage: 50

    rawConfig: |-
      @include common.conf
      @include metrics.conf

    ## Configuration for sumologic output plugin
    ## ref: https://github.com/SumoLogic/fluentd-output-sumologic
    ## ref: https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/master/deploy/helm/sumologic/conf/metrics/metrics.output.conf
    outputConf: |-
      @include metrics.output.conf

    ## To use additional filter plugins
    extraFilterPluginConf: |-

    ## To use additional output plugins
    extraOutputPluginConf: |-

    # Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #  - name: VALUE_FROM_SECRET
    #    valueFrom:
    #      secretKeyRef:
    #        name: secret_name
    #        key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

    ## Output configuration
    ## tag: tag for fluentd match
    ## id: sumologic output @id
    ## endpoint: [optional] [output key by default] It should match the sumologic.sources.metrics[].endpoint
    ## drop: [optional] [false by default] Change it to true to drop traffic for specific output
    ## weight: [optional [0 by default] Order of fluentd output (lower means higher priority)
    ## The weight has meaning in case of fluentd matches
    output:
      apiserver:
        tag: prometheus.metrics.apiserver**
        id: sumologic.endpoint.metrics.apiserver
        weight: 90
      kubelet:
        tag: prometheus.metrics.kubelet**
        id: sumologic.endpoint.metrics.kubelet
        weight: 90
      container:
        tag: prometheus.metrics.container**
        id: sumologic.endpoint.metrics.container
        source: kubelet
        weight: 90
      controller:
        tag: prometheus.metrics.controller-manager**
        id: sumologic.endpoint.metrics.kube.controller.manager
        weight: 90
      scheduler:
        tag: prometheus.metrics.scheduler**
        id: sumologic.endpoint.metrics.kube.scheduler
        weight: 90
      state:
        tag: prometheus.metrics.state**
        id: sumologic.endpoint.metrics.kube.state
        weight: 90
      node:
        tag: prometheus.metrics.node**
        id: sumologic.endpoint.metrics.node.exporter
        weight: 90
      control-plane:
        tag: prometheus.metrics.control-plane**
        id: sumologic.endpoint.metrics.control.plane
        weight: 90
      default:
        tag: prometheus.metrics**
        id: sumologic.endpoint.metrics
        weight: 100

  events:
    # If enabled, collect K8s events
    enabled: true

    statefulset:
      nodeSelector: {}
      tolerations: {}
      resources:
        limits:
          memory: 512Mi
          cpu: "200m"
        requests:
          memory: 256Mi
          cpu: "100m"

    # Source category for the Events source. Default: "{clusterName}/events"
    sourceCategory: ""
    ## Override Kubernetes resource types you want to get events for from different Kubernetes
    ## API versions. The key represents the name of the resource type and the value represents
    ## the API version.
    # watchResourceEventsOverrides:
    #   pods: "v1"
    #   events: "events.k8s.io/v1beta1"

    # Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #  - name: VALUE_FROM_SECRET
    #    valueFrom:
    #      secretKeyRef:
    #        name: secret_name
    #        key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

## Configure metrics-server
## ref: https://github.com/helm/charts/blob/master/stable/metrics-server/values.yaml
metrics-server:
  ## Set the enabled flag to true for enabling metrics-server.
  ## This is required before enabling fluentd autoscaling unless you have an existing metrics-server in the cluster.
  enabled: false
  args:
    - --kubelet-insecure-tls
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname

## Configure fluent-bit
## ref: https://github.com/helm/charts/blob/master/stable/fluent-bit/values.yaml
fluent-bit:
  ## Resource limits for fluent-bit
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 10m
    #   memory: 8Mi

  ## Uncomment the flag below to not install fluent-bit helm chart as a dependency along with this helm chart.
  ## Comment it out if you wish to have the fluent-bit dependency installed.
  ## Setting the flag to true instead of commenting out will break other functionality.
  # enabled: false

  service:
    flush: 5
  metrics:
    enabled: true
  env:
    - name: CHART
      valueFrom:
        configMapKeyRef:
          name: sumologic-configmap
          key: fluentdLogs
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace

  backend:
    type: forward
    forward:
      # NOTE: Requires trailing "." for fully-qualified name resolution
      host: ${CHART}.${NAMESPACE}.svc.cluster.local.
      port: 24321
      tls: "off"
      tls_verify: "on"
      tls_debug: 1
      shared_key:

  trackOffsets: true

  tolerations:
    - effect: NoSchedule
      operator: Exists

  input:
    systemd:
      enabled: true

  parsers:
    enabled: true
    # This regex matches the first line of a multiline log starting with a date of the format :  "2019-11-17 07:14:12" or "2019-11-17T07:14:12"
    regex:
      - name: multi_line
        regex: (?<log>^{"log":"\d{4}-\d{1,2}-\d{1,2}.\d{2}:\d{2}:\d{2}.*)

  rawConfig: |-
    @INCLUDE fluent-bit-service.conf

    [INPUT]
      Name             tail
      Path             /var/log/containers/*.log
      Multiline        On
      Parser_Firstline multi_line
      Tag              containers.*
      Refresh_Interval 1
      Rotate_Wait      60
      Mem_Buf_Limit    5MB
      Skip_Long_Lines  On
      DB               /tail-db/tail-containers-state-sumo.db
      DB.Sync          Normal
      Ignore_Older     24h
    [INPUT]
      Name            systemd
      Tag             host.*
      DB              /tail-db/systemd-state-sumo.db
      Systemd_Filter  _SYSTEMD_UNIT=addon-config.service
      Systemd_Filter  _SYSTEMD_UNIT=addon-run.service
      Systemd_Filter  _SYSTEMD_UNIT=cfn-etcd-environment.service
      Systemd_Filter  _SYSTEMD_UNIT=cfn-signal.service
      Systemd_Filter  _SYSTEMD_UNIT=clean-ca-certificates.service
      Systemd_Filter  _SYSTEMD_UNIT=containerd.service
      Systemd_Filter  _SYSTEMD_UNIT=coreos-metadata.service
      Systemd_Filter  _SYSTEMD_UNIT=coreos-setup-environment.service
      Systemd_Filter  _SYSTEMD_UNIT=coreos-tmpfiles.service
      Systemd_Filter  _SYSTEMD_UNIT=dbus.service
      Systemd_Filter  _SYSTEMD_UNIT=docker.service
      Systemd_Filter  _SYSTEMD_UNIT=efs.service
      Systemd_Filter  _SYSTEMD_UNIT=etcd-member.service
      Systemd_Filter  _SYSTEMD_UNIT=etcd.service
      Systemd_Filter  _SYSTEMD_UNIT=etcd2.service
      Systemd_Filter  _SYSTEMD_UNIT=etcd3.service
      Systemd_Filter  _SYSTEMD_UNIT=etcdadm-check.service
      Systemd_Filter  _SYSTEMD_UNIT=etcdadm-reconfigure.service
      Systemd_Filter  _SYSTEMD_UNIT=etcdadm-save.service
      Systemd_Filter  _SYSTEMD_UNIT=etcdadm-update-status.service
      Systemd_Filter  _SYSTEMD_UNIT=flanneld.service
      Systemd_Filter  _SYSTEMD_UNIT=format-etcd2-volume.service
      Systemd_Filter  _SYSTEMD_UNIT=kube-node-taint-and-uncordon.service
      Systemd_Filter  _SYSTEMD_UNIT=kubelet.service
      Systemd_Filter  _SYSTEMD_UNIT=ldconfig.service
      Systemd_Filter  _SYSTEMD_UNIT=locksmithd.service
      Systemd_Filter  _SYSTEMD_UNIT=logrotate.service
      Systemd_Filter  _SYSTEMD_UNIT=lvm2-monitor.service
      Systemd_Filter  _SYSTEMD_UNIT=mdmon.service
      Systemd_Filter  _SYSTEMD_UNIT=nfs-idmapd.service
      Systemd_Filter  _SYSTEMD_UNIT=nfs-mountd.service
      Systemd_Filter  _SYSTEMD_UNIT=nfs-server.service
      Systemd_Filter  _SYSTEMD_UNIT=nfs-utils.service
      Systemd_Filter  _SYSTEMD_UNIT=node-problem-detector.service
      Systemd_Filter  _SYSTEMD_UNIT=ntp.service
      Systemd_Filter  _SYSTEMD_UNIT=oem-cloudinit.service
      Systemd_Filter  _SYSTEMD_UNIT=rkt-gc.service
      Systemd_Filter  _SYSTEMD_UNIT=rkt-metadata.service
      Systemd_Filter  _SYSTEMD_UNIT=rpc-idmapd.service
      Systemd_Filter  _SYSTEMD_UNIT=rpc-mountd.service
      Systemd_Filter  _SYSTEMD_UNIT=rpc-statd.service
      Systemd_Filter  _SYSTEMD_UNIT=rpcbind.service
      Systemd_Filter  _SYSTEMD_UNIT=set-aws-environment.service
      Systemd_Filter  _SYSTEMD_UNIT=system-cloudinit.service
      Systemd_Filter  _SYSTEMD_UNIT=systemd-timesyncd.service
      Systemd_Filter  _SYSTEMD_UNIT=update-ca-certificates.service
      Systemd_Filter  _SYSTEMD_UNIT=user-cloudinit.service
      Systemd_Filter  _SYSTEMD_UNIT=var-lib-etcd2.service
      Max_Entries     1000
      Read_From_Tail  true

    @INCLUDE fluent-bit-output.conf

## Configure prometheus-operator
## ref: https://github.com/helm/charts/blob/master/stable/prometheus-operator/values.yaml
prometheus-operator:
  ## NOTE changing the serviceMonitor scrape interval to be >1m can result in metrics from recording rules to be missing and empty panels in Sumo Logic Kubernetes apps.
  kubeApiServer:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
  kubelet:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
  kubeControllerManager:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
  coreDns:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
  kubeEtcd:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
  kubeScheduler:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
  kubeStateMetrics:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
  nodeExporter:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
  # Ensure we use pre 1.14 recording rules consistently as current content depends on them.
  kubeTargetVersionOverride: 1.13.0-0

  ## Uncomment the flag below to not install prometheus-operator helm chart as a dependency along with this helm chart.
  ## Comment it out if you wish to have the prometheus-operator dependency installed.
  ## Setting the flag to true instead of commenting out will break other functionality.
  # enabled: false

  alertmanager:
    enabled: false
  grafana:
    enabled: false
    defaultDashboardsEnabled: false
  prometheusOperator:
    ## Resource limits for prometheus operator
    resources: {}
      # limits:
      #   cpu: 200m
      #   memory: 200Mi
      # requests:
      #   cpu: 100m
      #   memory: 100Mi
    admissionWebhooks:
      enabled: false
    tlsProxy:
      enabled: false
    ## Resource limits for kube-state-metrics
    kube-state-metrics:
      resources: {}
        # limits:
        #   cpu: 200m
        #   memory: 200Mi
        # requests:
        #   cpu: 200m
        #   memory: 200Mi
    ## Resource limits for prometheus node exporter
    prometheus-node-exporter:
      resources: {}
        # limits:
        #   cpu: 200m
        #   memory: 50Mi
        # requests:
        #   cpu: 100m
        #   memory: 30Mi
  prometheus:
    additionalServiceMonitors:
      - name: collection-sumologic-fluentd-logs
        additionalLabels:
          sumologic.com/app: fluentd-logs
        endpoints:
        - port: metrics
        namespaceSelector:
          matchNames:
          - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-logs
      - name: collection-sumologic-fluentd-metrics
        additionalLabels:
          sumologic.com/app: fluentd-metrics
        endpoints:
        - port: metrics
        namespaceSelector:
          matchNames:
          - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-metrics
      - name: collection-sumologic-fluentd-events
        additionalLabels:
          sumologic.com/app: fluentd-events
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-events
      - name: collection-fluent-bit
        additionalLabels:
          app: collection-fluent-bit
        endpoints:
          - port: metrics
            path: /api/v1/metrics/prometheus
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            app: fluent-bit
      - name: collection-sumologic-otelcol
        additionalLabels:
          sumologic.com/app: otelcol
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: otelcol
    prometheusSpec:
      ## Prometheus default scrape interval, default from upstream Prometheus Operator Helm chart
      ## NOTE changing the scrape interval to be >1m can result in metrics from recording rules to be missing and empty panels in Sumo Logic Kubernetes apps.
      scrapeInterval: "30s"
      ## Define resources requests and limits for single Pods.
      resources: {}
      # limits:
      #   cpu: 800m
      #   memory: 800Mi
      # requests:
      #   cpu: 400m
      #   memory: 400Mi
      thanos:
        baseImage: quay.io/thanos/thanos
        version: v0.10.0
      containers:
      - name: "prometheus-config-reloader"
        env:
        - name: CHART
          valueFrom:
            configMapKeyRef:
              name: sumologic-configmap
              key: fluentdMetrics
        - name: NAMESPACE
          valueFrom:
            configMapKeyRef:
              name: sumologic-configmap
              key: fluentdNamespace

      ## Enable WAL compression to reduce Prometheus memory consumption
      walCompression: true

      remoteWrite:
        # kube state metrics
        # kube_daemonset_status_current_number_scheduled
        # kube_daemonset_status_desired_number_scheduled
        # kube_daemonset_status_number_misscheduled
        # kube_daemonset_status_number_unavailable
        # kube_deployment_spec_replicas
        # kube_deployment_status_replicas_available
        # kube_deployment_status_replicas_unavailable
        # kube_node_info
        # kube_node_status_allocatable
        # kube_node_status_capacity
        # kube_node_status_condition
        # kube_pod_container_info
        # kube_pod_container_resource_limits
        # kube_pod_container_resource_requests
        # kube_pod_container_status_ready
        # kube_pod_container_status_restarts_total
        # kube_pod_container_status_terminated_reason
        # kube_pod_container_status_waiting_reason
        # kube_pod_status_phase
        # kube_statefulset_metadata_generation
        # kube_statefulset_replicas
        # kube_statefulset_status_observed_generation
        # kube_statefulset_status_replicas
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.state
          writeRelabelConfigs:
            - action: keep
              regex: kube-state-metrics;(?:kube_statefulset_status_observed_generation|kube_statefulset_status_replicas|kube_statefulset_replicas|kube_statefulset_metadata_generation|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_number_unavailable|kube_deployment_spec_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_node_info|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_info|kube_pod_container_resource_requests|kube_pod_container_resource_limits|kube_pod_container_status_ready|kube_pod_container_status_terminated_reason|kube_pod_container_status_waiting_reason|kube_pod_container_status_restarts_total|kube_pod_status_phase)
              sourceLabels: [job, __name__]
        # controller manager metrics
        # https://kubernetes.io/docs/concepts/cluster-administration/monitoring/#kube-controller-manager-metrics
        # e.g.
        # cloudprovider_aws_api_request_duration_seconds_bucket
        # cloudprovider_aws_api_request_duration_seconds_count
        # cloudprovider_aws_api_request_duration_seconds_sum
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.controller-manager
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;cloudprovider_.*_api_request_duration_seconds.*
              sourceLabels: [job, __name__]
        # scheduler metrics_latency_microseconds
        # scheduler_e2e_scheduling_duration_seconds
        # scheduler_e2e_scheduling_duration_seconds_bucket
        # scheduler_e2e_scheduling_duration_seconds_count
        # scheduler_binding_duration_seconds
        # scheduler_binding_duration_seconds_bucket
        # scheduler_binding_duration_seconds_count
        # scheduler_scheduling_algorithm_duration_seconds
        # scheduler_scheduling_algorithm_duration_seconds_bucket
        # scheduler_scheduling_algorithm_duration_seconds_count
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.scheduler
          writeRelabelConfigs:
            - action: keep
              regex: kube-scheduler;scheduler_(?:e2e_scheduling|binding|scheduling_algorithm)_duration_seconds.*
              sourceLabels: [job, __name__]
        # api server metrics:
        # apiserver_request_count
        # apiserver_request_total
        # apiserver_request_duration_seconds_count
        # apiserver_request_duration_seconds_sum
        # apiserver_request_latencies_count
        # apiserver_request_latencies_sum
        # apiserver_request_latencies_summary
        # apiserver_request_latencies_summary_count
        # apiserver_request_latencies_summary_sum
        # etcd_request_cache_get_duration_seconds_count
        # etcd_request_cache_get_duration_seconds_sum
        # etcd_request_cache_add_duration_seconds_count
        # etcd_request_cache_add_duration_seconds_sum
        # etcd_request_cache_add_latencies_summary_count
        # etcd_request_cache_add_latencies_summary_sum
        # etcd_request_cache_get_latencies_summary_count
        # etcd_request_cache_get_latencies_summary_sum
        # etcd_helper_cache_hit_count
        # etcd_helper_cache_hit_total
        # etcd_helper_cache_miss_count
        # etcd_helper_cache_miss_total
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.apiserver
          writeRelabelConfigs:
            - action: keep
              regex: apiserver;(?:apiserver_request_(?:count|total)|apiserver_request_(?:duration_seconds|latencies)_(?:count|sum)|apiserver_request_latencies_summary(?:|_count|_sum)|etcd_request_cache_(?:add|get)_(?:duration_seconds|latencies_summary)_(?:count|sum)|etcd_helper_cache_(?:hit|miss)_(?:count|total))
              sourceLabels: [job, __name__]
        # kubelet metrics:
        # kubelet_docker_operations_errors
        # kubelet_docker_operations_errors_total
        # kubelet_docker_operations_duration_seconds_count
        # kubelet_docker_operations_duration_seconds_sum
        # kubelet_runtime_operations_duration_seconds_count
        # kubelet_runtime_operations_duration_seconds_sum
        # kubelet_running_container_count
        # kubelet_running_pod_count
        # kubelet_docker_operations_latency_microseconds
        # kubelet_docker_operations_latency_microseconds_count
        # kubelet_docker_operations_latency_microseconds_sum
        # kubelet_runtime_operations_latency_microseconds
        # kubelet_runtime_operations_latency_microseconds_count
        # kubelet_runtime_operations_latency_microseconds_sum
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.kubelet
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;(?:kubelet_docker_operations_errors(?:|_total)|kubelet_(?:docker|runtime)_operations_duration_seconds_(?:count|sum)|kubelet_running_(?:container|pod)_count|kubelet_(:?docker|runtime)_operations_latency_microseconds(?:|_count|_sum))
              sourceLabels: [job, __name__]
        # cadvisor container metrics
        # container_cpu_usage_seconds_total
        # container_fs_limit_bytes
        # container_fs_usage_bytes
        # container_memory_working_set_bytes
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.container
          writeRelabelConfigs:
            - action: labelmap
              regex: container_name
              replacement: container
            - action: drop
              regex: POD
              sourceLabels: [container]
            - action: keep
              regex: kubelet;.+;(?:container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_fs_usage_bytes|container_fs_limit_bytes)
              sourceLabels: [job,container,__name__]
        # cadvisor aggregate container metrics
        # container_network_receive_bytes_total
        # container_network_transmit_bytes_total
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.container
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;(?:container_network_receive_bytes_total|container_network_transmit_bytes_total)
              sourceLabels: [job, __name__]
        # node exporter metrics
        # node_cpu_seconds_total
        # node_load1
        # node_load5
        # node_load15
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.node
          writeRelabelConfigs:
            - action: keep
              regex: node-exporter;(?:node_load1|node_load5|node_load15|node_cpu_seconds_total)
              sourceLabels: [job, __name__]
        # prometheus operator rules
        # :kube_pod_info_node_count:
        # :node_cpu_saturation_load1:
        # :node_cpu_utilisation:avg1m
        # :node_disk_saturation:avg_irate
        # :node_disk_utilisation:avg_irate
        # :node_memory_swap_io_bytes:sum_rate
        # :node_memory_utilisation:
        # :node_net_saturation:sum_irate
        # :node_net_utilisation:sum_irate
        # cluster_quantile:apiserver_request_latencies:histogram_quantile
        # cluster_quantile:scheduler_binding_latency:histogram_quantile
        # cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile
        # cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile
        # instance:node_filesystem_usage:sum
        # instance:node_network_receive_bytes:rate:sum
        # node:cluster_cpu_utilisation:ratio
        # node:cluster_memory_utilisation:ratio
        # node:node_cpu_saturation_load1:
        # node:node_cpu_utilisation:avg1m
        # node:node_disk_saturation:avg_irate
        # node:node_disk_utilisation:avg_irate
        # node:node_filesystem_avail:
        # node:node_filesystem_usage:
        # node:node_inodes_free:
        # node:node_inodes_total:
        # node:node_memory_bytes_total:sum
        # node:node_memory_swap_io_bytes:sum_rate
        # node:node_memory_utilisation:
        # node:node_memory_utilisation:ratio
        # node:node_memory_utilisation_2:
        # node:node_net_saturation:sum_irate
        # node:node_net_utilisation:sum_irate
        # node:node_num_cpu:sum
        # node_namespace_pod:kube_pod_info:
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.operator.rule
          writeRelabelConfigs:
            - action: keep
              regex: 'cluster_quantile:apiserver_request_latencies:histogram_quantile|instance:node_filesystem_usage:sum|instance:node_network_receive_bytes:rate:sum|cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile|cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile|cluster_quantile:scheduler_binding_latency:histogram_quantile|node_namespace_pod:kube_pod_info:|:kube_pod_info_node_count:|node:node_num_cpu:sum|:node_cpu_utilisation:avg1m|node:node_cpu_utilisation:avg1m|node:cluster_cpu_utilisation:ratio|:node_cpu_saturation_load1:|node:node_cpu_saturation_load1:|:node_memory_utilisation:|node:node_memory_bytes_total:sum|node:node_memory_utilisation:ratio|node:cluster_memory_utilisation:ratio|:node_memory_swap_io_bytes:sum_rate|node:node_memory_utilisation:|node:node_memory_utilisation_2:|node:node_memory_swap_io_bytes:sum_rate|:node_disk_utilisation:avg_irate|node:node_disk_utilisation:avg_irate|:node_disk_saturation:avg_irate|node:node_disk_saturation:avg_irate|node:node_filesystem_usage:|node:node_filesystem_avail:|:node_net_utilisation:sum_irate|node:node_net_utilisation:sum_irate|:node_net_saturation:sum_irate|node:node_net_saturation:sum_irate|node:node_inodes_total:|node:node_inodes_free:'
              sourceLabels: [__name__]
        # health
        # fluentbit_input_bytes_total
        # fluentbit_input_files_closed_total
        # fluentbit_input_files_opened_total
        # fluentbit_input_files_rotated_total
        # fluentbit_input_records_total
        # fluentbit_output_errors_total
        # fluentbit_output_proc_bytes_total
        # fluentbit_output_proc_records_total
        # fluentbit_output_retries_failed_total
        # fluentbit_output_retries_total
        # fluentd_output_status_buffer_available_space_ratio
        # fluentd_output_status_buffer_queue_length
        # fluentd_output_status_buffer_stage_byte_size
        # fluentd_output_status_buffer_stage_length
        # fluentd_output_status_buffer_total_bytes
        # fluentd_output_status_emit_count
        # fluentd_output_status_emit_records
        # fluentd_output_status_flush_time_count
        # fluentd_output_status_num_errors
        # fluentd_output_status_queue_byte_size
        # fluentd_output_status_retry_count
        # fluentd_output_status_retry_wait
        # fluentd_output_status_rollback_count
        # fluentd_output_status_slow_flush_count
        # fluentd_output_status_write_count
        # otelcol_otelsvc_k8s_other_added
        # otelcol_otelsvc_k8s_other_updated
        # otelcol_otelsvc_k8s_pod_added
        # otelcol_otelsvc_k8s_pod_deleted
        # otelcol_otelsvc_k8s_pod_updated
        # otelcol_process_cpu_seconds
        # otelcol_process_runtime_heap_alloc_bytes
        # otelcol_process_runtime_total_alloc_bytes
        # otelcol_process_runtime_total_sys_memory_bytes
        # otelcol_queue_length
        # otelcol_spans_dropped
        # otelcol_trace_batches_dropped
        # prometheus_remote_storage_dropped_samples_total
        # prometheus_remote_storage_enqueue_retries_total
        # prometheus_remote_storage_failed_samples_total
        # prometheus_remote_storage_highest_timestamp_in_seconds
        # prometheus_remote_storage_pending_samples
        # prometheus_remote_storage_queue_highest_sent_timestamp_seconds
        # prometheus_remote_storage_retried_samples_total
        # prometheus_remote_storage_samples_in_total
        # prometheus_remote_storage_sent_batch_duration_seconds_bucket
        # prometheus_remote_storage_sent_batch_duration_seconds_count
        # prometheus_remote_storage_sent_batch_duration_seconds_sum
        # prometheus_remote_storage_shard_capacity
        # prometheus_remote_storage_shards
        # prometheus_remote_storage_shards_desired
        # prometheus_remote_storage_shards_max
        # prometheus_remote_storage_shards_min
        # prometheus_remote_storage_string_interner_zero_reference_releases_total
        # prometheus_remote_storage_succeeded_samples_total
        # up
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics
          writeRelabelConfigs:
            - action: keep
              regex: (?:up|prometheus_remote_storage_.*|fluentd_.*|fluentbit.*|otelcol.*)
              sourceLabels: [__name__]
        # control plane metrics
        # coredns:
        # coredns_cache_size
        # coredns_cache_hits_total
        # coredns_cache_misses_total
        # coredns_dns_request_duration_seconds_count
        # coredns_dns_request_duration_seconds_sum
        # coredns_dns_request_count_total
        # coredns_dns_response_rcode_count_total
        # coredns_forward_request_count_total
        # process_cpu_seconds_total
        # process_open_fds
        # process_resident_memory_bytes
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.control-plane.coredns
          writeRelabelConfigs:
            - action: keep
              regex: coredns;(?:coredns_cache_(size|(hits|misses)_total)|coredns_dns_request_duration_seconds_(count|sum)|coredns_(dns_request|dns_response_rcode|forward_request)_count_total|process_(cpu_seconds_total|open_fds|resident_memory_bytes))
              sourceLabels: [job, __name__]
        # etcd server:
        # etcd_debugging_mvcc_db_total_size_in_bytes
        # etcd_debugging_store_expires_total
        # etcd_debugging_store_watchers
        # etcd_disk_backend_commit_duration_seconds_bucket
        # etcd_disk_wal_fsync_duration_seconds_bucket
        # etcd_grpc_proxy_cache_hits_total
        # etcd_grpc_proxy_cache_misses_total
        # etcd_network_client_grpc_received_bytes_total
        # etcd_network_client_grpc_sent_bytes_total
        # etcd_server_has_leader
        # etcd_server_leader_changes_seen_total
        # etcd_server_proposals_applied_total
        # etcd_server_proposals_committed_total
        # etcd_server_proposals_failed_total
        # etcd_server_proposals_pending
        # process_cpu_seconds_total
        # process_open_fds
        # process_resident_memory_bytes
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.control-plane.kube-etcd
          writeRelabelConfigs:
            - action: keep
              regex: kube-etcd;(?:etcd_debugging_(mvcc_db_total_size_in_bytes|store_(expires_total|watchers))|etcd_disk_(backend_commit|wal_fsync)_duration_seconds_bucket|etcd_grpc_proxy_cache_(hits|misses)_total|etcd_network_client_grpc_(received|sent)_bytes_total|etcd_server_(has_leader|leader_changes_seen_total)|etcd_server_proposals_(pending|(applied|committed|failed)_total)|process_(cpu_seconds_total|open_fds|resident_memory_bytes))
              sourceLabels: [job, __name__]

## Configure falco
## ref: https://github.com/falcosecurity/charts/blob/master/falco/values.yaml
falco:
  ## Set the enabled flag to true to enable falco.
  enabled: false
  #ebpf:
  #  enabled: true
  falco:
    jsonOutput: true

##Configure otelcol
otelcol:
  deployment:
    nodeSelector: {}
    tolerations: {}
    replicas: 1
    resources:
      limits:
        memory: 2Gi
        cpu: 1
      requests:
        memory: 384Mi
        cpu: "200m"
    # Memory Ballast size should be max 1/3 to 1/2 of memory.
    memBallastSizeMib: "683"
    image:
      name: "sumologic/opentelemetry-collector"
      tag: "0.4.0.0"
      pullPolicy: IfNotPresent
  config:
    receivers:
      jaeger:
        protocols:
          thrift_compact:
            endpoint: "0.0.0.0:6831"
          thrift_binary:
            endpoint: "0.0.0.0:6832"
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_http:
            endpoint: "0.0.0.0:14268"
      opencensus:
        endpoint: "0.0.0.0:55678"
      otlp:
        endpoint: "0.0.0.0:55680"
      zipkin:
        endpoint: "0.0.0.0:9411"
    processors:
      # Tags spans with K8S metadata, basing on the context IP
      k8s_tagger:
        # When true, only IP is assigned and passed (so it could be tagged on another collector)
        passthrough: false
        # When true, additional fields, such as serviceName are being also extracted
        owner_lookup_enabled: true
        # Extracted fields and assigned names
        extract:
          metadata:
            # extract the following well-known metadata fields
            - containerId
            - containerName
            - clusterName
            - daemonSetName
            - deploymentName
            - hostName
            - namespace
            - nodeName
            - podId
            - podName
            - replicaSetName
            - serviceName
            - statefulSetName
          annotations:
            - tag_name: "k8s.pod.annotation.%s"
              key: "*"
          namespace_labels:
            - tag_name: "k8s.namespace.label.%s"
              key: "*"
          labels:
            - tag_name: "k8s.pod.label.%s"
              key: "*"
      # Source processor adds Sumo Logic related metadata
      source:
        collector: "processors.source.collector.replace"
        source_name: "processors.source.name.replace"
        source_category: "processors.source.category.replace"
        source_category_prefix: "processors.source.category_prefix.replace"
        source_category_replace_dash: "processors.source.category_replace_dash.replace"
        exclude_namespace_regex: "processors.source.exclude_namespace_regex.replace"
        exclude_pod_regex: "processors.source.exclude_pod_regex.replace"
        exclude_container_regex: "processors.source.exclude_container_regex.replace"
        exclude_host_regex: "processors.source.exclude_host_regex.replace"
        annotation_prefix: "k8s.pod.annotation."
        pod_template_hash_key: "k8s.pod.label.pod-template-hash"
        pod_name_key: "k8s.pod.pod_name"
        namespace_key: "k8s.namespace.name"
        pod_key: "k8s.pod.name"
        container_key: "k8s.container.name"
        source_host_key: "k8s.pod.hostname"

      # Resource processor sets the associted cluster attribute
      resource:
        labels: {
          k8s.cluster.name: "processors.resource.cluster.replace"
        }

      # The memory_limiter processor is used to prevent out of memory situations on the collector.
      memory_limiter:
        # check_interval is the time between measurements of memory usage for the
        # purposes of avoiding going over the limits. Defaults to zero, so no
        # checks will be performed. Values below 1 second are not recommended since
        # it can result in unnecessary CPU consumption.
        check_interval: 5s

        # Maximum amount of memory, in MiB, targeted to be allocated by the process heap.
        # Note that typically the total memory usage of process will be about 50MiB higher
        # than this value.
        limit_mib: 1900

      # The queued_retry processor uses a bounded queue to relay batches from the receiver or previous
      # processor to the next processor.
      queued_retry:
        # Number of workers that dequeue batches
        num_workers: 16
        # Maximum number of batches kept in memory before data is dropped
        queue_size: 10000
        # Whether to retry on failure or give up and drop
        retry_on_failure: true

      # The batch processor accepts spans and places them into batches grouped by node and resource
      batch:
        # Number of spans after which a batch will be sent regardless of time
        send_batch_size: 256
        # Never more than this many spans are being sent in a batch
        send_batch_hard_limit: 512
        # Time duration after which a batch will be sent regardless of size
        timeout: 5s
    extensions:
      health_check: {}
    exporters:
      zipkin:
        url: ${SUMO_ENDPOINT_DEFAULT_TRACES_SOURCE}
      # Following generates verbose logs with span content, useful to verify what
      # metadata is being tagged. To enable, uncomment and add "logging" to exporters below.
      # There are two levels that could be used: `debug` and `info` with the former
      # being much more verbose and including (sampled) spans content
      # logging:
      #   loglevel: debug
    service:
      extensions: [health_check]
      pipelines:
        traces:
          receivers: [jaeger, opencensus, otlp, zipkin]
          processors: [memory_limiter, k8s_tagger, source, resource, batch, queued_retry]
          exporters: [zipkin]
