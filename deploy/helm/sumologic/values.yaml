## Sumo Logic Kubernetes Collection configuration file
## All the comments start with two or more # characters

nameOverride: ""
fullnameOverride: ""

sumologic:
  ### Setup

  ## If enabled, a pre-install hook will create Collector and Sources in Sumo Logic
  setupEnabled: true

  ## If enabled, a pre-delete hook will destroy Collector in Sumo Logic
  cleanupEnabled: false

  ## If enabled, accessId and accessKey will be sourced from Secret Name given
  ## Be sure to include at least the following env variables in your secret
  ## (1) SUMOLOGIC_ACCESSID, (2) SUMOLOGIC_ACCESSKEY
  # envFromSecret: sumo-api-secret

  ## Sumo access ID
  # accessId: ""

  ## Sumo access key
  # accessKey: ""

  ## Sumo API endpoint; Leave blank for automatic endpoint discovery and redirection
  ## ref: https://help.sumologic.com/docs/api/getting-started#sumo-logic-endpoints-by-deployment-and-firewall-security
  endpoint: ""

  ## proxy urls
  httpProxy: ""
  httpsProxy: ""
  ## Exclude Kubernetes internal traffic from proxy
  noProxy: kubernetes.default.svc

  ## Collector name
  # collectorName: ""

  ## Cluster name: Note spaces are not allowed and will be replaced with dashes.
  clusterName: "kubernetes"

  ## Configuration of Kubernetes for Terraform client
  ## https://www.terraform.io/docs/providers/kubernetes/index.html#argument-reference
  ## All double quotes should be escaped here regarding Terraform syntax
  cluster:
    host: "https://kubernetes.default.svc"
    # username:
    # password:
    # insecure:
    # client_certificate:
    # client_key:
    cluster_ca_certificate: "${file(\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")}"
    # config_path:
    # config_context:
    # config_context_auth_info:
    # config_context_cluster:
    token: "${file(\"/var/run/secrets/kubernetes.io/serviceaccount/token\")}"
    # exec:
    #   api_version:
    #   command:
    #   args: []
    #   env: {}

  ## If you set it to false, it would set EXCLUDE_NAMESPACE=<release-namespace>
  ## and not add the fluentD/fluent-bit logs and Prometheus remotestorage metrics.
  collectionMonitoring: true

  ## Optionally specify an array of pullSecrets.
  ## They will be added to serviceaccount that is used for Sumo Logic's
  ## deployments and statefulsets.

  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - name: myRegistryKeySecretName

  ## Add custom labels to the following sumologic resources(fluentd sts, setup job, otelcol deployment)
  podLabels: {}

  ## Add custom annotations to the following sumologic resources(fluentd sts, setup job, otelcol deployment)
  podAnnotations: {}

  ## Add custom annotations to sumologic serviceAccounts
  serviceAccount:
    annotations: {}

  ## creation of Security Context Constraints in Openshift
  scc:
    create: false

  setup:
    ## uncomment to force collection installation (disables k8s version verification)
    # force: true
    job:
      image:
        repository: public.ecr.aws/sumologic/kubernetes-setup
        tag: 3.4.0
        pullPolicy: IfNotPresent
      ## Optionally specify an array of pullSecrets.
      ## They will be added to serviceaccount that is used for Sumo Logic's
      ## setup job.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ##
      # pullSecrets:
      #   - name: myRegistryKeySecretName
      resources:
        limits:
          memory: 256Mi
          cpu: 2000m
        requests:
          memory: 64Mi
          cpu: 200m
      nodeSelector: {}
      ## Add custom labels only to setup job pod

      ## Node tolerations for server scheduling to nodes with taints
      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      tolerations: []
      # - key: null
      #   operator: Exists
      #   effect: "NoSchedule"

      ## Affinity and anti-affinity
      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      ##
      affinity: {}

      podLabels: {}
      ## Add custom annotations only to setup job pod
      podAnnotations: {}

    ## uncomment for the debug mode (disables the automatic run of the setup.sh script)
    # debug: true

    monitors:
      ## If enabled, a pre-install hook will create k8s monitors in Sumo Logic
      enabled: true

      ## The installed monitors default status: enabled/disabled
      monitorStatus: enabled

      ## A list of emails to send notifications from monitors
      notificationEmails: []

    dashboards:
      ## If enabled, a pre-install hook will install k8s dashboards in Sumo Logic
      enabled: true

  collector:
    ## Configuration of additional collector fields
    ## https://help.sumologic.com/docs/manage/fields/#http-source-fields
    fields: {}

    ## Configuration of http sources
    ## See docs/Terraform.md for more information
    ## name: source name visible in sumologic platform
    ## config-name: This is mostly for backward compatibility
    sources:
      metrics:
        default:
          name: (default-metrics)
          config-name: endpoint-metrics
        apiserver:
          name: apiserver-metrics
          config-name: endpoint-metrics-apiserver
        controller:
          name: kube-controller-manager-metrics
          config-name: endpoint-metrics-kube-controller-manager
        scheduler:
          name: kube-scheduler-metrics
          config-name: endpoint-metrics-kube-scheduler
        state:
          name: kube-state-metrics
          config-name: endpoint-metrics-kube-state
        kubelet:
          name: kubelet-metrics
          config-name: endpoint-metrics-kubelet
        node:
          name: node-exporter-metrics
          config-name: endpoint-metrics-node-exporter
        control-plane:
          name: control-plane-metrics
      logs:
        default:
          name: logs
          config-name: endpoint-logs

          ## Properties can be used to extend default settings, such as processing rules, fields etc
          # properties:
          #  filters:
          #    - name: "Test Exclude Debug"
          #      filter_type: "Exclude"
          #      regexp: ".*DEBUG.*"
      events:
        default:
          name: events
          config-name: endpoint-events
      traces:
        default:
          name: traces
          config-name: endpoint-traces
          properties:
            content_type: Zipkin

  ### Configuration for collection of Kubernetes events
  events:
    provider: fluentd

  ### Logs configuration
  ## Set the enabled flag to false for disabling logs ingestion altogether.
  logs:
    enabled: true
    metadata:
      ## Set provider service (either fluentd or otelcol).
      provider: fluentd

    collector:
      otelcol:
        enabled: false

      ## Allow running otel and Fluent-Bit side by side. This will result in duplicated
      ## logs being ingested. Only enabled this if you're **certain** it's what you want.
      allowSideBySide: false

    multiline:
      enabled: true
      first_line_regex: "^\\[?\\d{4}-\\d{1,2}-\\d{1,2}.\\d{2}:\\d{2}:\\d{2}"

    container:
      enabled: true

    systemd:
      enabled: true
      # systemd units to collect logs from
      # units:
      #   - docker.service

    ## Fields to be created at Sumo Logic to ensure logs are tagged with
    ## relevant metadata.
    ## https://help.sumologic.com/docs/manage/fields/#manage-fields
    fields:
      - cluster
      - container
      - deployment
      - host
      - namespace
      - node
      - pod
      - service

  ### Metrics configuration
  ## Set the enabled flag to false for disabling metrics ingestion altogether.
  metrics:
    enabled: true
    metadata:
      ## Set metadata provider service (either fluentd or otelcol).
      provider: fluentd

    ### Enable a load balancing proxy for Prometheus remote writes.
    ## Prometheus remote write uses a single persistent HTTP connection per target,
    ## which interacts poorly with TCP load balancing with iptables that K8s Services do.
    ## Use a real HTTP load balancer for this instead.
    ## This is an advanced feature, enable only if you're experiencing performance
    ## issues with metrics metadata enrichment.
    remoteWriteProxy:
      enabled: false
      config:
        ## Increase this if you've increased samples_per_send in Prometheus to prevent nginx
        ## from spilling proxied request bodies to disk
        clientBodyBufferSize: "64k"
        ## This feature autodetects how much CPU is assigned to the nginx instance and sets
        ## the right amount of workers based on that. Disable to use the default of 8 workers.
        workerCountAutotune: true
        ## Nginx listen port
        port: 8080
      replicaCount: 3
      image:
        repository: public.ecr.aws/sumologic/nginx-unprivileged
        tag: 1.23.1-alpine
        pullPolicy: IfNotPresent
      resources:
        limits:
          cpu: 1000m
          memory: 256Mi
        requests:
          cpu: 200m
          memory: 128Mi
      livenessProbe:
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        successThreshold: 1
        failureThreshold: 6
      readinessProbe:
        initialDelaySeconds: 5
        periodSeconds: 5
        timeoutSeconds: 3
        successThreshold: 1
        failureThreshold: 3

      securityContext: {}
      nodeSelector: {}
      tolerations: []
      affinity: {}
      ## Option to define priorityClassName to assign a priority class to pods.
      priorityClassName:

      ## Add custom labels only to metrics sts pods
      podLabels: {}
      ## Add custom annotations only to metrics sts pods
      podAnnotations: {}

  ### Traces configuration
  ## Set the enabled flag to true to enable traces ingestion.
  traces:
    enabled: false
    ## How many spans per request should be send to receiver
    spans_per_request: 100

fluentd:
  image:
    repository: public.ecr.aws/sumologic/kubernetes-fluentd
    tag: 1.14.6-sumo-5
    pullPolicy: IfNotPresent

  ## Specifies whether a PodSecurityPolicy should be created
  podSecurityPolicy:
    create: false
  additionalPlugins: []

  ## Sets the fluentd log level. The default log level, if not specified, is info.
  ## Sumo will only ingest the error log level and some specific warnings, the info logs can be seen in kubectl logs.
  ## ref: https://docs.fluentd.org/deployment/logging
  logLevel: "info"
  ## to ingest all fluentd logs, turn the logLevelFilter to false
  logLevelFilter: true

  ## Verify SumoLogic HTTPS certificates
  verifySsl: true
  ## Proxy URI for sumologic output plugin
  proxyUri: ""

  ## Enable and set compression encoding for fluentd output plugin
  compression:
    enabled: true
    encoding: gzip

  securityContext:
    ## The group ID of all processes in the statefulset containers. By default this needs to be fluent(999).
    fsGroup: 999

  ## Add custom labels to all fluentd sts pods(logs, metrics, events)
  podLabels: {}

  ## Add custom annotations to all fluentd sts pods(logs, metrics, events)
  podAnnotations: {}

  ## Add custom labels to all fluentd svc (logs, metrics, events)
  serviceLabels: {}

  ## Add custom labels to all fluentd statefulset PVC (logs, metrics, events)
  pvcLabels: {}

  ## Persist data to a persistent volume; When enabled, fluentd uses the file buffer instead of memory buffer.
  persistence:
    ## After changing this value please follow steps described in:
    ## https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/main/deploy/docs/FluentdPersistence.md
    enabled: true

    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, Azure & OpenStack)
    ##
    # storageClass: "-"
    # annotations: {}
    accessMode: ReadWriteOnce
    size: 10Gi

  buffer:
    ## Option to specify the Fluentd buffer as file/memory.
    ## If fluentd.persistence.enabled is true, this will be ignored.
    type: "memory"

    ## How frequently to push logs to SumoLogic
    ## ref: https://github.com/SumoLogic/fluentd-kubernetes-sumologic#options
    flushInterval: "5s"
    ## Increase number of http threads to Sumo. May be required in heavy logging/high DPM clusters
    numThreads: 8
    chunkLimitSize: "1m"
    queueChunkLimitSize: 128
    totalLimitSize: "128m"
    retryMaxInterval: "10m"
    retryForever: true
    compress: gzip

    ## File paths to buffer to, if Fluentd buffer type is specified as file above.
    ## Each sumologic output plugin buffers to its own unique file.
    filePaths:
      logs:
        containers: /fluentd/buffer/logs.containers
        kubelet: /fluentd/buffer/logs.kubelet
        systemd: /fluentd/buffer/logs.systemd
        default: /fluentd/buffer/logs.default
      metrics:
        apiserver: /fluentd/buffer/metrics.apiserver
        kubelet: /fluentd/buffer/metrics.kubelet
        container: /fluentd/buffer/metrics.container
        controller: /fluentd/buffer/metrics.controller
        scheduler: /fluentd/buffer/metrics.scheduler
        state: /fluentd/buffer/metrics.state
        node: /fluentd/buffer/metrics.node
        control-plane: /fluentd/buffer/metrics.control_plane
        default: /fluentd/buffer/metrics.default
      events: /fluentd/buffer/events
      traces: /fluentd/buffer/traces

    ## Additional config for buffer settings
    extraConf: |-

  ## configuration of fluentd monitoring metrics
  ## input -> fluentd_input_status_num_records_total (~5% DPM increase for empty cluster)
  ## output -> fluentd_output_status_num_records_total (~25% DPM increase for empty cluster)
  monitoring:
    input: false
    output: false

  metadata:
    ## Option to control capturing of annotations by metadata filter plugin.
    annotation_match:
      - 'sumologic\.com.*'
    ## Option to control the enabling of metadata filter plugin cache_size.
    ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
    cacheSize: "10000"
    ## Option to control the enabling of metadata filter plugin cache_ttl (in seconds).
    ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
    cacheTtl: "7200"
    ## Option to control the interval at which metadata cache is asynchronously refreshed (in seconds).
    cacheRefresh: "3600"
    ## Option to control the variation in seconds by which the cacheRefresh option is changed for each pod separately.
    ## For example, if cache refresh is 1 hour and variation is 15 minutes, then actual cache refresh interval
    ## will be a random value between 45 minutes and 1 hour 15 minutes, different for each pod. This helps spread
    ## the load on API server that the cache refresh induces. Setting this to 0 disables cache refresh variation.
    cacheRefreshVariation: "900"
    ## Option to control the delay with which cache refresh calls hit the api server.
    ## For example, if 0 then all metadata enrichment happen immediately. Setting this to a non-zero values ensures the
    ## traffic to api server is much distributed.
    ## Disclaimer: Not recommended for use, if your api server read latency is > 1s
    cacheRefreshApiserverRequestDelay: "0"
    ## Option to disable metadata cache refresh
    cacheRefreshExcludePodRegex: ""
    ## Option to give plugin specific log level
    pluginLogLevel: "error"
    ## Option to specify K8s API versions
    coreApiVersions:
      - v1
    ## Option to specify K8s API groups
    apiGroups:
      - apps/v1
    ## Option to control the enrichment of logs and metrics with pod owner metadata like `daemonset`, `deployment`,
    ## `replicaset`, `statefulset`.
    addOwners: true
    ## Option to control the enrichment of logs and metrics with `service` metadata.
    addService: true
    ## Option to specify custom API server URL instead of the default,
    ## that is taken from KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT environment variables.
    ## Example: "https://kubernetes.default.svc:443".
    apiServerUrl: ""

  logs:
    enabled: true
    statefulset:
      nodeSelector: {}
      tolerations: []
      topologySpreadConstraints: []
      affinity: {}
      ## Acceptable values for podAntiAffinity:
      ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
      ## hard: specifies rules that must be met for a pod to be scheduled onto a node
      podAntiAffinity: "soft"
      replicaCount: 3
      resources:
        limits:
          memory: 1Gi
          cpu: 1000m
        requests:
          memory: 768Mi
          ## Warning! Increasing the CPU requests for Fluentd above 1000m might result in broken autoscaling
          ## ref: https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/main/deploy/docs/Best_Practices.md#cpu-resources-warning
          cpu: 500m
      ## Option to define priorityClassName to assign a priority class to pods.
      priorityClassName:

      ## Add custom labels only to logs fluentd sts pods
      podLabels: {}
      ## Add custom annotations only to logs fluentd sts pods
      podAnnotations: {}

      ## Set securityContext for containers running in pods in logs statefulset.
      containers:
        fluentd:
          securityContext: {}

      ## This supports either a structured array or a templatable string
      initContainers: []

      ## Array mode
      # initContainers:
      #   - name: do-something
      #     image: bitnami/kubectl:1.22
      #     command: ['kubectl', 'version']

      ## String mode
      # initContainers: |-
      #   - name: do-something
      #     image: bitnami/kubectl:{{ .Capabilities.KubeVersion.Major }}.{{ trimSuffix "+" .Capabilities.KubeVersion.Minor }}
      #     command: ['kubectl', 'version']

    ## Option to turn autoscaling on for fluentd and specify params for HPA.
    ## Autoscaling needs metrics-server to access cpu metrics.
    autoscaling:
      enabled: false
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilizationPercentage: 100
      # targetMemoryUtilizationPercentage: 50

    ## Option to specify PodDisrutionBudgets
    ## You can specify only one of maxUnavailable and minAvailable in a single PodDisruptionBudget
    podDisruptionBudget:
      minAvailable: 2
    ## To use maxUnavailable, set minAvailable to null and uncomment the below:
    #   maxUnavailable: 1

    rawConfig: |-
      @include common.conf
      @include logs.conf

    ## Configuration for the forward input plugin that receives logs from FluentBit
    ## ref: https://docs.fluentd.org/input/forward
    input:
      ## Use to specify additional config, including transport or security options.
      forwardExtraConf: |-

    ## Configuration for sumologic output plugin
    ## See below links for full reference:
    ## https://github.com/SumoLogic/fluentd-output-sumologic
    ## https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/main/deploy/helm/sumologic/conf/logs/fluentd/logs.output.conf
    output:
      ## Format to post logs into Sumo: fields, json, json_merge, or text.
      ## NOTE: for logs metadata, fields is required.
      ## NOTE: Multiline log detection works differently for `text` format. See below link for full reference:
      ## https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/main/deploy/docs/Troubleshoot_Collection.md#using-text-format
      logFormat: fields
      ## Option to control adding timestamp to logs.
      addTimestamp: true
      ## Field name when add_timestamp is on.
      timestampKey: "timestamp"
      ## Option to give plugin specific log level
      pluginLogLevel: "error"
      ## Additional config parameters for sumologic output plugin
      extraConf: |-

    ## Additional config for custom log pipelines
    ## ref: TODO: documentation for custom logs pipelines
    extraLogs: |-

    ## Container log configuration
    containers:
      ## To override the entire contents of logs.source.containers.conf file. Leave empty for the default pipeline
      overrideRawConfig: |-

      outputConf: |-
        @include logs.output.conf

      ## Override output section for container logs. Leave empty for the default output section
      overrideOutputConf: |-

      ## Set the _sourceName metadata field in Sumo Logic.
      sourceName: "%{namespace}.%{pod}.%{container}"
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "%{namespace}/%{pod_name}"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for containers.
      ## Matching containers will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeContainerRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for namespaces.
      ## Matching namespaces will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeNamespaceRegex: ""
      ## A regular expression for pods.
      ## Matching pods will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePodRegex: ""

      ## Defines whether container-level pod annotations are enabled.
      ## Setting this to `true` might slightly affect Fluentd performance.
      ## See below link for full reference:
      ## https://github.com/SumoLogic/sumologic-kubernetes-fluentd/tree/v1.14.6-sumo-5/fluent-plugin-kubernetes-sumologic#container-level-pod-annotations
      perContainerAnnotationsEnabled: false
      ## Defines the list of prefixes of container-level pod annotations.
      ## See below link for full reference:
      ## https://github.com/SumoLogic/sumologic-kubernetes-fluentd/tree/v1.14.6-sumo-5/fluent-plugin-kubernetes-sumologic#container-level-pod-annotations
      perContainerAnnotationPrefixes: []

      ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
      k8sMetadataFilter:
        ## Option to control the enabling of metadata filter plugin watch.
        watch: "true"
        ## path to CA file for Kubernetes server certificate validation
        caFile: ""
        ## Validate SSL certificates
        verifySsl: true
        ## Path to a client cert file to authenticate to the API server
        clientCert: ""
        ## Path to a client key file to authenticate to the API server
        clientKey: ""
        ## Path to a file containing the bearer token to use for authentication
        bearerTokenFile: ""
        ## Regex to match containers tag to `pod_name`, `namespace`, `container_name` and `docker_id`. All of them are obligatory
        ## See below links for reference:
        ## https://github.com/SumoLogic/sumologic-kubernetes-fluentd/blob/v1.14.6-sumo-5/fluent-plugin-kubernetes-metadata-filter/lib/fluent/plugin/filter_kubernetes_metadata.rb#L322-L325
        ## https://github.com/SumoLogic/sumologic-kubernetes-fluentd/tree/v1.14.6-sumo-5/fluent-plugin-kubernetes-metadata-filter#configuration
        tagToMetadataRegexp: '.+?\.containers\.(?<pod_name>[^_]+)_(?<namespace>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$'

      ## To use additional filter plugins
      extraFilterPluginConf: |-

      ## To use additional output plugins
      extraOutputPluginConf: |-

      ## To enable stiching multiline logs in fluentd when fluent-bit Multiline feature is On
      multiline:
        enabled: true

    ## Kubelet log configuration
    kubelet:
      enabled: true
      ## To use additional filter plugins
      extraFilterPluginConf: |-

      ## To use additional output plugins
      extraOutputPluginConf: |-

      outputConf: |-
        @include logs.output.conf

      ## Override output section for kubelet logs. Leave empty for the default output section.
      overrideOutputConf: |-

      ## Set the _sourceName metadata field in Sumo Logic.
      sourceName: "k8s_kubelet"
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "kubelet"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for facility.
      ## Matching facility will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeFacilityRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for priority.
      ## Matching priority will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePriorityRegex: ""
      ## A regular expression for unit.
      ## Matching unit will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeUnitRegex: ""

    ## Systemd log configuration
    systemd:
      enabled: true
      ## To use additional filter plugins
      extraFilterPluginConf: |-

      ## To use additional output plugins
      extraOutputPluginConf: |-

      outputConf: |-
        @include logs.output.conf

      ## Override output section for systemd logs. Leave empty for the default output section.
      overrideOutputConf: |-

      ## Set the _sourceName metadata field in Sumo Logic.
      sourceName: "k8s_systemd"
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "system"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for facility.
      ## Matching facility will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeFacilityRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for priority.
      ## Matching priority will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePriorityRegex: ""
      ## A regular expression for unit.
      ## Matching unit will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeUnitRegex: ""

    ## Default log configuration (catch-all)
    default:
      ## To use additional filter plugins
      extraFilterPluginConf: |-

      ## To use additional output plugins
      extraOutputPluginConf: |-

      outputConf: |-
        @include logs.output.conf

      ## Override output section for untagged logs. Leave empty for the default output section.
      overrideOutputConf: |-

      ## Set the _sourceName metadata field in Sumo Logic.
      sourceName: "k8s_default"
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "default"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for facility.
      ## Matching facility will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeFacilityRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for priority.
      ## Matching priority will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePriorityRegex: ""
      ## A regular expression for unit.
      ## Matching unit will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeUnitRegex: ""

    ## Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #   - name: VALUE_FROM_SECRET
    #     valueFrom:
    #       secretKeyRef:
    #         name: secret_name
    #         key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

  metrics:
    enabled: true
    statefulset:
      nodeSelector: {}
      tolerations: []
      topologySpreadConstraints: []
      affinity: {}
      ## Acceptable values for podAntiAffinity:
      ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
      ## hard: specifies rules that must be met for a pod to be scheduled onto a node
      podAntiAffinity: "soft"
      replicaCount: 3
      resources:
        limits:
          memory: 1Gi
          cpu: 1000m
        requests:
          memory: 768Mi
          ## Warning! Increasing the CPU requests for Fluentd above 1000m might result in broken autoscaling
          ## ref: https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/main/deploy/docs/Best_Practices.md#cpu-resources-warning
          cpu: 500m
      ## Option to define priorityClassName to assign a priority class to pods.
      priorityClassName:

      ## Add custom labels only to metrics fluentd sts pods
      podLabels: {}
      ## Add custom annotations only to metrics fluentd sts pods
      podAnnotations: {}

      ## Set securityContext for containers running in pods in metrics statefulset.
      containers:
        fluentd:
          securityContext: {}

      ## This supports either a structured array or a templatable string
      initContainers: []

      ## Array mode
      # initContainers:
      #   - name: do-something
      #     image: bitnami/kubectl:1.22
      #     command: ['kubectl', 'version']

      ## String mode
      # initContainers: |-
      #   - name: do-something
      #     image: bitnami/kubectl:{{ .Capabilities.KubeVersion.Major }}.{{ trimSuffix "+" .Capabilities.KubeVersion.Minor }}
      #     command: ['kubectl', 'version']

    ## Option to turn autoscaling on for fluentd and specify params for HPA.
    ## Autoscaling needs metrics-server to access cpu metrics.
    autoscaling:
      enabled: false
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilizationPercentage: 100
      # targetMemoryUtilizationPercentage: 50

    ## Option to specify PodDisrutionBudgets
    ## You can specify only one of maxUnavailable and minAvailable in a single PodDisruptionBudget
    podDisruptionBudget:
      minAvailable: 2

    rawConfig: |-
      @include common.conf
      @include metrics.conf

    ## Configuration for sumologic output plugin
    ## See below links for full reference:
    ## https://github.com/SumoLogic/fluentd-output-sumologic
    ## https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/main/deploy/helm/sumologic/conf/metrics/fluentd/metrics.output.conf
    outputConf: |-
      @include metrics.output.conf

    ## Additional config parameters for sumologic output plugin
    extraOutputConf: |-

    ## Override output section for metrics. Leave empty for the default output section
    overrideOutputConf: |-

    ## To use additional filter plugins
    extraFilterPluginConf: |-

    ## To use additional output plugins
    extraOutputPluginConf: |-

    ## Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #   - name: VALUE_FROM_SECRET
    #     valueFrom:
    #       secretKeyRef:
    #         name: secret_name
    #         key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

    ## Output configuration
    ## tag: tag for fluentd match
    ## id: sumologic output @id
    ## endpoint: [optional] [output key by default] It should match the sumologic.collector.sources.metrics[].endpoint
    ## drop: [optional] [false by default] Change it to true to drop traffic for specific output
    ## weight: [optional [0 by default] Order of fluentd output (lower means higher priority)
    ## The weight has meaning in case of fluentd matches
    output:
      apiserver:
        tag: prometheus.metrics.apiserver**
        id: sumologic.endpoint.metrics.apiserver
        weight: 90
      kubelet:
        tag: prometheus.metrics.kubelet**
        id: sumologic.endpoint.metrics.kubelet
        weight: 90
      container:
        tag: prometheus.metrics.container**
        id: sumologic.endpoint.metrics.container
        source: kubelet
        weight: 90
      controller:
        tag: prometheus.metrics.controller-manager**
        id: sumologic.endpoint.metrics.kube.controller.manager
        weight: 90
      scheduler:
        tag: prometheus.metrics.scheduler**
        id: sumologic.endpoint.metrics.kube.scheduler
        weight: 90
      state:
        tag: prometheus.metrics.state**
        id: sumologic.endpoint.metrics.kube.state
        weight: 90
      node:
        tag: prometheus.metrics.node**
        id: sumologic.endpoint.metrics.node.exporter
        weight: 90
      control-plane:
        tag: prometheus.metrics.control-plane**
        id: sumologic.endpoint.metrics.control.plane
        weight: 90
      default:
        tag: prometheus.metrics**
        id: sumologic.endpoint.metrics
        weight: 100

  events:
    ## If enabled, collect K8s events
    ## Deprecated, use `sumologic.events.enabled` instead
    enabled: true

    statefulset:
      nodeSelector: {}
      tolerations: []
      affinity: {}
      resources:
        limits:
          memory: 512Mi
          cpu: 200m
        requests:
          memory: 256Mi
          cpu: 100m
      ## Option to define priorityClassName to assign a priority class to pods.
      priorityClassName:

      ## Add custom labels only to events fluentd sts pods
      podLabels: {}
      ## Add custom annotations only to events fluentd sts pods
      podAnnotations: {}

      ## Set securityContext for containers running in pods in events statefulset.
      containers:
        fluentd:
          securityContext: {}

      ## This supports either a structured array or a templatable string
      initContainers: []

      ## Array mode
      # initContainers:
      #   - name: do-something
      #     image: bitnami/kubectl:1.22
      #     command: ['kubectl', 'version']

      ## String mode
      # initContainers: |-
      #   - name: do-something
      #     image: bitnami/kubectl:{{ .Capabilities.KubeVersion.Major }}.{{ trimSuffix "+" .Capabilities.KubeVersion.Minor }}
      #     command: ['kubectl', 'version']

    ## Override output section for events. Leave empty for the default output section
    overrideOutputConf: |-

    ## Source name for the Events source. Default: "events"
    sourceName: "events"
    ## Source category for the Events source. Default: "{clusterName}/events"
    # sourceCategory: "kubernetes/events"
    ## Override Kubernetes resource types you want to get events for from different Kubernetes
    ## API versions. The key represents the name of the resource type and the value represents
    ## the API version.
    # watchResourceEventsOverrides:
    #   pods: "v1"
    #   events: "events.k8s.io/v1beta1"

    ## Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #   - name: VALUE_FROM_SECRET
    #     valueFrom:
    #       secretKeyRef:
    #         name: secret_name
    #         key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

## Configure metrics-server
## ref: https://github.com/bitnami/charts/blob/master/bitnami/metrics-server/values.yaml
metrics-server:
  ## Set the enabled flag to true for enabling metrics-server.
  ## This is required before enabling fluentd autoscaling unless you have an existing metrics-server in the cluster.
  enabled: false
  apiService:
    create: true
  extraArgs:
    kubelet-insecure-tls: true
    kubelet-preferred-address-types: InternalIP,ExternalIP,Hostname
  ## Optionally specify image options for metrics-server
  # image:
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - imagepullsecret

## Configure fluent-bit
## ref: https://github.com/fluent/helm-charts/blob/master/charts/fluent-bit/values.yaml
fluent-bit:
  ## If specified, use these secrets to access the image
  # imagePullSecrets:
  #   - name: "image-pull-secret"
  image:
    repository: public.ecr.aws/sumologic/fluent-bit
    tag: 1.6.10-sumo-3
    pullPolicy: IfNotPresent

  ## Resource limits for fluent-bit
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 10m
    #   memory: 8Mi

  ## Uncomment the flag below to not install fluent-bit helm chart as a dependency along with this helm chart.
  ## Do not use this flag to disable logs collection.
  ## Instead, set the `sumologic.logs.enabled: false` flag to disable logs collection.
  ## Do not set this flag explicitly to `true` while at the same time setting `sumologic.logs.enabled: false`,
  ## as this will make Fluent Bit try to write to an non-existent logs enrichment service.
  # enabled: false

  ## Configuration for fluent-bit service
  service:
    ## Add custom labels to fluent-bit service
    labels:
      sumologic.com/scrape: "true"  # label used by fluent-bit service monitor

  ## Add custom pod labels to fluent-bit daemonset pods
  podLabels: {}

  ## Add custom pod annotations to fluent-bit daemonset pods
  podAnnotations: {}

  securityContext:
    capabilities:
      drop:
        - ALL
  ## Set securityContext of fluent-bit daemonset containers as privileged for running in Openshift
  #   privileged: true

  env:
    - name: FLUENTD_LOGS_SVC
      valueFrom:
        configMapKeyRef:
          name: sumologic-configmap
          key: fluentdLogs
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace

  tolerations:
    - effect: NoSchedule
      operator: Exists

  extraVolumeMounts:
    - mountPath: /tail-db
      name: tail-db
  extraVolumes:
    - hostPath:
        path: /var/lib/fluent-bit
        type: DirectoryOrCreate
      name: tail-db

  ## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit
  config:
    ## https://docs.fluentbit.io/manual/service
    service: |
      [SERVICE]
          Flush        1
          Daemon       Off
          Log_Level    info
          Parsers_File parsers.conf
          Parsers_File custom_parsers.conf
          HTTP_Server  On
          HTTP_Listen  0.0.0.0
          HTTP_Port    2020
    ## https://docs.fluentbit.io/manual/pipeline/inputs
    ## ref: https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/main/deploy/docs/ContainerLogs.md
    inputs: |
      [INPUT]
          Name                tail
          Path                /var/log/containers/*.log
          Docker_Mode         On
          Docker_Mode_Parser  multi_line
          Tag                 containers.*
          Refresh_Interval    1
          Rotate_Wait         60
          Mem_Buf_Limit       5MB
          Skip_Long_Lines     On
          DB                  /tail-db/tail-containers-state-sumo.db
          DB.Sync             Normal
      [INPUT]
          Name            systemd
          Tag             host.*
          DB              /tail-db/systemd-state-sumo.db
          Systemd_Filter  _SYSTEMD_UNIT=addon-config.service
          Systemd_Filter  _SYSTEMD_UNIT=addon-run.service
          Systemd_Filter  _SYSTEMD_UNIT=cfn-etcd-environment.service
          Systemd_Filter  _SYSTEMD_UNIT=cfn-signal.service
          Systemd_Filter  _SYSTEMD_UNIT=clean-ca-certificates.service
          Systemd_Filter  _SYSTEMD_UNIT=containerd.service
          Systemd_Filter  _SYSTEMD_UNIT=coreos-metadata.service
          Systemd_Filter  _SYSTEMD_UNIT=coreos-setup-environment.service
          Systemd_Filter  _SYSTEMD_UNIT=coreos-tmpfiles.service
          Systemd_Filter  _SYSTEMD_UNIT=dbus.service
          Systemd_Filter  _SYSTEMD_UNIT=docker.service
          Systemd_Filter  _SYSTEMD_UNIT=efs.service
          Systemd_Filter  _SYSTEMD_UNIT=etcd-member.service
          Systemd_Filter  _SYSTEMD_UNIT=etcd.service
          Systemd_Filter  _SYSTEMD_UNIT=etcd2.service
          Systemd_Filter  _SYSTEMD_UNIT=etcd3.service
          Systemd_Filter  _SYSTEMD_UNIT=etcdadm-check.service
          Systemd_Filter  _SYSTEMD_UNIT=etcdadm-reconfigure.service
          Systemd_Filter  _SYSTEMD_UNIT=etcdadm-save.service
          Systemd_Filter  _SYSTEMD_UNIT=etcdadm-update-status.service
          Systemd_Filter  _SYSTEMD_UNIT=flanneld.service
          Systemd_Filter  _SYSTEMD_UNIT=format-etcd2-volume.service
          Systemd_Filter  _SYSTEMD_UNIT=kube-node-taint-and-uncordon.service
          Systemd_Filter  _SYSTEMD_UNIT=kubelet.service
          Systemd_Filter  _SYSTEMD_UNIT=ldconfig.service
          Systemd_Filter  _SYSTEMD_UNIT=locksmithd.service
          Systemd_Filter  _SYSTEMD_UNIT=logrotate.service
          Systemd_Filter  _SYSTEMD_UNIT=lvm2-monitor.service
          Systemd_Filter  _SYSTEMD_UNIT=mdmon.service
          Systemd_Filter  _SYSTEMD_UNIT=nfs-idmapd.service
          Systemd_Filter  _SYSTEMD_UNIT=nfs-mountd.service
          Systemd_Filter  _SYSTEMD_UNIT=nfs-server.service
          Systemd_Filter  _SYSTEMD_UNIT=nfs-utils.service
          Systemd_Filter  _SYSTEMD_UNIT=node-problem-detector.service
          Systemd_Filter  _SYSTEMD_UNIT=ntp.service
          Systemd_Filter  _SYSTEMD_UNIT=oem-cloudinit.service
          Systemd_Filter  _SYSTEMD_UNIT=rkt-gc.service
          Systemd_Filter  _SYSTEMD_UNIT=rkt-metadata.service
          Systemd_Filter  _SYSTEMD_UNIT=rpc-idmapd.service
          Systemd_Filter  _SYSTEMD_UNIT=rpc-mountd.service
          Systemd_Filter  _SYSTEMD_UNIT=rpc-statd.service
          Systemd_Filter  _SYSTEMD_UNIT=rpcbind.service
          Systemd_Filter  _SYSTEMD_UNIT=set-aws-environment.service
          Systemd_Filter  _SYSTEMD_UNIT=system-cloudinit.service
          Systemd_Filter  _SYSTEMD_UNIT=systemd-timesyncd.service
          Systemd_Filter  _SYSTEMD_UNIT=update-ca-certificates.service
          Systemd_Filter  _SYSTEMD_UNIT=user-cloudinit.service
          Systemd_Filter  _SYSTEMD_UNIT=var-lib-etcd2.service
          Max_Entries     1000
          Read_From_Tail  true
    ## NOTE: Requires trailing "." for fully-qualified name resolution
    outputs: |
      [OUTPUT]
          Name          forward
          Match         *
          Host          ${FLUENTD_LOGS_SVC}.${NAMESPACE}.svc.cluster.local.
          Port          24321
          Retry_Limit   False
          tls           off
          tls.verify    on
          tls.debug     1
          # Disable keepalive for better load balancing
          net.keepalive off
    customParsers: |
      [PARSER]
          Name        multi_line
          Format      regex
          Regex       (?<log>^{"log":"\[?\d{4}-\d{1,2}-\d{1,2}.\d{2}:\d{2}:\d{2}.*)
      [PARSER]
          Name         crio
          Format       regex
          Regex        ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
          Time_Key     time
          Time_Format  %Y-%m-%dT%H:%M:%S.%L%z
      [PARSER]
          Name         containerd
          Format       regex
          Regex        ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
          Time_Key     time
          Time_Format  %Y-%m-%dT%H:%M:%S.%LZ

## Configure kube-prometheus-stack
## ref: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
kube-prometheus-stack:

  ## Uncomment the flag below to not install kube-prometheus-stack helm chart
  ## as a dependency along with this helm chart.
  ## This is needed e.g. if you want to use a different version of kube-prometheus-stack -
  ## see /deploy/docs/Best_Practices.md#using-newer-kube-prometheus-stack.
  ## To disable metrics collection, set `sumologic.metrics.enabled: false` and leave this flag commented out or set it to `false`.
  ## Do not set this flag explicitly to `true` while at the same time setting `sumologic.metrics.enabled: false`,
  ## as this will make Prometheus try to write to an non-existent metrics enrichment service.
  # enabled: false

  # global:
    ## Reference to one or more secrets to be used when pulling images
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # imagePullSecrets:
    #   - name: "image-pull-secret"

  ## Labels to apply to all kube-prometheus-stack resources
  commonLabels: {}

  ## k8s pre-1.14 prometheus recording rules
  additionalPrometheusRulesMap:
    pre-1.14-node-rules:
      groups:
        - name: node-pre-1.14.rules
          rules:
            - expr: sum(min(kube_pod_info) by (node))
              record: ':kube_pod_info_node_count:'
            - expr: 1 - avg(rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m]))
              record: :node_cpu_utilisation:avg1m
            - expr: |-
                1 - avg by (node) (
                  rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m])
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:)
              record: node:node_cpu_utilisation:avg1m
            - expr: |-
                1 -
                sum(
                  node_memory_MemFree_bytes{job="node-exporter"} +
                  node_memory_Cached_bytes{job="node-exporter"} +
                  node_memory_Buffers_bytes{job="node-exporter"}
                )
                /
                sum(node_memory_MemTotal_bytes{job="node-exporter"})
              record: ':node_memory_utilisation:'
            - expr: |-
                sum by (node) (
                  (
                    node_memory_MemFree_bytes{job="node-exporter"} +
                    node_memory_Cached_bytes{job="node-exporter"} +
                    node_memory_Buffers_bytes{job="node-exporter"}
                  )
                  * on (namespace, pod) group_left(node)
                    node_namespace_pod:kube_pod_info:
                )
              record: node:node_memory_bytes_available:sum
            - expr: |-
                (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
                /
                node:node_memory_bytes_total:sum
              record: node:node_memory_utilisation:ratio
            - expr: |-
                1 -
                sum by (node) (
                  (
                    node_memory_MemFree_bytes{job="node-exporter"} +
                    node_memory_Cached_bytes{job="node-exporter"} +
                    node_memory_Buffers_bytes{job="node-exporter"}
                  )
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
                /
                sum by (node) (
                  node_memory_MemTotal_bytes{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_memory_utilisation:'
            - expr: 1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
              record: 'node:node_memory_utilisation_2:'
            - expr: |-
                max by (instance, namespace, pod, device) ((node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
                - node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
                / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})
              record: 'node:node_filesystem_usage:'
            - expr: |-
                sum by (node) (
                  node_memory_MemTotal_bytes{job="node-exporter"}
                  * on (namespace, pod) group_left(node)
                    node_namespace_pod:kube_pod_info:
                )
              record: node:node_memory_bytes_total:sum
            - expr: |-
                sum(irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[1m])) +
                sum(irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[1m]))
              record: :node_net_utilisation:sum_irate
            - expr: |-
                sum by (node) (
                  (irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[1m]) +
                  irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[1m]))
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_net_utilisation:sum_irate
            - expr: |-
                sum(irate(node_network_receive_drop_total{job="node-exporter",device!~"veth.+"}[1m])) +
                sum(irate(node_network_transmit_drop_total{job="node-exporter",device!~"veth.+"}[1m]))
              record: :node_net_saturation:sum_irate
            - expr: |-
                sum by (node) (
                  (irate(node_network_receive_drop_total{job="node-exporter",device!~"veth.+"}[1m]) +
                  irate(node_network_transmit_drop_total{job="node-exporter",device!~"veth.+"}[1m]))
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_net_saturation:sum_irate
            - expr: |-
                sum(node_load1{job="node-exporter"})
                /
                sum(node:node_num_cpu:sum)
              record: ':node_cpu_saturation_load1:'
            - expr: avg(irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
              record: :node_disk_saturation:avg_irate
            - expr: |-
                avg by (node) (
                  irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_disk_saturation:avg_irate
            - expr: avg(irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
              record: :node_disk_utilisation:avg_irate
            - expr: |-
                avg by (node) (
                  irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_disk_utilisation:avg_irate
            - expr: |-
                1e3 * sum(
                  (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
                + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
                )
              record: :node_memory_swap_io_bytes:sum_rate
            - expr: |-
                1e3 * sum by (node) (
                  (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
                + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: node:node_memory_swap_io_bytes:sum_rate
            - expr: |-
                node:node_cpu_utilisation:avg1m
                  *
                node:node_num_cpu:sum
                  /
                scalar(sum(node:node_num_cpu:sum))
              record: node:cluster_cpu_utilisation:ratio
            - expr: |-
                (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
                /
                scalar(sum(node:node_memory_bytes_total:sum))
              record: node:cluster_memory_utilisation:ratio
            - expr: |-
                sum by (node) (
                  node_load1{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
                /
                node:node_num_cpu:sum
              record: 'node:node_cpu_saturation_load1:'
            - expr: |-
                max by (instance, namespace, pod, device) (
                  node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
                  /
                  node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
                  )
              record: 'node:node_filesystem_avail:'
            - expr: |-
                max(
                  max(
                    kube_pod_info{job="kube-state-metrics", host_ip!=""}
                  ) by (node, host_ip)
                  * on (host_ip) group_right (node)
                  label_replace(
                    (
                      max(node_filesystem_files{job="node-exporter", mountpoint="/"})
                      by (instance)
                    ), "host_ip", "$1", "instance", "(.*):.*"
                  )
                ) by (node)
              record: 'node:node_inodes_total:'
            - expr: |-
                max(
                  max(
                    kube_pod_info{job="kube-state-metrics", host_ip!=""}
                  ) by (node, host_ip)
                  * on (host_ip) group_right (node)
                  label_replace(
                    (
                      max(node_filesystem_files_free{job="node-exporter", mountpoint="/"})
                      by (instance)
                    ), "host_ip", "$1", "instance", "(.*):.*"
                  )
                ) by (node)
              record: 'node:node_inodes_free:'

  ## NOTE changing the serviceMonitor scrape interval to be >1m can result in metrics from recording
  ## rules to be missing and empty panels in Sumo Logic Kubernetes apps.
  kubeApiServer:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
      ## see docs/scraped_metrics.md
      ## apiserver_request_count
      ## apiserver_request_total
      ## apiserver_request_duration_seconds_count
      ## apiserver_request_duration_seconds_bucket
      ## apiserver_request_duration_seconds_sum
      ## apiserver_request_latencies_count
      ## apiserver_request_latencies_sum
      ## apiserver_request_latencies_summary
      ## apiserver_request_latencies_summary_count
      ## apiserver_request_latencies_summary_sum
      metricRelabelings:
        - action: keep
          regex: (?:apiserver_request_(?:count|total)|apiserver_request_(?:duration_seconds|latencies)_(?:count|sum)|apiserver_request_latencies_summary(?:|_count|_sum)|apiserver_request_duration_seconds_bucket)
          sourceLabels: [ __name__]
  kubelet:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
      ## Enable scraping /metrics/probes from kubelet's service
      probes: false
      ## Enable scraping /metrics/resource/v1alpha1 from kubelet's service
      resource: false
      ## see docs/scraped_metrics.md
      ## kubelet metrics:
      ## kubelet_docker_operations_errors
      ## kubelet_docker_operations_errors_total
      ## kubelet_docker_operations_duration_seconds_count
      ## kubelet_docker_operations_duration_seconds_sum
      ## kubelet_runtime_operations_duration_seconds_count
      ## kubelet_runtime_operations_duration_seconds_sum
      ## kubelet_running_container_count
      ## kubelet_running_containers
      ## kubelet_running_pod_count
      ## kubelet_running_pods
      ## kubelet_docker_operations_latency_microseconds
      ## kubelet_docker_operations_latency_microseconds_count
      ## kubelet_docker_operations_latency_microseconds_sum
      ## kubelet_runtime_operations_latency_microseconds
      ## kubelet_runtime_operations_latency_microseconds_count
      ## kubelet_runtime_operations_latency_microseconds_sum
      metricRelabelings:
        - action: keep
          regex: (?:kubelet_docker_operations_errors(?:|_total)|kubelet_(?:docker|runtime)_operations_duration_seconds_(?:count|sum)|kubelet_running_(?:container|pod)(?:_count|s)|kubelet_(:?docker|runtime)_operations_latency_microseconds(?:|_count|_sum))
          sourceLabels: [__name__]
        - action: labeldrop
          regex: id
      ## see docs/scraped_metrics.md
      ## cadvisor container metrics
      ## container_cpu_usage_seconds_total
      ## container_fs_limit_bytes
      ## container_fs_usage_bytes
      ## container_memory_working_set_bytes
      ## container_cpu_cfs_throttled_seconds_total
      ##
      ## cadvisor aggregate container metrics
      ## container_network_receive_bytes_total
      ## container_network_transmit_bytes_total
      cAdvisorMetricRelabelings:
        - action: keep
          regex: (?:container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_fs_usage_bytes|container_fs_limit_bytes|container_cpu_cfs_throttled_seconds_total|container_network_receive_bytes_total|container_network_transmit_bytes_total)
          sourceLabels: [__name__]
        - action: labelmap
          regex: container_name
          replacement: container
        - action: drop
          sourceLabels: [container]
          regex: POD
        - action: labeldrop
          regex: (id|name)
  kubeControllerManager:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
      ## see docs/scraped_metrics.md
      ## controller manager metrics
      ## https://kubernetes.io/docs/concepts/cluster-administration/monitoring/#kube-controller-manager-metrics
      ## e.g.
      ## cloudprovider_aws_api_request_duration_seconds_bucket
      ## cloudprovider_aws_api_request_duration_seconds_count
      ## cloudprovider_aws_api_request_duration_seconds_sum
      metricRelabelings:
        - action: keep
          regex: (?:cloudprovider_.*_api_request_duration_seconds.*)
          sourceLabels: [__name__]
  coreDns:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
      ## see docs/scraped_metrics.md
      ## coredns:
      ## coredns_cache_size
      ## coredns_cache_entries
      ## coredns_cache_hits_total
      ## coredns_cache_misses_total
      ## coredns_dns_request_duration_seconds_count
      ## coredns_dns_request_duration_seconds_sum
      ## coredns_dns_request_count_total
      ## coredns_dns_requests_total
      ## coredns_dns_response_rcode_count_total
      ## coredns_dns_responses_total
      ## coredns_forward_request_count_total
      ## coredns_forward_requests_total
      ## process_cpu_seconds_total
      ## process_open_fds
      ## process_resident_memory_bytes
      metricRelabelings:
        - action: keep
          regex: (?:coredns_cache_(size|entries|(hits|misses)_total)|coredns_dns_request_duration_seconds_(count|sum)|coredns_(dns_request|dns_response_rcode|forward_request)_count_total|coredns_(forward_requests|dns_requests|dns_responses)_total|process_(cpu_seconds_total|open_fds|resident_memory_bytes))
          sourceLabels: [__name__]
  kubeEtcd:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
      ## see docs/scraped_metrics.md
      ## etcd_request_cache_get_duration_seconds_count
      ## etcd_request_cache_get_duration_seconds_sum
      ## etcd_request_cache_add_duration_seconds_count
      ## etcd_request_cache_add_duration_seconds_sum
      ## etcd_request_cache_add_latencies_summary_count
      ## etcd_request_cache_add_latencies_summary_sum
      ## etcd_request_cache_get_latencies_summary_count
      ## etcd_request_cache_get_latencies_summary_sum
      ## etcd_helper_cache_hit_count
      ## etcd_helper_cache_hit_total
      ## etcd_helper_cache_miss_count
      ## etcd_helper_cache_miss_total
      ## etcd server:
      ## etcd_debugging_mvcc_db_total_size_in_bytes
      ## etcd_debugging_store_expires_total
      ## etcd_debugging_store_watchers
      ## etcd_disk_backend_commit_duration_seconds_bucket
      ## etcd_disk_wal_fsync_duration_seconds_bucket
      ## etcd_grpc_proxy_cache_hits_total
      ## etcd_grpc_proxy_cache_misses_total
      ## etcd_network_client_grpc_received_bytes_total
      ## etcd_network_client_grpc_sent_bytes_total
      ## etcd_server_has_leader
      ## etcd_server_leader_changes_seen_total
      ## etcd_server_proposals_applied_total
      ## etcd_server_proposals_committed_total
      ## etcd_server_proposals_failed_total
      ## etcd_server_proposals_pending
      ## process_cpu_seconds_total
      ## process_open_fds
      ## process_resident_memory_bytes
      metricRelabelings:
        - action: keep
          regex: (?:etcd_request_cache_(?:add|get)_(?:duration_seconds|latencies_summary)_(?:count|sum)|etcd_helper_cache_(?:hit|miss)_(?:count|total)|etcd_debugging_(mvcc_db_total_size_in_bytes|store_(expires_total|watchers))|etcd_disk_(backend_commit|wal_fsync)_duration_seconds_bucket|etcd_grpc_proxy_cache_(hits|misses)_total|etcd_network_client_grpc_(received|sent)_bytes_total|etcd_server_(has_leader|leader_changes_seen_total)|etcd_server_proposals_(pending|(applied|committed|failed)_total)|process_(cpu_seconds_total|open_fds|resident_memory_bytes))
          sourceLabels: [ __name__]
  kubeScheduler:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
      ## see docs/scraped_metrics.md
      ## scheduler metrics_latency_microseconds
      ## scheduler_e2e_scheduling_duration_seconds_bucket
      ## scheduler_e2e_scheduling_duration_seconds_count
      ## scheduler_e2e_scheduling_duration_seconds_sum
      ## scheduler_binding_duration_seconds_bucket
      ## scheduler_binding_duration_seconds_count
      ## scheduler_binding_duration_seconds_sum
      ## scheduler_framework_extension_point_duration_seconds_bucket
      ## scheduler_framework_extension_point_duration_seconds_count
      ## scheduler_framework_extension_point_duration_seconds_sum
      ## scheduler_scheduling_algorithm_duration_seconds_bucket
      ## scheduler_scheduling_algorithm_duration_seconds_count
      ## scheduler_scheduling_algorithm_duration_seconds_sum
      metricRelabelings:
        - action: keep
          regex: (?:scheduler_(?:e2e_scheduling|binding|framework_extension_point|scheduling_algorithm)_duration_seconds.*)
          sourceLabels: [__name__]
  kubeStateMetrics:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
      ## see docs/scraped_metrics.md
      ## kube_daemonset_status_current_number_scheduled
      ## kube_daemonset_status_desired_number_scheduled
      ## kube_daemonset_status_number_misscheduled
      ## kube_daemonset_status_number_unavailable
      ## kube_deployment_spec_replicas
      ## kube_deployment_status_replicas_available
      ## kube_deployment_status_replicas_unavailable
      ## kube_node_info
      ## kube_node_status_allocatable
      ## kube_node_status_capacity
      ## kube_node_status_condition
      ## kube_statefulset_metadata_generation
      ## kube_statefulset_replicas
      ## kube_statefulset_status_observed_generation
      ## kube_statefulset_status_replicas
      ## kube_hpa_spec_max_replicas
      ## kube_hpa_spec_min_replicas
      ## kube_hpa_status_condition
      ## kube_hpa_status_current_replicas
      ## kube_hpa_status_desired_replicas
      ## kube pod state metrics
      ## kube_pod_container_info
      ## kube_pod_container_resource_limits
      ## kube_pod_container_resource_requests
      ## kube_pod_container_status_ready
      ## kube_pod_container_status_restarts_total
      ## kube_pod_container_status_terminated_reason
      ## kube_pod_container_status_waiting_reason
      ## kube_pod_status_phase
      ## kube_pod_info
      ## kube_service_info
      ## kube_service_spec_external_ip
      ## kube_service_spec_type
      ## kube_service_status_load_balancer_ingress
      metricRelabelings:
        - action: keep
          regex: (?:kube_statefulset_status_observed_generation|kube_statefulset_status_replicas|kube_statefulset_replicas|kube_statefulset_metadata_generation|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_number_unavailable|kube_deployment_spec_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_node_info|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_hpa_spec_max_replicas|kube_hpa_spec_min_replicas|kube_hpa_status_(condition|(current|desired)_replicas)|kube_pod_container_info|kube_pod_container_resource_requests|kube_pod_container_resource_limits|kube_pod_container_status_ready|kube_pod_container_status_terminated_reason|kube_pod_container_status_waiting_reason|kube_pod_container_status_restarts_total|kube_pod_status_phase|kube_pod_info|kube_service_info|kube_service_spec_external_ip|kube_service_spec_type|kube_service_status_load_balancer_ingress)
          sourceLabels: [ __name__]
  nodeExporter:
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:
      ## see docs/scraped_metrics.md
      ## node exporter metrics
      ## node_cpu_seconds_total
      ## node_load1
      ## node_load5
      ## node_load15
      ## node_disk_io_time_weighted_seconds_total
      ## node_disk_io_time_seconds_total
      ## node_vmstat_pgpgin
      ## node_vmstat_pgpgout
      ## node_memory_MemFree_bytes
      ## node_memory_Cached_bytes
      ## node_memory_Buffers_bytes
      ## node_memory_MemTotal_bytes
      ## node_network_receive_drop_total
      ## node_network_transmit_drop_total
      ## node_network_receive_bytes_total
      ## node_network_transmit_bytes_total
      ## node_filesystem_avail_bytes
      ## node_filesystem_size_bytes
      ## node_filesystem_files_free
      ## node_filesystem_files
      metricRelabelings:
        - action: keep
          regex: (?:node_load1|node_load5|node_load15|node_cpu_seconds_total|node_disk_io_time_weighted_seconds_total|node_disk_io_time_seconds_total|node_vmstat_pgpgin|node_vmstat_pgpgout|node_memory_MemFree_bytes|node_memory_Cached_bytes|node_memory_Buffers_bytes|node_memory_MemTotal_bytes|node_network_receive_drop_total|node_network_transmit_drop_total|node_network_receive_bytes_total|node_network_transmit_bytes_total|node_filesystem_avail_bytes|node_filesystem_size_bytes)
          sourceLabels: [__name__]

  alertmanager:
    enabled: false
  grafana:
    enabled: false
    defaultDashboardsEnabled: false
  prometheusOperator:
    ## Labels to add to the operator pod
    podLabels: {}
    ## Annotations to add to the operator pod
    podAnnotations: {}
    ## Resource limits for prometheus operator
    resources: {}
      # limits:
      #   cpu: 200m
      #   memory: 200Mi
      # requests:
      #   cpu: 100m
      #   memory: 100Mi
    admissionWebhooks:
      enabled: false
    tls:
      enabled: false
  ## Resource limits for kube-state-metrics
  kube-state-metrics:
    ## Use the GCR repo, it's more recent and has ARM images starting from 1.9.8
    image:
      repository: k8s.gcr.io/kube-state-metrics/kube-state-metrics
      tag: v1.9.8
    # Do not collect metrics for CertificateSigningRequest or Ingress resources.
    # This is because the v1 version of kube-state-metrics uses API versions
    # that are removed in Kubernetes v1.22.
    collectors:
      certificatesigningrequests: false
      ingresses: false
    ## Custom labels to apply to service, deployment and pods
    customLabels: {}
    ## Additional annotations for pods in the DaemonSet
    podAnnotations: {}
    resources: {}
      # limits:
      #   cpu: 100m
      #   memory: 64Mi
      # requests:
      #   cpu: 10m
      #   memory: 32Mi
  ## Resource limits for prometheus node exporter
  prometheus-node-exporter:
    image:
      tag: v1.3.1
    ## Additional labels for pods in the DaemonSet
    podLabels: {}
    ## Additional annotations for pods in the DaemonSet
    podAnnotations: {}
    resources: {}
      # limits:
      #   cpu: 200m
      #   memory: 50Mi
      # requests:
      #   cpu: 100m
      #   memory: 30Mi
  prometheus:
    additionalServiceMonitors:
      - name: collection-sumologic-fluentd-logs
        additionalLabels:
          sumologic.com/app: fluentd-logs
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-logs
            sumologic.com/scrape: "true"
      - name: collection-sumologic-otelcol-logs
        additionalLabels:
          sumologic.com/app: otelcol-logs
        endpoints:
          - port: otelcol-metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-logs
            sumologic.com/scrape: "true"
      - name: collection-sumologic-fluentd-metrics
        additionalLabels:
          sumologic.com/app: fluentd-metrics
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-metrics
            sumologic.com/scrape: "true"
      - name: collection-sumologic-otelcol-metrics
        additionalLabels:
          sumologic.com/app: otelcol-metrics
        endpoints:
          - port: otelcol-metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-metrics
            sumologic.com/scrape: "true"
      - name: collection-sumologic-fluentd-events
        additionalLabels:
          sumologic.com/app: fluentd-events
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: fluentd-events
            sumologic.com/scrape: "true"
      - name: collection-fluent-bit
        additionalLabels:
          sumologic.com/app: collection-fluent-bit
        endpoints:
          - port: http
            path: /api/v1/metrics/prometheus
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            app.kubernetes.io/name: fluent-bit
            sumologic.com/scrape: "true"
      - name: collection-sumologic-otelcol-logs-collector
        additionalLabels:
          sumologic.com/app: otelcol-logs-collector
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/app: otelcol-logs-collector
            sumologic.com/scrape: "true"
      - name: collection-sumologic-otelcol-traces
        additionalLabels:
          sumologic.com/app: otelcol
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            sumologic.com/component: traces
            sumologic.com/scrape: "true"
      - name: collection-sumologic-prometheus
        endpoints:
          - port: web
            path: /metrics
        namespaceSelector:
          matchNames:
            - $(NAMESPACE)
        selector:
          matchLabels:
            operated-prometheus: "true"
    prometheusSpec:
      ## Prometheus default scrape interval, default from upstream Kube Prometheus Stack Helm chart
      ## NOTE changing the scrape interval to be >1m can result in metrics
      ## from recording rules to be missing and empty panels in Sumo Logic Kubernetes apps.
      scrapeInterval: "30s"
      ## Prometheus data retention period
      retention: "1d"
      ## Add custom pod annotations and labels to prometheus pods
      podMetadata:
        labels: {}
        annotations: {}
      ## Define resources requests and limits for single Pods.
      resources:
        limits:
          cpu: 2000m
          memory: 8Gi
        requests:
          cpu: 500m
          memory: 1Gi
      thanos:
        # set this to a version with a ARM Docker image
        version: v0.25.2
        ## Defines the resource requirements for the Thanos sidecar
        ## Setting those as there are no default values in the upstream chart
        resources:
          limits:
            cpu: 10m
            memory: 32Mi
          requests:
            cpu: 1m
            memory: 8Mi
      containers:
        - name: "config-reloader"
          env:
            - name: FLUENTD_METRICS_SVC
              valueFrom:
                configMapKeyRef:
                  name: sumologic-configmap
                  key: fluentdMetrics
            - name: NAMESPACE
              valueFrom:
                configMapKeyRef:
                  name: sumologic-configmap
                  key: fluentdNamespace

      ## Enable WAL compression to reduce Prometheus memory consumption
      walCompression: true

      ## prometheus scrape config
      ## rel: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config
      additionalScrapeConfigs:
        ## scraping metrics basing on annotations:
        ##   - prometheus.io/scrape: true - to scrape metrics from the pod
        ##   - prometheus.io/path: /metrics - path which the metric should be scrape from
        ##   - prometheus.io/port: 9113 - port which the metric should be scrape from
        ## rel: https://github.com/prometheus-operator/kube-prometheus/pull/16#issuecomment-424318647
        - job_name: 'kubernetes-pods'
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
              separator: ;
              regex: Node;(.*)
              target_label: node
              replacement: ${1}
              action: replace
            - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
              separator: ;
              regex: Pod;(.*)
              target_label: pod
              replacement: ${1}
              action: replace
            - source_labels: [__metrics_path__]
              separator: ;
              regex: (.*)
              target_label: endpoint
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_service_name]
              separator: ;
              regex: (.*)
              target_label: service
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_pod_name]
              separator: ;
              regex: (.*)
              target_label: pod
              replacement: $1
              action: replace
            - source_labels: [__meta_kubernetes_service_name]
              separator: ;
              regex: (.*)
              target_label: job
              replacement: ${1}
              action: replace

      remoteWrite:
        ## kube non pod state metrics
        ## kube_daemonset_status_current_number_scheduled
        ## kube_daemonset_status_desired_number_scheduled
        ## kube_daemonset_status_number_misscheduled
        ## kube_daemonset_status_number_unavailable
        ## kube_deployment_spec_replicas
        ## kube_deployment_status_replicas_available
        ## kube_deployment_status_replicas_unavailable
        ## kube_node_info
        ## kube_node_status_allocatable
        ## kube_node_status_capacity
        ## kube_node_status_condition
        ## kube_statefulset_metadata_generation
        ## kube_statefulset_replicas
        ## kube_statefulset_status_observed_generation
        ## kube_statefulset_status_replicas
        ## kube_hpa_spec_max_replicas
        ## kube_hpa_spec_min_replicas
        ## kube_hpa_status_condition
        ## kube_hpa_status_current_replicas
        ## kube_hpa_status_desired_replicas
        ## kube_service_info
        ## kube_service_spec_external_ip
        ## kube_service_spec_type
        ## kube_service_status_load_balancer_ingress
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.state
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: kube-state-metrics;(?:kube_statefulset_status_observed_generation|kube_statefulset_status_replicas|kube_statefulset_replicas|kube_statefulset_metadata_generation|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_number_unavailable|kube_deployment_spec_replicas|kube_deployment_status_replicas_available|kube_deployment_status_replicas_unavailable|kube_node_info|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_hpa_spec_max_replicas|kube_hpa_spec_min_replicas|kube_hpa_status_(condition|(current|desired)_replicas)|kube_service_info|kube_service_spec_external_ip|kube_service_spec_type|kube_service_status_load_balancer_ingress)
              sourceLabels: [job, __name__]
            - action: labelmap
              regex: (pod|service)
              replacement: service_discovery_${1}
            - action: labeldrop
              regex: (pod|service|container)
        ## kube pod state metrics
        ## kube_pod_status_phase
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.state
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: kube-state-metrics;(?:kube_pod_status_phase)
              sourceLabels: [job, __name__]
            - action: labeldrop
              regex: container
        ## kube container state metrics
        ## kube_pod_container_info
        ## kube_pod_container_resource_limits
        ## kube_pod_container_resource_requests
        ## kube_pod_container_status_ready
        ## kube_pod_container_status_restarts_total
        ## kube_pod_container_status_terminated_reason
        ## kube_pod_container_status_waiting_reason
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.state
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: kube-state-metrics;(?:kube_pod_container_info|kube_pod_container_resource_requests|kube_pod_container_resource_limits|kube_pod_container_status_ready|kube_pod_container_status_terminated_reason|kube_pod_container_status_waiting_reason|kube_pod_container_status_restarts_total)
              sourceLabels: [job, __name__]
        ## controller manager metrics
        ## https://kubernetes.io/docs/concepts/cluster-administration/monitoring/#kube-controller-manager-metrics
        ## e.g.
        ## cloudprovider_aws_api_request_duration_seconds_bucket
        ## cloudprovider_aws_api_request_duration_seconds_count
        ## cloudprovider_aws_api_request_duration_seconds_sum
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.controller-manager
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;cloudprovider_.*_api_request_duration_seconds.*
              sourceLabels: [job, __name__]
        ## scheduler metrics_latency_microseconds
        ## scheduler_e2e_scheduling_duration_seconds_bucket
        ## scheduler_e2e_scheduling_duration_seconds_count
        ## scheduler_e2e_scheduling_duration_seconds_sum
        ## scheduler_binding_duration_seconds_bucket
        ## scheduler_binding_duration_seconds_count
        ## scheduler_binding_duration_seconds_sum
        ## scheduler_framework_extension_point_duration_seconds_bucket
        ## scheduler_framework_extension_point_duration_seconds_count
        ## scheduler_framework_extension_point_duration_seconds_sum
        ## scheduler_scheduling_algorithm_duration_seconds_bucket
        ## scheduler_scheduling_algorithm_duration_seconds_count
        ## scheduler_scheduling_algorithm_duration_seconds_sum
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.scheduler
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: kube-scheduler;scheduler_(?:e2e_scheduling|binding|framework_extension_point|scheduling_algorithm)_duration_seconds.*
              sourceLabels: [job, __name__]
        ## api server metrics:
        ## apiserver_request_count
        ## apiserver_request_total
        ## apiserver_request_duration_seconds_count
        ## apiserver_request_duration_seconds_sum
        ## apiserver_request_latencies_count
        ## apiserver_request_latencies_sum
        ## apiserver_request_latencies_summary
        ## apiserver_request_latencies_summary_count
        ## apiserver_request_latencies_summary_sum
        ## etcd_request_cache_get_duration_seconds_count
        ## etcd_request_cache_get_duration_seconds_sum
        ## etcd_request_cache_add_duration_seconds_count
        ## etcd_request_cache_add_duration_seconds_sum
        ## etcd_request_cache_add_latencies_summary_count
        ## etcd_request_cache_add_latencies_summary_sum
        ## etcd_request_cache_get_latencies_summary_count
        ## etcd_request_cache_get_latencies_summary_sum
        ## etcd_helper_cache_hit_count
        ## etcd_helper_cache_hit_total
        ## etcd_helper_cache_miss_count
        ## etcd_helper_cache_miss_total
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.apiserver
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: apiserver;(?:apiserver_request_(?:count|total)|apiserver_request_(?:duration_seconds|latencies)_(?:count|sum)|apiserver_request_latencies_summary(?:|_count|_sum)|etcd_request_cache_(?:add|get)_(?:duration_seconds|latencies_summary)_(?:count|sum)|etcd_helper_cache_(?:hit|miss)_(?:count|total))
              sourceLabels: [job, __name__]
        ## kubelet metrics:
        ## kubelet_docker_operations_errors
        ## kubelet_docker_operations_errors_total
        ## kubelet_docker_operations_duration_seconds_count
        ## kubelet_docker_operations_duration_seconds_sum
        ## kubelet_runtime_operations_duration_seconds_count
        ## kubelet_runtime_operations_duration_seconds_sum
        ## kubelet_running_container_count
        ## kubelet_running_containers
        ## kubelet_running_pod_count
        ## kubelet_running_pods
        ## kubelet_docker_operations_latency_microseconds
        ## kubelet_docker_operations_latency_microseconds_count
        ## kubelet_docker_operations_latency_microseconds_sum
        ## kubelet_runtime_operations_latency_microseconds
        ## kubelet_runtime_operations_latency_microseconds_count
        ## kubelet_runtime_operations_latency_microseconds_sum
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.kubelet
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;(?:kubelet_docker_operations_errors(?:|_total)|kubelet_(?:docker|runtime)_operations_duration_seconds_(?:count|sum)|kubelet_running_(?:container|pod)(?:_count|s)|kubelet_(:?docker|runtime)_operations_latency_microseconds(?:|_count|_sum))
              sourceLabels: [job, __name__]
        ## cadvisor container metrics
        ## container_cpu_usage_seconds_total
        ## container_fs_limit_bytes
        ## container_fs_usage_bytes
        ## container_memory_working_set_bytes
        ## container_cpu_cfs_throttled_seconds_total
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.container
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: labelmap
              regex: container_name
              replacement: container
            - action: drop
              regex: POD
              sourceLabels: [container]
            - action: keep
              regex: kubelet;.+;(?:container_cpu_usage_seconds_total|container_memory_working_set_bytes|container_fs_usage_bytes|container_fs_limit_bytes|container_cpu_cfs_throttled_seconds_total)
              sourceLabels: [job, container, __name__]
        ## cadvisor aggregate container metrics
        ## container_network_receive_bytes_total
        ## container_network_transmit_bytes_total
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.container
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;(?:container_network_receive_bytes_total|container_network_transmit_bytes_total)
              sourceLabels: [job, __name__]
            - action: labeldrop
              regex: container
        ## node exporter metrics
        ## node_cpu_seconds_total
        ## node_load1
        ## node_load5
        ## node_load15
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.node
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: node-exporter;(?:node_load1|node_load5|node_load15|node_cpu_seconds_total)
              sourceLabels: [job, __name__]
        ## prometheus operator rules
        ## :kube_pod_info_node_count:
        ## :node_cpu_saturation_load1:
        ## :node_cpu_utilisation:avg1m
        ## :node_disk_saturation:avg_irate
        ## :node_disk_utilisation:avg_irate
        ## :node_memory_swap_io_bytes:sum_rate
        ## :node_memory_utilisation:
        ## :node_net_saturation:sum_irate
        ## :node_net_utilisation:sum_irate
        ## cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
        ## cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        ## cluster_quantile:scheduler_framework_extension_point_duration_seconds:histogram_quantile
        ## cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        ## cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        ## instance:node_filesystem_usage:sum  # no rules definition found
        ## instance:node_network_receive_bytes:rate:sum
        ## node:cluster_cpu_utilisation:ratio
        ## node:cluster_memory_utilisation:ratio
        ## node:node_cpu_saturation_load1:
        ## node:node_cpu_utilisation:avg1m
        ## node:node_disk_saturation:avg_irate
        ## node:node_disk_utilisation:avg_irate
        ## node:node_filesystem_avail:
        ## node:node_filesystem_usage:
        ## node:node_inodes_free:
        ## node:node_inodes_total:
        ## node:node_memory_bytes_total:sum
        ## node:node_memory_swap_io_bytes:sum_rate
        ## node:node_memory_utilisation:
        ## node:node_memory_utilisation:ratio
        ## node:node_memory_utilisation_2:
        ## node:node_net_saturation:sum_irate
        ## node:node_net_utilisation:sum_irate
        ## node:node_num_cpu:sum
        ## node_namespace_pod:kube_pod_info:
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.operator.rule
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile|instance:node_filesystem_usage:sum|instance:node_network_receive_bytes:rate:sum|cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile|cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile|cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile|cluster_quantile:scheduler_framework_extension_point_duration_seconds:histogram_quantile|node_namespace_pod:kube_pod_info:|:kube_pod_info_node_count:|node:node_num_cpu:sum|:node_cpu_utilisation:avg1m|node:node_cpu_utilisation:avg1m|node:cluster_cpu_utilisation:ratio|:node_cpu_saturation_load1:|node:node_cpu_saturation_load1:|:node_memory_utilisation:|node:node_memory_bytes_total:sum|node:node_memory_utilisation:ratio|node:cluster_memory_utilisation:ratio|:node_memory_swap_io_bytes:sum_rate|node:node_memory_utilisation:|node:node_memory_utilisation_2:|node:node_memory_swap_io_bytes:sum_rate|:node_disk_utilisation:avg_irate|node:node_disk_utilisation:avg_irate|:node_disk_saturation:avg_irate|node:node_disk_saturation:avg_irate|node:node_filesystem_usage:|node:node_filesystem_avail:|:node_net_utilisation:sum_irate|node:node_net_utilisation:sum_irate|:node_net_saturation:sum_irate|node:node_net_saturation:sum_irate|node:node_inodes_total:|node:node_inodes_free:'
              sourceLabels: [__name__]
        ## health
        ## fluentbit_input_bytes_total
        ## fluentbit_input_files_closed_total
        ## fluentbit_input_files_opened_total
        ## fluentbit_input_files_rotated_total
        ## fluentbit_input_records_total
        ## fluentbit_output_errors_total
        ## fluentbit_output_proc_bytes_total
        ## fluentbit_output_proc_records_total
        ## fluentbit_output_retries_failed_total
        ## fluentbit_output_retries_total
        ## fluentd_output_status_buffer_available_space_ratio
        ## fluentd_output_status_buffer_queue_length
        ## fluentd_output_status_buffer_stage_byte_size
        ## fluentd_output_status_buffer_stage_length
        ## fluentd_output_status_buffer_total_bytes
        ## fluentd_output_status_emit_count
        ## fluentd_output_status_emit_records
        ## fluentd_output_status_flush_time_count
        ## fluentd_output_status_num_errors
        ## fluentd_output_status_queue_byte_size
        ## fluentd_output_status_retry_count
        ## fluentd_output_status_retry_wait
        ## fluentd_output_status_rollback_count
        ## fluentd_output_status_slow_flush_count
        ## fluentd_output_status_write_count
        ## otelcol_otelsvc_k8s_other_added
        ## otelcol_otelsvc_k8s_other_updated
        ## otelcol_otelsvc_k8s_pod_added
        ## otelcol_otelsvc_k8s_pod_deleted
        ## otelcol_otelsvc_k8s_pod_updated
        ## otelcol_process_cpu_seconds
        ## otelcol_process_runtime_heap_alloc_bytes
        ## otelcol_process_runtime_total_alloc_bytes
        ## otelcol_process_runtime_total_sys_memory_bytes
        ## otelcol_queue_length
        ## otelcol_spans_dropped
        ## otelcol_trace_batches_dropped
        ## prometheus_remote_storage_dropped_samples_total
        ## prometheus_remote_storage_enqueue_retries_total
        ## prometheus_remote_storage_failed_samples_total
        ## prometheus_remote_storage_highest_timestamp_in_seconds
        ## prometheus_remote_storage_pending_samples
        ## prometheus_remote_storage_queue_highest_sent_timestamp_seconds
        ## prometheus_remote_storage_retried_samples_total
        ## prometheus_remote_storage_samples_in_total
        ## prometheus_remote_storage_sent_batch_duration_seconds_bucket
        ## prometheus_remote_storage_sent_batch_duration_seconds_count
        ## prometheus_remote_storage_sent_batch_duration_seconds_sum
        ## prometheus_remote_storage_shard_capacity
        ## prometheus_remote_storage_shards
        ## prometheus_remote_storage_shards_desired
        ## prometheus_remote_storage_shards_max
        ## prometheus_remote_storage_shards_min
        ## prometheus_remote_storage_string_interner_zero_reference_releases_total
        ## prometheus_remote_storage_succeeded_samples_total
        ## up
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:up|prometheus_remote_storage_.*|fluentd_.*|fluentbit.*|otelcol.*)
              sourceLabels: [__name__]
        ## control plane metrics
        ## coredns:
        ## coredns_cache_size
        ## coredns_cache_entries
        ## coredns_cache_hits_total
        ## coredns_cache_misses_total
        ## coredns_dns_request_duration_seconds_count
        ## coredns_dns_request_duration_seconds_sum
        ## coredns_dns_request_count_total
        ## coredns_dns_requests_total
        ## coredns_dns_response_rcode_count_total
        ## coredns_dns_responses_total
        ## coredns_forward_request_count_total
        ## coredns_forward_requests_total
        ## process_cpu_seconds_total
        ## process_open_fds
        ## process_resident_memory_bytes
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.control-plane.coredns
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: coredns;(?:coredns_cache_(size|entries|(hits|misses)_total)|coredns_dns_request_duration_seconds_(count|sum)|coredns_(dns_request|dns_response_rcode|forward_request)_count_total|coredns_(forward_requests|dns_requests|dns_responses)_total|process_(cpu_seconds_total|open_fds|resident_memory_bytes))
              sourceLabels: [job, __name__]
        ## etcd server:
        ## etcd_debugging_mvcc_db_total_size_in_bytes
        ## etcd_debugging_store_expires_total
        ## etcd_debugging_store_watchers
        ## etcd_disk_backend_commit_duration_seconds_bucket
        ## etcd_disk_wal_fsync_duration_seconds_bucket
        ## etcd_grpc_proxy_cache_hits_total
        ## etcd_grpc_proxy_cache_misses_total
        ## etcd_network_client_grpc_received_bytes_total
        ## etcd_network_client_grpc_sent_bytes_total
        ## etcd_server_has_leader
        ## etcd_server_leader_changes_seen_total
        ## etcd_server_proposals_applied_total
        ## etcd_server_proposals_committed_total
        ## etcd_server_proposals_failed_total
        ## etcd_server_proposals_pending
        ## process_cpu_seconds_total
        ## process_open_fds
        ## process_resident_memory_bytes
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.control-plane.kube-etcd
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: kube-etcd;(?:etcd_debugging_(mvcc_db_total_size_in_bytes|store_(expires_total|watchers))|etcd_disk_(backend_commit|wal_fsync)_duration_seconds_bucket|etcd_grpc_proxy_cache_(hits|misses)_total|etcd_network_client_grpc_(received|sent)_bytes_total|etcd_server_(has_leader|leader_changes_seen_total)|etcd_server_proposals_(pending|(applied|committed|failed)_total)|process_(cpu_seconds_total|open_fds|resident_memory_bytes))
              sourceLabels: [job, __name__]

        ## Nginx ingress controller metrics
        ## rel: https://docs.nginx.com/nginx-ingress-controller/logging-and-monitoring/prometheus/#available-metrics
        ## nginx_ingress_controller_ingress_resources_total
        ## nginx_ingress_controller_nginx_last_reload_milliseconds
        ## nginx_ingress_controller_nginx_last_reload_status
        ## nginx_ingress_controller_nginx_reload_errors_total
        ## nginx_ingress_controller_nginx_reloads_total
        ## nginx_ingress_controller_virtualserver_resources_total
        ## nginx_ingress_controller_virtualserverroute_resources_total
        ## nginx_ingress_nginx_connections_accepted
        ## nginx_ingress_nginx_connections_active
        ## nginx_ingress_nginx_connections_handled
        ## nginx_ingress_nginx_connections_reading
        ## nginx_ingress_nginx_connections_waiting
        ## nginx_ingress_nginx_connections_writing
        ## nginx_ingress_nginx_http_requests_total
        ## nginx_ingress_nginxplus_connections_accepted
        ## nginx_ingress_nginxplus_connections_active
        ## nginx_ingress_nginxplus_connections_dropped
        ## nginx_ingress_nginxplus_connections_idle
        ## nginx_ingress_nginxplus_http_requests_current
        ## nginx_ingress_nginxplus_http_requests_total
        ## nginx_ingress_nginxplus_resolver_addr
        ## nginx_ingress_nginxplus_resolver_formerr
        ## nginx_ingress_nginxplus_resolver_name
        ## nginx_ingress_nginxplus_resolver_noerror
        ## nginx_ingress_nginxplus_resolver_notimp
        ## nginx_ingress_nginxplus_resolver_nxdomain
        ## nginx_ingress_nginxplus_resolver_refused
        ## nginx_ingress_nginxplus_resolver_servfail
        ## nginx_ingress_nginxplus_resolver_srv
        ## nginx_ingress_nginxplus_resolver_timedout
        ## nginx_ingress_nginxplus_resolver_unknown
        ## nginx_ingress_nginxplus_ssl_handshakes_failed
        ## nginx_ingress_nginxplus_ssl_session_reuses
        ## nginx_ingress_nginxplus_stream_server_zone_connections
        ## nginx_ingress_nginxplus_stream_server_zone_received
        ## nginx_ingress_nginxplus_stream_server_zone_sent
        ## nginx_ingress_nginxplus_stream_upstream_server_active
        ## nginx_ingress_nginxplus_stream_upstream_server_connect_time
        ## nginx_ingress_nginxplus_stream_upstream_server_fails
        ## nginx_ingress_nginxplus_stream_upstream_server_health_checks_fails
        ## nginx_ingress_nginxplus_stream_upstream_server_health_checks_unhealthy
        ## nginx_ingress_nginxplus_stream_upstream_server_received
        ## nginx_ingress_nginxplus_stream_upstream_server_response_time
        ## nginx_ingress_nginxplus_stream_upstream_server_sent
        ## nginx_ingress_nginxplus_stream_upstream_server_unavail
        ## nginx_ingress_nginxplus_stream_upstream_server_state
        ## nginx_ingress_nginxplus_location_zone_discarded
        ## nginx_ingress_nginxplus_location_zone_received
        ## nginx_ingress_nginxplus_location_zone_requests
        ## nginx_ingress_nginxplus_location_zone_responses
        ## nginx_ingress_nginxplus_location_zone_sent
        ## nginx_ingress_nginxplus_server_zone_discarded
        ## nginx_ingress_nginxplus_server_zone_processing
        ## nginx_ingress_nginxplus_server_zone_received
        ## nginx_ingress_nginxplus_server_zone_requests
        ## nginx_ingress_nginxplus_server_zone_responses
        ## nginx_ingress_nginxplus_server_zone_sent
        ## nginx_ingress_nginxplus_upstream_server_fails
        ## nginx_ingress_nginxplus_upstream_server_header_time
        ## nginx_ingress_nginxplus_upstream_server_health_checks_fails
        ## nginx_ingress_nginxplus_upstream_server_health_checks_unhealthy
        ## nginx_ingress_nginxplus_upstream_server_received
        ## nginx_ingress_nginxplus_upstream_server_sent
        ## nginx_ingress_nginxplus_upstream_server_unavail
        ## nginx_ingress_nginxplus_upstream_server_response_time
        ## nginx_ingress_nginxplus_upstream_server_responses
        ## nginx_ingress_nginxplus_upstream_server_requests
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.nginx-ingress
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:nginx_ingress_controller_ingress_resources_total|nginx_ingress_controller_nginx_(last_reload_(milliseconds|status)|reload(s|_errors)_total)|nginx_ingress_controller_virtualserver(|route)_resources_total|nginx_ingress_nginx_connections_(accepted|active|handled|reading|waiting|writing)|nginx_ingress_nginx_http_requests_total|nginx_ingress_nginxplus_(connections_(accepted|active|dropped|idle)|http_requests_(current|total)|resolver_(addr|formerr|name|noerror|notimp|nxdomain|refused|servfail|srv|timedout|unknown)|ssl_(handshakes_failed|session_reuses)|stream_server_zone_(connections|received|sent)|stream_upstream_server_(active|connect_time|fails|health_checks_fails|health_checks_unhealthy|received|response_time|sent|unavail|state)|(location|server)_zone_(discarded|received|requests|responses|sent|processing)|upstream_server_(fails|header_time|health_checks_fails|health_checks_unhealthy|received|sent|unavail|response_time|responses|requests)))
              sourceLabels: [__name__]

        ## Nginx telegraf metrics
        ## nginx_accepts
        ## nginx_active
        ## nginx_handled
        ## nginx_reading
        ## nginx_requests
        ## nginx_waiting
        ## nginx_writing
        ## **************** Nginx Plus telegraf metrics
        ## nginx_plus_api_connections_accepted
        ## nginx_plus_api_connections_active
        ## nginx_plus_api_connections_dropped
        ## nginx_plus_api_connections_idle
        ## nginx_plus_api_http_caches_cold
        ## nginx_plus_api_http_caches_hit_bytes
        ## nginx_plus_api_http_caches_max_size
        ## nginx_plus_api_http_caches_miss_bytes
        ## nginx_plus_api_http_caches_size
        ## nginx_plus_api_http_caches_updating_bytes
        ## nginx_plus_api_http_location_zones_discarded
        ## nginx_plus_api_http_location_zones_received
        ## nginx_plus_api_http_location_zones_requests
        ## nginx_plus_api_http_location_zones_responses_1xx
        ## nginx_plus_api_http_location_zones_responses_2xx
        ## nginx_plus_api_http_location_zones_responses_3xx
        ## nginx_plus_api_http_location_zones_responses_4xx
        ## nginx_plus_api_http_location_zones_responses_5xx
        ## nginx_plus_api_http_location_zones_responses_total
        ## nginx_plus_api_http_location_zones_sent
        ## nginx_plus_api_http_requests_current
        ## nginx_plus_api_http_requests_total
        ## nginx_plus_api_http_server_zones_discarded
        ## nginx_plus_api_http_server_zones_processing
        ## nginx_plus_api_http_server_zones_received
        ## nginx_plus_api_http_server_zones_requests
        ## nginx_plus_api_http_server_zones_responses_1xx
        ## nginx_plus_api_http_server_zones_responses_2xx
        ## nginx_plus_api_http_server_zones_responses_3xx
        ## nginx_plus_api_http_server_zones_responses_4xx
        ## nginx_plus_api_http_server_zones_responses_5xx
        ## nginx_plus_api_http_server_zones_responses_total
        ## nginx_plus_api_http_server_zones_sent
        ## nginx_plus_api_http_upstream_peers_backup
        ## nginx_plus_api_http_upstream_peers_downtime
        ## nginx_plus_api_http_upstream_peers_fails
        ## nginx_plus_api_http_upstream_peers_healthchecks_fails
        ## nginx_plus_api_http_upstream_peers_healthchecks_unhealthy
        ## nginx_plus_api_http_upstream_peers_received
        ## nginx_plus_api_http_upstream_peers_requests
        ## nginx_plus_api_http_upstream_peers_response_time
        ## nginx_plus_api_http_upstream_peers_responses_1xx
        ## nginx_plus_api_http_upstream_peers_responses_2xx
        ## nginx_plus_api_http_upstream_peers_responses_3xx
        ## nginx_plus_api_http_upstream_peers_responses_4xx
        ## nginx_plus_api_http_upstream_peers_responses_5xx
        ## nginx_plus_api_http_upstream_peers_responses_total
        ## nginx_plus_api_http_upstream_peers_sent
        ## nginx_plus_api_http_upstream_peers_unavail
        ## nginx_plus_api_resolver_zones_addr
        ## nginx_plus_api_resolver_zones_formerr
        ## nginx_plus_api_resolver_zones_name
        ## nginx_plus_api_resolver_zones_noerror
        ## nginx_plus_api_resolver_zones_notimp
        ## nginx_plus_api_resolver_zones_nxdomain
        ## nginx_plus_api_resolver_zones_refused
        ## nginx_plus_api_resolver_zones_servfail
        ## nginx_plus_api_resolver_zones_srv
        ## nginx_plus_api_resolver_zones_timedout
        ## nginx_plus_api_ssl_handshakes_failed
        ## nginx_plus_api_ssl_session_reuses
        ## nginx_plus_api_stream_server_zones_connections
        ## nginx_plus_api_stream_server_zones_received
        ## nginx_plus_api_stream_server_zones_sent
        ## nginx_plus_api_stream_upstream_peers_active
        ## nginx_plus_api_stream_upstream_peers_backup
        ## nginx_plus_api_stream_upstream_peers_connect_time
        ## nginx_plus_api_stream_upstream_peers_downtime
        ## nginx_plus_api_stream_upstream_peers_fails
        ## nginx_plus_api_stream_upstream_peers_healthchecks_fails
        ## nginx_plus_api_stream_upstream_peers_healthchecks_last_passed
        ## nginx_plus_api_stream_upstream_peers_healthchecks_unhealthy
        ## nginx_plus_api_stream_upstream_peers_received
        ## nginx_plus_api_stream_upstream_peers_response_time
        ## nginx_plus_api_stream_upstream_peers_sent
        ## nginx_plus_api_stream_upstream_peers_unavail

        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.nginx
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:nginx_(accepts|active|handled|reading|requests|waiting|writing)|nginx_plus_api_connections_(accepted|active|dropped|idle)|nginx_plus_api_http_caches_(cold|hit_bytes|max_size|miss_bytes|size|updating_bytes)|nginx_plus_api_http_location_zones_(discarded|received|requests|sent)|nginx_plus_api_http_location_zones_responses_(1xx|2xx|3xx|4xx|5xx|total)|nginx_plus_api_http_requests_(current|total)|nginx_plus_api_http_server_zones_(discarded|processing|received|requests|sent)|nginx_plus_api_http_server_zones_responses_(1xx|2xx|3xx|4xx|5xx|total)|nginx_plus_api_http_upstream_peers_(backup|downtime|fails|healthchecks_fails|healthchecks_unhealthy|received|requests|sent|unavail|response_time)|nginx_plus_api_http_upstream_peers_responses_(1xx|2xx|3xx|4xx|5xx|total)|nginx_plus_api_resolver_zones_(addr|formerr|name|noerror|notimp|nxdomain|refused|servfail|srv|timedout)|nginx_plus_api_ssl_(handshakes_failed|session_reuses)|nginx_plus_api_stream_server_zones_(connections|received|sent)|nginx_plus_api_stream_upstream_peers_(active|backup|connect_time|downtime|fails|healthchecks_fails|healthchecks_last_passed|healthchecks_unhealthy|received|response_time|sent|unavail))
              sourceLabels: [__name__]

        ## Redis metrics
        ## redis_blocked_clients
        ## redis_clients
        ## redis_cluster_enabled
        ## redis_cmdstat_calls
        ## redis_connected_slaves
        ## redis_evicted_keys
        ## redis_expired_keys
        ## redis_instantaneous_ops_per_sec
        ## redis_keyspace_hitrate
        ## redis_keyspace_hits
        ## redis_keyspace_misses
        ## redis_master_repl_offset
        ## redis_maxmemory
        ## redis_mem_fragmentation_bytes
        ## redis_mem_fragmentation_ratio
        ## redis_rdb_changes_since_last_save
        ## redis_rejected_connections
        ## redis_slave_repl_offset
        ## redis_total_commands_processed
        ## redis_total_net_input_bytes
        ## redis_total_net_output_bytes
        ## redis_tracking_total_keys
        ## redis_uptime
        ## redis_used_cpu_sys
        ## redis_used_cpu_user
        ## redis_used_memory
        ## redis_used_memory_overhead
        ## redis_used_memory_rss
        ## redis_used_memory_startup
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.redis
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:redis_((blocked_|)clients|cluster_enabled|cmdstat_calls|connected_slaves|(evicted|expired|tracking_total)_keys|instantaneous_ops_per_sec|keyspace_(hitrate|hits|misses)|(master|slave)_repl_offset|maxmemory|mem_fragmentation_(bytes|ratio)|rdb_changes_since_last_save|rejected_connections|total_commands_processed|total_net_(input|output)_bytes|uptime|used_(cpu_(sys|user)|memory(_overhead|_rss|_startup|))))
              sourceLabels: [__name__]

        ## JMX Metrics
        ## java_lang_ClassLoading_LoadedClassCount
        ## java_lang_ClassLoading_TotalLoadedClassCount
        ## java_lang_ClassLoading_UnloadedClassCount
        ## java_lang_Compilation_TotalCompilationTime
        ## java_lang_GarbageCollector_CollectionCount
        ## java_lang_GarbageCollector_CollectionTime
        ## java_lang_GarbageCollector_LastGcInfo_GcThreadCount  # unavailable for adoptopenjdk-openj9
        ## java_lang_GarbageCollector_LastGcInfo_duration  # unavailable for adoptopenjdk-openj9
        ## java_lang_GarbageCollector_LastGcInfo_memoryUsageAfterGc_*_used
        ## java_lang_GarbageCollector_LastGcInfo_memoryUsageBeforeGc_*_used
        ## java_lang_GarbageCollector_LastGcInfo_usageAfterGc_*_used  # only for adoptopenjdk-openj9
        ## java_lang_GarbageCollector_LastGcInfo_usageBeforeGc_*_used  # only for adoptopenjdk-openj9
        ## java_lang_MemoryPool_CollectionUsageThresholdSupported
        ## java_lang_MemoryPool_CollectionUsage_committed
        ## java_lang_MemoryPool_CollectionUsage_max
        ## java_lang_MemoryPool_CollectionUsage_used
        ## java_lang_MemoryPool_PeakUsage_committed
        ## java_lang_MemoryPool_PeakUsage_max
        ## java_lang_MemoryPool_PeakUsage_used
        ## java_lang_MemoryPool_UsageThresholdSupported
        ## java_lang_MemoryPool_Usage_committed
        ## java_lang_MemoryPool_Usage_max
        ## java_lang_MemoryPool_Usage_used
        ## java_lang_Memory_HeapMemoryUsage_committed
        ## java_lang_Memory_HeapMemoryUsage_max
        ## java_lang_Memory_HeapMemoryUsage_used
        ## java_lang_Memory_NonHeapMemoryUsage_committed
        ## java_lang_Memory_NonHeapMemoryUsage_max
        ## java_lang_Memory_NonHeapMemoryUsage_used
        ## java_lang_Memory_ObjectPendingFinalizationCount
        ## java_lang_OperatingSystem_AvailableProcessors
        ## java_lang_OperatingSystem_CommittedVirtualMemorySize
        ## java_lang_OperatingSystem_FreeMemorySize  # Added in jdk14
        ## java_lang_OperatingSystem_FreePhysicalMemorySize
        ## java_lang_OperatingSystem_FreeSwapSpaceSize
        ## java_lang_OperatingSystem_MaxFileDescriptorCount
        ## java_lang_OperatingSystem_OpenFileDescriptorCount
        ## java_lang_OperatingSystem_ProcessCpuLoad
        ## java_lang_OperatingSystem_ProcessCpuTime
        ## java_lang_OperatingSystem_SystemCpuLoad
        ## java_lang_OperatingSystem_SystemLoadAverage
        ## java_lang_OperatingSystem_TotalMemorySize  # Added in jdk14
        ## java_lang_OperatingSystem_TotalPhysicalMemorySize
        ## java_lang_OperatingSystem_TotalSwapSpaceSize
        ## java_lang_Runtime_BootClassPathSupported
        ## java_lang_Runtime_Pid  # not available for jdk8
        ## java_lang_Runtime_Uptime
        ## java_lang_Runtime_StartTime
        ## java_lang_Threading_CurrentThreadAllocatedBytes  # Added in jdk14
        ## java_lang_Threading_CurrentThreadCpuTime
        ## java_lang_Threading_CurrentThreadUserTime
        ## java_lang_Threading_DaemonThreadCount
        ## java_lang_Threading_ObjectMonitorUsageSupported
        ## java_lang_Threading_PeakThreadCount
        ## java_lang_Threading_SynchronizerUsageSupported
        ## java_lang_Threading_ThreadAllocatedMemory*  # Not available for adoptopenjdk-openj9
        ## java_lang_Threading_ThreadContentionMonitoring*
        ## java_lang_Threading_ThreadCount
        ## java_lang_Threading_ThreadCpuTime*
        ## java_lang_Threading_TotalStartedThreadCount
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.jmx
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:java_lang_(ClassLoading_(TotalL|Unl|L)oadedClassCount|Compilation_TotalCompilationTime|GarbageCollector_(Collection(Count|Time)|LastGcInfo_(GcThreadCount|duration|(memoryU|u)sage(After|Before)Gc_.*_used))|MemoryPool_(CollectionUsage(ThresholdSupported|_committed|_max|_used)|(Peak|)Usage_(committed|max|used)|UsageThresholdSupported)|Memory_((Non|)HeapMemoryUsage_(committed|max|used)|ObjectPendingFinalizationCount)|OperatingSystem_(AvailableProcessors|(CommittedVirtual|(Free|Total)(Physical|))MemorySize|(Free|Total)SwapSpaceSize|(Max|Open)FileDescriptorCount|ProcessCpu(Load|Time)|System(CpuLoad|LoadAverage))|Runtime_(BootClassPathSupported|Pid|Uptime|StartTime)|Threading_(CurrentThread(AllocatedBytes|(Cpu|User)Time)|(Daemon|Peak|TotalStarted|)ThreadCount|(ObjectMonitor|Synchronizer)UsageSupported|Thread(AllocatedMemory.*|ContentionMonitoring.*|CpuTime.*))))
              sourceLabels: [__name__]

        # Kafka Metrics
        # List of Metrics are on following dochub page:
        # https://help.sumologic.com/docs/integrations/containers-orchestration/kafka/#kafka-metrics
        # Metrics follow following format:
        # kafka_broker_*
        # kafka_controller_*
        # kafka_java_lang_*
        # kafka_partition_*
        # kafka_purgatory_*
        # kafka_network_*
        # kafka_replica_*
        # kafka_request_*
        # kafka_topic_*
        # kafka_topics_*
        # kafka_zookeeper_*
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.kafka
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:kafka_(broker_.*|controller_.*|java_lang_.*|partition_.*|purgatory_.*|network_.*|replica_.*|request_.*|topic_.*|topics_.*|zookeeper_.*))
              sourceLabels: [__name__]

        ## MySQL Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/v1.18.1/plugins/inputs/mysql#metrics
        ## Metrics follow following format:
        ## mysql_uptime
        ## mysql_connection_errors_*
        ## mysql_queries
        ## mysql_slow_queries
        ## mysql_questions
        ## mysql_table_open_cache_*
        ## mysql_table_locks_*
        ## mysql_commands_*
        ## mysql_select_*
        ## mysql_sort_*
        ## mysql_mysqlx_connections_*
        ## mysql_mysqlx_worker_*
        ## mysql_connections
        ## mysql_aborted_*
        ## mysql_locked_connects
        ## mysql_bytes_*
        ## mysql_qcache_*
        ## mysql_threads_*
        ## mysql_opened_*
        ## mysql_created_tmp_*
        ## mysql_innodb_buffer_pool_*
        ## mysql_innodb_data_*
        ## mysql_innodb_rows_*
        ## mysql_innodb_row_lock_*
        ## mysql_innodb_log_waits
        ## mysql_perf_schema_events_statements_*
        ## mysql_perf_schema_table_io_waits_*
        ## mysql_perf_schema_index_io_waits_*
        ## mysql_perf_schema_read*
        ## mysql_perf_schema_write*
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.mysql
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:mysql_((uptime|connection_errors_.*|queries|slow_queries|questions|table_open_cache_.*|table_locks_.*|commands_.*|select_.*|sort_.*|mysqlx_connections_.*|mysqlx_worker_.*|connections|aborted_.*|locked_connects|bytes_.*|qcache_.*|threads_.*|opened_.*|created_tmp_.*)|innodb_(buffer_pool_.*|data_.*|rows_.*|row_lock_.*|log_waits)|perf_schema_(events_statements_.*|table_io_waits_.*|index_io_waits_.*|read.*|write.*)))
              sourceLabels: [__name__]

        ## PostgreSQL Telegraf Metrics
        ## List of Metrics are on following dochub page:
        ## https://help.sumologic.com/docs/integrations/databases/postgresql/#postgresql-metrics
        ## Metrics follow following format:
        ## postgresql_blks_hit
        ## postgresql_blks_read
        ## postgresql_buffers_backend
        ## postgresql_buffers_checkpoint
        ## postgresql_buffers_clean
        ## postgresql_checkpoints_req
        ## postgresql_checkpoints_timed
        ## postgresql_db_size
        ## postgresql_deadlocks
        ## postgresql_flush_lag
        ## postgresql_heap_blks_hit
        ## postgresql_heap_blks_read
        ## postgresql_idx_blks_hit
        ## postgresql_idx_blks_read
        ## postgresql_idx_scan
        ## postgresql_idx_tup_fetch
        ## postgresql_idx_tup_read
        ## postgresql_index_size
        ## postgresql_n_dead_tup
        ## postgresql_n_live_tup
        ## postgresql_n_tup_del
        ## postgresql_n_tup_hot_upd
        ## postgresql_n_tup_ins
        ## postgresql_n_tup_upd
        ## postgresql_num_locks
        ## postgresql_numbackends
        ## postgresql_replay_lag
        ## postgresql_replication_delay
        ## postgresql_replication_lag
        ## postgresql_seq_scan
        ## postgresql_seq_tup_read
        ## postgresql_stat_ssl_compression_count
        ## postgresql_table_size
        ## postgresql_tup_deleted
        ## postgresql_tup_fetched
        ## postgresql_tup_inserted
        ## postgresql_tup_returned
        ## postgresql_tup_updated
        ## postgresql_write_lag
        ## postgresql_xact_commit
        ## postgresql_xact_rollback
        ## postgresql_toast_blks_read
        ## postgresql_toast_blks_hit
        ## postgresql_tidx_blks_read
        ## postgresql_tidx_blks_hit
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.postgresql
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:postgresql_(blks_(hit|read)|buffers_(backend|checkpoint|clean)|checkpoints_(req|timed)|db_size|deadlocks|flush_lag|heap_blks_(hit|read)|idx_blks_(hit|read)|idx_scan|idx_tup_(fetch|read)|index_size|n_dead_tup|n_live_tup|n_tup_(upd|ins|del|hot_upd)|num_locks|numbackends|replay_lag|replication_(delay|lag)|seq_scan|seq_tup_read|stat_ssl_compression_count|table_size|tidx_blks_(hit|read)|toast_blks_(hit|read)|tup_(deleted|fetched|inserted|returned|updated)|write_lag|xact_(commit|rollback)))
              sourceLabels: [__name__]

        ## Apache Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/v1.18.2/plugins/inputs/apache
        ## Metrics follow following format:
        ## apache_BusyWorkers
        ## apache_BytesPerReq
        ## apache_BytesPerSec
        ## apache_CPUChildrenSystem
        ## apache_CPUChildrenUser
        ## apache_CPULoad
        ## apache_CPUSystem
        ## apache_CPUUser
        ## apache_DurationPerReq
        ## apache_IdleWorkers
        ## apache_Load1
        ## apache_Load5
        ## apache_Load15
        ## apache_ParentServerConfigGeneration
        ## apache_ParentServerMPMGeneration
        ## apache_ReqPerSec
        ## apache_ServerUptimeSeconds
        ## apache_TotalAccesses
        ## apache_TotalDuration
        ## apache_TotalkBytes
        ## apache_Uptime
        ## apache_scboard_closing
        ## apache_scboard_dnslookup
        ## apache_scboard_finishing
        ## apache_scboard_idle_cleanup
        ## apache_scboard_keepalive
        ## apache_scboard_logging
        ## apache_scboard_open
        ## apache_scboard_reading
        ## apache_scboard_sending
        ## apache_scboard_starting
        ## apache_scboard_waiting
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.apache
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:apache_((BusyWorkers|BytesPerReq|BytesPerSec|CPUChildrenSystem|CPUChildrenUser|CPULoad|CPUSystem|CPUUser|DurationPerReq|IdleWorkers|Load1|Load15|Load5|ParentServerConfigGeneration|ParentServerMPMGeneration|ReqPerSec|ServerUptimeSeconds|TotalAccesses|TotalDuration|TotalkBytes|Uptime)|(scboard_(closing|dnslookup|finishing|idle_cleanup|keepalive|logging|open|reading|sending|starting|waiting))))
              sourceLabels: [__name__]

        ## SQLServer Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/v1.18.2/plugins/inputs/sqlserver
        ## Metrics follow following format:
        ## sqlserver_cpu_sqlserver_process_cpu
        ## sqlserver_database_io_read_bytes
        ## sqlserver_database_io_read_latency_ms
        ## sqlserver_database_io_write_bytes
        ## sqlserver_database_io_write_latency_ms
        ## sqlserver_memory_clerks_size_kb
        ## sqlserver_performance_value
        ## sqlserver_server_properties_server_memory
        ## sqlserver_volume_space_total_space_bytes
        ## sqlserver_volume_space_used_space_bytes
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.sqlserver
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:sqlserver_(cpu_sqlserver_process_cpu|database_io_(read_(bytes|latency_ms)|write_(bytes|latency_ms))|memory_clerks_size_kb|performance_value|server_properties_server_memory|volume_space_(total_space_bytes|used_space_bytes)))
              sourceLabels: [__name__]

        ## Haproxy Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/v1.18.2/plugins/inputs/haproxy
        ## Metrics follow following format:
        ## haproxy_active_servers
        ## haproxy_backup_servers
        ## haproxy_bin
        ## haproxy_bout
        ## haproxy_chkfail
        ## haproxy_ctime
        ## haproxy_dreq
        ## haproxy_dresp
        ## haproxy_econ
        ## haproxy_ereq
        ## haproxy_eresp
        ## haproxy_http_response_*
        ## haproxy_qcur
        ## haproxy_qmax
        ## haproxy_qtime
        ## haproxy_rate
        ## haproxy_rtime
        ## haproxy_scur
        ## haproxy_slim
        ## haproxy_smax
        ## haproxy_ttime
        ## haproxy_weight
        ## haproxy_wredis
        ## haproxy_wretr
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.haproxy
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:haproxy_(active_servers|backup_servers|bin|bout|chkfail|ctime|dreq|dresp|econ|ereq|eresp|http_response_(1xx|2xx|3xx|4xx|5xx|other)|qcur|qmax|qtime|rate|rtime|scur|slim|smax|ttime|weight|wredis|wretr))
              sourceLabels: [__name__]

        ## Cassandra Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/v1.18.2/plugins/inputs/cassandra
        ## cassandra_CacheMetrics_ChunkCache_OneMinuteRate
        ## cassandra_ClientMetrics_connectedNativeClients_Value
        ## cassandra_ClientMetrics_RequestDiscarded_OneMinuteRate
        ## cassandra_CommitLogMetrics_CompletedTasks_Value
        ## cassandra_CommitLogMetrics_PendingTasks_Value
        ## cassandra_DroppedMessageMetrics_Dropped_OneMinuteRate
        ## cassandra_java_GarbageCollector_*_CollectionCount
        ## cassandra_java_GarbageCollector_*_CollectionTime
        ## cassandra_java_GarbageCollector_*_LastGcInfo_duration
        ## cassandra_java_GarbageCollector_*_LastGcInfo_GcThreadCount
        ## cassandra_java_GarbageCollector_*_LastGcInfo_memoryUsageAfterGc_*_used
        ## cassandra_java_GarbageCollector_*_LastGcInfo_memoryUsageBeforeGc_*_used
        ## cassandra_java_Memory_HeapMemoryUsage_used
        ## cassandra_java_OperatingSystem_AvailableProcessors
        ## cassandra_java_OperatingSystem_FreePhysicalMemorySize
        ## cassandra_java_OperatingSystem_SystemCpuLoad
        ## cassandra_java_OperatingSystem_TotalPhysicalMemorySize
        ## cassandra_java_OperatingSystem_TotalSwapSpaceSize
        ## cassandra_Net_FailureDetector_DownEndpointCount
        ## cassandra_Net_FailureDetector_UpEndpointCount
        ## cassandra_TableMetrics_AllMemtablesHeapSize_Value
        ## cassandra_TableMetrics_AllMemtablesLiveDataSize_Value
        ## cassandra_TableMetrics_CompactionBytesWritten_Count
        ## cassandra_TableMetrics_EstimatedPartitionCount_Value
        ## cassandra_TableMetrics_KeyCacheHitRate_Value
        ## cassandra_TableMetrics_LiveSSTableCount_Value
        ## cassandra_TableMetrics_MemtableColumnsCount_Value
        ## cassandra_TableMetrics_MemtableLiveDataSize_Value
        ## cassandra_TableMetrics_MemtableOffHeapSize_Value
        ## cassandra_TableMetrics_MemtableOnHeapSize_Value
        ## cassandra_TableMetrics_MemtableSwitchCount_Count
        ## cassandra_TableMetrics_PendingCompactions_Value
        ## cassandra_TableMetrics_PendingFlushes_Count
        ## cassandra_TableMetrics_PercentRepaired_Value
        ## cassandra_TableMetrics_RangeLatency_Count
        ## cassandra_TableMetrics_ReadLatency_50thPercentile
        ## cassandra_TableMetrics_ReadLatency_Max
        ## cassandra_TableMetrics_ReadLatency_OneMinuteRate
        ## cassandra_TableMetrics_RowCacheHit_Count
        ## cassandra_TableMetrics_RowCacheMiss_Count
        ## cassandra_TableMetrics_SSTablesPerReadHistogram_50thPercentile
        ## cassandra_TableMetrics_SSTablesPerReadHistogram_99thPercentile
        ## cassandra_TableMetrics_SSTablesPerReadHistogram_Count
        ## cassandra_TableMetrics_SSTablesPerReadHistogram_Max
        ## cassandra_TableMetrics_TombstoneScannedHistogram_50thPercentile
        ## cassandra_TableMetrics_TombstoneScannedHistogram_99thPercentile
        ## cassandra_TableMetrics_TombstoneScannedHistogram_Max
        ## cassandra_TableMetrics_TotalDiskSpaceUsed_Count
        ## cassandra_TableMetrics_WaitingOnFreeMemtableSpace_Max
        ## cassandra_TableMetrics_WriteLatency_50thPercentile
        ## cassandra_TableMetrics_WriteLatency_99thPercentile
        ## cassandra_TableMetrics_WriteLatency_Max
        ## cassandra_TableMetrics_WriteLatency_OneMinuteRate
        ## cassandra_ThreadPoolMetrics_internal_Count
        ## cassandra_ThreadPoolMetrics_internal_Value
        ## cassandra_ThreadPoolMetrics_request_Count
        ## cassandra_ThreadPoolMetrics_request_Value
        ## cassandra_ThreadPoolMetrics_transport_Count
        ## cassandra_ThreadPoolMetrics_transport_Value
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.cassandra
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:cassandra_(CacheMetrics_ChunkCache_OneMinuteRate|ClientMetrics_(connectedNativeClients_Value|RequestDiscarded_OneMinuteRate)|CommitLogMetrics_(CompletedTasks_Value|PendingTasks_Value)|DroppedMessageMetrics_Dropped_OneMinuteRate|java_(GarbageCollector_(ConcurrentMarkSweep|ParNew)_(CollectionCount|CollectionTime|LastGcInfo_duration|LastGcInfo_GcThreadCount|LastGcInfo_memoryUsageAfterGc_.*_used|LastGcInfo_memoryUsageBeforeGc_.*_used)|Memory_HeapMemoryUsage_used|OperatingSystem_(AvailableProcessors|FreePhysicalMemorySize|SystemCpuLoad|TotalPhysicalMemorySize|TotalSwapSpaceSize))|Net_FailureDetector_(DownEndpointCount|UpEndpointCount)|TableMetrics_(AllMemtablesHeapSize_Value|AllMemtablesLiveDataSize_Value|CompactionBytesWritten_Count|EstimatedPartitionCount_Value|KeyCacheHitRate_Value|LiveSSTableCount_Value|MemtableColumnsCount_Value|MemtableLiveDataSize_Value|MemtableOffHeapSize_Value|MemtableOnHeapSize_Value|MemtableSwitchCount_Count|PendingCompactions_Value|PendingFlushes_Count|PercentRepaired_Value|RangeLatency_Count|ReadLatency_50thPercentile|ReadLatency_Max|ReadLatency_OneMinuteRate|RowCacheHit_Count|RowCacheMiss_Count|SSTablesPerReadHistogram_50thPercentile|SSTablesPerReadHistogram_99thPercentile|SSTablesPerReadHistogram_Count|SSTablesPerReadHistogram_Max|TombstoneScannedHistogram_50thPercentile|TombstoneScannedHistogram_99thPercentile|TombstoneScannedHistogram_Max|TotalDiskSpaceUsed_Count|WaitingOnFreeMemtableSpace_Max|WriteLatency_50thPercentile|WriteLatency_99thPercentile|WriteLatency_Max|WriteLatency_OneMinuteRate)|ThreadPoolMetrics_(internal_(Count|Value)|request_(Count|Value)|transport_(Count|Value))))
              sourceLabels: [__name__]

        ## MongoDB Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/master/plugins/inputs/mongodb
        ## Metrics follow following format:
        ## mongodb_active_reads
        ## mongodb_active_writes
        ## mongodb_commands_per_sec
        ## mongodb_connections_current
        ## mongodb_db_stats_storage_size
        ## mongodb_deletes_per_sec
        ## mongodb_document_*
        ## mongodb_flushes_per_sec
        ## mongodb_getmores_per_sec
        ## mongodb_inserts_per_sec
        ## mongodb_net_*_bytes_count
        ## mongodb_open_connections
        ## mongodb_page_faults
        ## mongodb_percent_cache_dirty
        ## mongodb_percent_cache_used
        ## mongodb_queries_per_sec
        ## mongodb_queued_reads
        ## mongodb_queued_writes
        ## mongodb_repl_queries
        ## mongodb_repl_commands_per_sec
        ## mongodb_repl_deletes_per_sec
        ## mongodb_repl_getmores_per_sec
        ## mongodb_repl_inserts_per_sec
        ## mongodb_repl_oplog_window_sec
        ## mongodb_repl_queries_per_sec
        ## mongodb_repl_updates_per_sec
        ## mongodb_resident_megabytes
        ## mongodb_updates_per_sec
        ## mongodb_uptime_ns
        ## mongodb_vsize_megabytes
        ## mongodb_wtcache_bytes_read_into
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.mongodb
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:mongodb_(active_(reads|writes)|commands_per_sec|connections_current|db_stats_storage_size|deletes_per_sec|document_.*|flushes_per_sec|getmores_per_sec|inserts_per_sec|net_.*_bytes_count|open_connections|page_faults|percent_cache_(dirty|used)|queries_per_sec|queued_(reads|writes)|repl_((commands|deletes|getmores|inserts|oplog|queries|updates)_per_sec|queries|oplog_window_sec)|resident_megabytes|updates_per_sec|uptime_ns|vsize_megabytes|wtcache_bytes_read_into))
              sourceLabels: [__name__]

        ## Rabbitmq Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/master/plugins/inputs/rabbitmq
        ## Metrics follow following format:
        ## rabbitmq_exchange_messages_publish_in
        ## rabbitmq_exchange_messages_publish_in_rate
        ## rabbitmq_exchange_messages_publish_out
        ## rabbitmq_exchange_messages_publish_out_rate
        ## rabbitmq_node_disk_free
        ## rabbitmq_node_disk_free_limit
        ## rabbitmq_node_fd_used
        ## rabbitmq_node_gc_num_rate
        ## rabbitmq_node_mem_limit
        ## rabbitmq_node_mem_used
        ## rabbitmq_node_mnesia_disk_tx_count
        ## rabbitmq_node_mnesia_ram_tx_count
        ## rabbitmq_node_uptime
        ## rabbitmq_overview_clustering_listerners
        ## rabbitmq_overview_connections
        ## rabbitmq_overview_consumers
        ## rabbitmq_overview_exchanges
        ## rabbitmq_overview_messages_delivered
        ## rabbitmq_overview_messages_published
        ## rabbitmq_overview_messages_unacked
        ## rabbitmq_overview_queues
        ## rabbitmq_queue_consumers
        ## rabbitmq_queue_memory
        ## rabbitmq_queue_messages_deliver_rate
        ## rabbitmq_queue_messages_max_time
        ## rabbitmq_queue_messages_memory
        ## rabbitmq_queue_messages_publish_rate
        ## rabbitmq_queue_messages_unack
        ## rabbitmq_queue_slave_nodes
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.rabbitmq
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:rabbitmq_(exchange_messages_publish_(in_rate|in|out_rate|out)|node_(disk_free_limit|disk_free|mem_(limit|used)|uptime|fd_used|mnesia_(disk_tx_count|ram_tx_count)|gc_num_rate)|overview_(clustering_listerners|connections|exchanges|consumers|queues|messages_(delivered|published|unacked))|queue_(consumers|memory|slave_nodes|messages_(publish_rate|deliver_rate|memory|max_time|unack))))
              sourceLabels: [__name__]

        ## Tomcat Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/master/plugins/inputs/tomcat
        ## Metrics follow following format:
        ## tomcat_connector_bytes_received
        ## tomcat_connector_bytes_sent
        ## tomcat_connector_current_thread_busy
        ## tomcat_connector_current_thread_count
        ## tomcat_connector_current_threads_busy
        ## tomcat_connector_error_count
        ## tomcat_connector_max_threads
        ## tomcat_connector_max_time
        ## tomcat_connector_processing_time
        ## tomcat_connector_request_count
        ## tomcat_jmx_jvm_memory_HeapMemoryUsage_max
        ## tomcat_jmx_jvm_memory_HeapMemoryUsage_used
        ## tomcat_jmx_jvm_memory_NonHeapMemoryUsage_max
        ## tomcat_jmx_jvm_memory_NonHeapMemoryUsage_used
        ## tomcat_jmx_OperatingSystem_FreePhysicalMemorySize
        ## tomcat_jmx_OperatingSystem_FreeSwapSpaceSize
        ## tomcat_jmx_OperatingSystem_SystemCpuLoad
        ## tomcat_jmx_OperatingSystem_TotalPhysicalMemorySize
        ## tomcat_jmx_OperatingSystem_TotalSwapSpaceSize
        ## tomcat_jmx_Servlet_processingTime
        ## tomcat_jvm_memory_free
        ## tomcat_jvm_memory_max
        ## tomcat_jvm_memory_total
        ## tomcat_jvm_memorypool_bytes_received
        ## tomcat_jvm_memorypool_bytes_sent
        ## tomcat_jvm_memorypool_current_thread_count
        ## tomcat_jvm_memorypool_current_threads_busy
        ## tomcat_jvm_memorypool_error_count
        ## tomcat_jvm_memorypool_max
        ## tomcat_jvm_memorypool_max_threads
        ## tomcat_jvm_memorypool_max_time
        ## tomcat_jvm_memorypool_processing_time
        ## tomcat_jvm_memorypool_request_count
        ## tomcat_jvm_memorypool_used
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.tomcat
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:tomcat_(connector_(bytes_(received|sent)|current_(thread_(busy|count)|threads_busy)|error_count|max_threads|max_time|processing_time|request_count)|jmx_(jvm_memory_(HeapMemoryUsage_(max|used)|NonHeapMemoryUsage_(max|used))|OperatingSystem_(FreePhysicalMemorySize|FreeSwapSpaceSize|SystemCpuLoad|TotalPhysicalMemorySize|TotalSwapSpaceSize)|Servlet_processingTime)|jvm_memory_(free|max|total)|jvm_memorypool_(bytes_(received|sent)|current_thread_count|current_threads_busy|error_count|max_threads|max_time|max|processing_time|request_count|used)))
              sourceLabels: [__name__]

        ## Varnish Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/master/plugins/inputs/varnish
        ## Metrics follow following format:
        ## varnish_backend_busy
        ## varnish_backend_conn
        ## varnish_backend_fail
        ## varnish_backend_recycle
        ## varnish_backend_req
        ## varnish_backend_retry
        ## varnish_backend_reuse
        ## varnish_backend_unhealthy
        ## varnish_bans
        ## varnish_bans_completed
        ## varnish_bans_deleted
        ## varnish_bans_dups
        ## varnish_bans_lurker_contention
        ## varnish_bans_lurker_obj_killed
        ## varnish_bans_lurker_tested
        ## varnish_bans_lurker_tests_tested
        ## varnish_bans_obj
        ## varnish_bans_obj_killed
        ## varnish_bans_persisted_bytes
        ## varnish_bans_persisted_fragmentation
        ## varnish_boot_*_*_bodybytes
        ## varnish_boot_*_*_hdrbytes
        ## varnish_boot_*_bereq_bodybytes
        ## varnish_boot_*_bereq_hdrbytes
        ## varnish_cache_hit
        ## varnish_cache_hit_grace
        ## varnish_cache_hitpass
        ## varnish_cache_miss
        ## varnish_client_req
        ## varnish_client_req_400
        ## varnish_client_req_417
        ## varnish_client_resp_500
        ## varnish_n_backend
        ## varnish_n_expired
        ## varnish_n_lru_nuked
        ## varnish_n_vcl_avail
        ## varnish_pools
        ## varnish_s0_g_bytes
        ## varnish_s0_g_space
        ## varnish_s_fetch
        ## varnish_s_pipe_in
        ## varnish_s_pipe_out
        ## varnish_s_req_bodybytes
        ## varnish_s_req_hdrbytes
        ## varnish_s_resp_bodybytes
        ## varnish_s_resp_hdrbytes
        ## varnish_s_sess
        ## varnish_sess_closed
        ## varnish_sess_closed_err
        ## varnish_sess_conn
        ## varnish_sess_drop
        ## varnish_sess_dropped
        ## varnish_sess_fail
        ## varnish_sess_queued
        ## varnish_thread_queue_len
        ## varnish_threads
        ## varnish_threads_created
        ## varnish_threads_destroyed
        ## varnish_threads_failed
        ## varnish_threads_limited
        ## varnish_uptime
        ## varnish_vmods
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.varnish
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:varnish_(backend_(busy|conn|fail|recycle|req|retry|reuse|unhealthy)|bans_(completed|deleted|dups|lurker_(contention|obj_killed|tests_tested|tested|)|obj_killed|obj|persisted_(bytes|fragmentation))|bans|boot_.*_.*_(bodybytes|hdrbytes)|cache_(hit_grace|hitpass|miss|hit)|client_(req_400|req_417|req|resp_500)|n_(backend|expired|lru_nuked|vcl_avail)|pools|s0_g_(bytes|space)|s_(fetch|pipe_(in|out)|req_(bodybytes|hdrbytes)|resp_(bodybytes|hdrbytes)|sess)|sess_(closed_err|closed|conn|drop|dropped|fail|queued)|thread_queue_len|threads_(created|destroyed|failed|limited)|threads|uptime|vmods))
              sourceLabels: [__name__]

        ## Memcached Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/master/plugins/inputs/memcache
        ## Metrics follow following format:
        ## memcached_accepting_conns
        ## memcached_auth_cmds
        ## memcached_auth_errors
        ## memcached_bytes
        ## memcached_bytes_read
        ## memcached_bytes_written
        ## memcached_cas_*
        ## memcached_cas_*
        ## memcached_cmd_*
        ## memcached_cmd_flush
        ## memcached_cmd_get
        ## memcached_cmd_set
        ## memcached_cmd_touch
        ## memcached_conn_yields
        ## memcached_connection_structures
        ## memcached_curr_connections
        ## memcached_curr_items
        ## memcached_decr_*
        ## memcached_delete_*
        ## memcached_evictions
        ## memcached_get_hits
        ## memcached_get_misses
        ## memcached_hash_bytes
        ## memcached_hash_is_expanding
        ## memcached_incr_*
        ## memcached_limit_maxbytes
        ## memcached_listen_disabled_num
        ## memcached_reclaimed
        ## memcached_threads
        ## memcached_total_connections
        ## memcached_total_items
        ## memcached_uptime
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.memcached
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:memcached_(accepting_conns|auth_(cmds|errors)|bytes_(read|written)|bytes|cas_*|cmd_.*|conn_yields|connection_structures|curr_(connections|items)|decr_.*|delete_.*|evictions|get_(hits|misses)|hash_(bytes|is_expanding)|incr_.*|limit_maxbytes|listen_disabled_num|reclaimed|threads|total_(connections|items)|uptime))
              sourceLabels: [__name__]

        ## Elasticsearch Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/master/plugins/inputs/elasticsearch
        ## elasticsearch_cluster_health_active_primary_shards
        ## elasticsearch_cluster_health_active_shards
        ## elasticsearch_cluster_health_delayed_unassigned_shards
        ## elasticsearch_cluster_health_indices_status_code
        ## elasticsearch_cluster_health_initializing_shards
        ## elasticsearch_cluster_health_number_of_data_nodes
        ## elasticsearch_cluster_health_number_of_nodes
        ## elasticsearch_cluster_health_number_of_pending_tasks
        ## elasticsearch_cluster_health_relocating_shards
        ## elasticsearch_cluster_health_unassigned_shards
        ## elasticsearch_clusterstats_indices_fielddata_evictions
        ## elasticsearch_clusterstats_nodes_jvm_mem_heap_used_in_bytes
        ## elasticsearch_fs_total_free_in_bytes
        ## elasticsearch_fs_total_total_in_bytes
        ## elasticsearch_indices_flush_total
        ## elasticsearch_indices_flush_total_time_in_millis
        ## elasticsearch_indices_get_exists_time_in_millis
        ## elasticsearch_indices_get_exists_total
        ## elasticsearch_indices_get_missing_time_in_millis
        ## elasticsearch_indices_get_missing_total
        ## elasticsearch_indices_get_time_in_millis
        ## elasticsearch_indices_get_total
        ## elasticsearch_indices_indexing_delete_time_in_millis
        ## elasticsearch_indices_indexing_delete_total
        ## elasticsearch_indices_indexing_index_time_in_millis
        ## elasticsearch_indices_indexing_index_total
        ## elasticsearch_indices_merges_total_time_in_millis
        ## elasticsearch_indices_search_query_time_in_millis
        ## elasticsearch_indices_search_query_total
        ## elasticsearch_indices_segments_fixed_bit_set_memory_in_bytes
        ## elasticsearch_indices_segments_terms_memory_in_bytes
        ## elasticsearch_indices_stats_primaries_docs_count
        ## elasticsearch_indices_stats_primaries_indexing_index_time_in_millis
        ## elasticsearch_indices_stats_primaries_query_cache_cache_size
        ## elasticsearch_indices_stats_primaries_query_cache_evictions
        ## elasticsearch_indices_stats_primaries_segments_doc_values_memory_in_bytes
        ## elasticsearch_indices_stats_primaries_segments_index_writer_memory_in_bytes
        ## elasticsearch_indices_stats_primaries_segments_memory_in_bytes
        ## elasticsearch_indices_stats_total___fielddata_memory_size_in_bytes
        ## elasticsearch_indices_stats_total___indexing_index_total
        ## elasticsearch_indices_stats_total___merges_total
        ## elasticsearch_indices_stats_total_docs_count
        ## elasticsearch_indices_stats_total_fielddata_memory_size_in_bytes
        ## elasticsearch_indices_stats_total_flush_total_time_in_millis
        ## elasticsearch_indices_stats_total_indexing_delete_total
        ## elasticsearch_indices_stats_total_indexing_index_time_in_millis
        ## elasticsearch_indices_stats_total_indexing_index_total
        ## elasticsearch_indices_stats_total_merges_total_docs
        ## elasticsearch_indices_stats_total_merges_total_size_in_bytes
        ## elasticsearch_indices_stats_total_merges_total_time_in_millis
        ## elasticsearch_indices_stats_total_query_cache_evictions
        ## elasticsearch_indices_stats_total_refresh_total
        ## elasticsearch_indices_stats_total_refresh_total_time_in_millis
        ## elasticsearch_indices_stats_total_search_fetch_time_in_millis
        ## elasticsearch_indices_stats_total_search_fetch_total
        ## elasticsearch_indices_stats_total_search_query_time_in_millis
        ## elasticsearch_indices_stats_total_search_query_total
        ## elasticsearch_indices_stats_total_segments_fixed_bit_set_memory_in_bytes
        ## elasticsearch_indices_stats_total_segments_index_writer_memory_in_bytes
        ## elasticsearch_indices_stats_total_segments_memory_in_bytes
        ## elasticsearch_indices_stats_total_segments_terms_memory_in_bytes
        ## elasticsearch_indices_stats_total_store_size_in_bytes
        ## elasticsearch_indices_stats_total_translog_operations
        ## elasticsearch_indices_stats_total_translog_size_in_bytes
        ## elasticsearch_jvm_gc_collectors_*_collection_time_in_millis
        ## elasticsearch_jvm_mem_heap_committed_in_bytes
        ## elasticsearch_jvm_mem_heap_used_in_bytes
        ## elasticsearch_jvm_mem_heap_used_percent
        ## elasticsearch_os_cpu_load_average_5m
        ## elasticsearch_os_cpu_percent
        ## elasticsearch_process_open_file_descriptors
        ## elasticsearch_thread_pool_analyze_completed
        ## elasticsearch_thread_pool_analyze_threads
        ## elasticsearch_thread_pool_get_rejected
        ## elasticsearch_thread_pool_search_queue
        ## elasticsearch_transport_rx_size_in_bytes
        ## elasticsearch_transport_tx_size_in_bytes
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.elasticsearch
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:elasticsearch_(cluster_health_(active_(primary_shards|shards)|delayed_unassigned_shards|indices_status_code|initializing_shards|number_of_(data_nodes|nodes|pending_tasks)|relocating_shards|unassigned_shards)|clusterstats_(indices_fielddata_evictions|nodes_jvm_mem_heap_used_in_bytes)|fs_total_(free_in_bytes|total_in_bytes)|indices_(flush_(total|total_time_in_millis)|get_(exists_time_in_millis|exists_total|missing_time_in_millis|missing_total|time_in_millis|total)|indexing_delete_time_in_millis|indexing_delete_total|indexing_index_time_in_millis|indexing_index_total|merges_total_time_in_millis|search_query_time_in_millis|search_query_total|segments_fixed_bit_set_memory_in_bytes|segments_terms_memory_in_bytes|stats_primaries_(docs_count|indexing_index_time_in_millis|query_cache_cache_size|query_cache_evictions|segments_doc_values_memory_in_bytes|segments_index_writer_memory_in_bytes|segments_memory_in_bytes)|stats_total___(fielddata_memory_size_in_bytes|indexing_index_total|merges_total)|stats_total_(docs_count|fielddata_memory_size_in_bytes|flush_total_time_in_millis|indexing_delete_total|indexing_index_time_in_millis|indexing_index_total|merges_total_docs|merges_total_size_in_bytes|merges_total_time_in_millis|query_cache_evictions|refresh_total|refresh_total_time_in_millis|search_fetch_time_in_millis|search_fetch_total|search_query_time_in_millis|search_query_total|segments_fixed_bit_set_memory_in_bytes|segments_index_writer_memory_in_bytes|segments_memory_in_bytes|segments_terms_memory_in_bytes|store_size_in_bytes|translog_operations|translog_size_in_bytes))|jvm_(gc_collectors_.*_collection_time_in_millis|mem_heap_committed_in_bytes|mem_heap_used_in_bytes|mem_heap_used_percent)|os_cpu_(load_average_5m|percent)|process_open_file_descriptors|thread_pool_(analyze_completed|analyze_threads|get_rejected|search_queue)|transport_(rx_size_in_bytes|tx_size_in_bytes)))
              sourceLabels: [__name__]

        ## Activemq Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/master/plugins/inputs/activemq
        ## activemq_queue_*
        ## activemq_topic_*
        ## activemq_*_QueueSize
        ## activemq_broker_AverageMessageSize
        ## activemq_broker_CurrentConnectionsCount
        ## activemq_broker_MemoryLimit
        ## activemq_broker_StoreLimit
        ## activemq_broker_TempLimit
        ## activemq_broker_TotalConnectionsCount
        ## activemq_broker_TotalConsumerCount
        ## activemq_broker_TotalDequeueCount
        ## activemq_broker_TotalEnqueueCount
        ## activemq_broker_TotalMessageCount
        ## activemq_broker_TotalProducerCount
        ## activemq_broker_UptimeMillis
        ## activemq_jvm_memory_HeapMemoryUsage_max
        ## activemq_jvm_memory_HeapMemoryUsage_used
        ## activemq_jvm_memory_NonHeapMemoryUsage_used
        ## activemq_jvm_runtime_Uptime
        ## activemq_OperatingSystem_FreePhysicalMemorySize
        ## activemq_OperatingSystem_SystemCpuLoad
        ## activemq_OperatingSystem_TotalPhysicalMemorySize
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.activemq
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:activemq_(topic_.*|queue_.*|.*_QueueSize|broker_(AverageMessageSize|CurrentConnectionsCount|MemoryLimit|StoreLimit|TempLimit|TotalConnectionsCount|TotalConsumerCount|TotalDequeueCount|TotalEnqueueCount|TotalMessageCount|TotalProducerCount|UptimeMillis)|jvm_memory_(HeapMemoryUsage_max|HeapMemoryUsage_used|NonHeapMemoryUsage_used)|jvm_runtime_Uptime|OperatingSystem_(FreePhysicalMemorySize|SystemCpuLoad|TotalPhysicalMemorySize)))
              sourceLabels: [__name__]

        ## Couchbase Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://github.com/influxdata/telegraf/tree/master/plugins/inputs/couchbase
        ## couchbase_node_memory_free
        ## couchbase_node_memory_total
        ## couchbase_bucket_item_count
        ## couchbase_bucket_curr_connections
        ## couchbase_bucket_ops_per_sec
        ## couchbase_bucket_ep_num_value_ejects
        ## couchbase_bucket_disk_write_queue
        ## couchbase_bucket_ep_oom_errors
        ## couchbase_bucket_delete_misses
        ## couchbase_bucket_delete_hits
        ## couchbase_bucket_bytes_read
        ## couchbase_bucket_bytes_written
        ## couchbase_bucket_cmd_get
        ## couchbase_bucket_cmd_set
        ## couchbase_bucket_cas_hits
        ## couchbase_bucket_ops
        ## couchbase_bucket_curr_items
        ## couchbase_bucket_mem_actual_free
        ## couchbase_bucket_cpu_utilization_rate
        ## couchbase_bucket_swap_used
        ## couchbase_bucket_disk_used
        ## couchbase_bucket_rest_requests
        ## couchbase_bucket_hibernated_waked
        ## couchbase_bucket_mem_used
        ## couchbase_bucket_xdc_ops
        ## couchbase_bucket_ep_mem_low_wat
        ## couchbase_bucket_ep_mem_high_wat
        ## couchbase_bucket_ep_ops_update
        ## couchbase_bucket_ep_tmp_oom_errors
        ## couchbase_bucket_ep_dcp_replica_count
        ## couchbase_bucket_ep_dcp_replica_producer_count
        ## couchbase_bucket_ep_dcp_xdcr_producer_count
        ## couchbase_bucket_ep_dcp_replica_items_remaining
        ## couchbase_bucket_ep_dcp_xdcr_items_remaining
        ## couchbase_bucket_ep_dcp_replica_items_sent
        ## couchbase_bucket_ep_dcp_xdcr_items_sent
        ## couchbase_bucket_ep_dcp_replica_total_bytes
        ## couchbase_bucket_ep_dcp_xdcr_total_bytes
        ## couchbase_bucket_ep_num_ops_get_meta
        ## couchbase_bucket_ep_num_ops_set_meta
        ## couchbase_bucket_ep_num_ops_del_meta
        ## couchbase_bucket_ep_dcp_xdcr_count
        ## couchbase_bucket_ep_resident_items_rate
        ## couchbase_bucket_vb_active_queue_size
        ## couchbase_bucket_vb_replica_queue_size
        ## couchbase_bucket_vb_pending_queue_size
        ## couchbase_bucket_vb_active_queue_fill
        ## couchbase_bucket_vb_replica_queue_fill
        ## couchbase_bucket_vb_pending_queue_fill
        ## couchbase_bucket_vb_avg_active_queue_age
        ## couchbase_bucket_vb_avg_replica_queue_age
        ## couchbase_bucket_vb_avg_pending_queue_age
        ## couchbase_bucket_vb_active_num
        ## couchbase_bucket_vb_replica_num
        ## couchbase_bucket_vb_pending_num
        ## couchbase_bucket_vb_pending_curr_items
        ## couchbase_bucket_vb_active_resident_items_ratio
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.couchbase
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:couchbase_(node_.*|bucket_(ep_.*|vb_.*|delete_.*|cmd.*|bytes_.*|item_count|curr_connections|ops_per_sec|disk_write_queue|mem_.*|cas_hits|ops|curr_items|cpu_utilization_rate|swap_used|disk_used|rest_requests|hibernated_waked|xdc_ops)))
              sourceLabels: [__name__]

        ## SquidProxy Telegraf Metrics
        ## List of Metrics are on following github page:
        ## https://wiki.squid-cache.org/Features/Snmp
        ## squid_cacheIpEntries
        ## squid_cacheIpRequests
        ## squid_cacheIpHits
        ## squid_cacheFqdnEntries
        ## squid_cacheFqdnRequests
        ## squid_cacheFqdnMisses
        ## squid_cacheFqdnNegativeHits
        ## squid_cacheDnsRequests
        ## squid_cacheDnsReplies
        ## squid_cacheDnsSvcTime5
        ## squid_cacheSysPageFaults
        ## squid_cacheSysNumReads
        ## squid_cacheCurrentFileDescrCnt
        ## squid_cacheCurrentUnusedFDescrCnt
        ## squid_cacheCurrentResFileDescrCnt
        ## squid_cacheServerRequests
        ## squid_cacheServerInKb
        ## squid_cacheServerOutKb
        ## squid_cacheHttpAllSvcTime5
        ## squid_cacheHttpErrors
        ## squid_cacheHttpInKb
        ## squid_cacheHttpOutKb
        ## squid_cacheHttpAllSvcTime1
        ## squid_cacheMemMaxSize
        ## squid_cacheMemUsage
        ## squid_cacheNumObjCount
        ## squid_cacheCpuTime
        ## squid_cacheMaxResSize
        ## squid_cacheProtoClientHttpRequests
        ## squid_cacheClients
        ## squid_uptime
        - url: http://$(FLUENTD_METRICS_SVC).$(NAMESPACE).svc.cluster.local.:9888/prometheus.metrics.applications.squidproxy
          remoteTimeout: 5s
          writeRelabelConfigs:
            - action: keep
              regex: (?:squid_(uptime|cache(Ip(Entries|Requests|Hits)|Fqdn(Entries|Requests|Misses|NegativeHits)|Dns(Requests|Replies|SvcTime5)|Sys(PageFaults|NumReads)|Current(FileDescrCnt|UnusedFDescrCnt|ResFileDescrCnt)|Server(Requests|InKb|OutKb)|Http(AllSvcTime5|Errors|InKb|OutKb|AllSvcTime1)|Mem(MaxSize|Usage)|NumObjCount|CpuTime|MaxResSize|ProtoClientHttpRequests|Clients)))
              sourceLabels: [__name__]

      serviceMonitor:
        selfMonitor: false

## Configure optional OpenTelemetry Collector in Agent mode
otelagent:
  enabled: true
  daemonset:
    nodeSelector: {}
    tolerations: []
    resources:
      limits:
        memory: 2Gi
        cpu: 1000m
      requests:
        memory: 1Gi
        cpu: 500m
    ## Option to define priorityClassName to assign a priority class to pods.
    ## If not set then temaplates/priorityclass.yaml is used.
    priorityClassName:
    ## Add custom labels only to otelagent daemonset.
    podLabels: {}
    ## Add custom annotations only to otelagent daemonset.
    podAnnotations: {}
    image:
      repository: "public.ecr.aws/sumologic/sumologic-otel-collector"
      tag: "0.54.0-sumo-0"
      pullPolicy: IfNotPresent

    ## Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #   - name: VALUE_FROM_SECRET
    #     valueFrom:
    #       secretKeyRef:
    #         name: secret_name
    #         key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

  config:
    receivers:
      jaeger:
        protocols:
          thrift_compact:
            endpoint: "0.0.0.0:6831"
          thrift_binary:
            endpoint: "0.0.0.0:6832"
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_http:
            endpoint: "0.0.0.0:14268"
      opencensus:
        endpoint: "0.0.0.0:55678"
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
      otlp/deprecated:
        protocols:
          http:
            endpoint: "0.0.0.0:55681"
      zipkin:
        endpoint: "0.0.0.0:9411"
    processors:
      ## Tags spans with K8S metadata, basing on the context IP
      k8s_tagger:
        ## When true, only IP is assigned and passed (so it could be tagged on another collector)
        passthrough: false
        ## When true, additional fields, such as serviceName are being also extracted
        owner_lookup_enabled: true
        ## Extracted fields and assigned names
        extract:
          metadata:
            ## extract the following well-known metadata fields
            - containerId
            - containerName
            - daemonSetName
            - deploymentName
            - hostName
            - namespace
            - nodeName
            - podId
            - podName
            - replicaSetName
            - serviceName
            - statefulSetName
          annotations:
            - tag_name: "k8s.pod.annotation.%s"
              key: "*"
          namespace_labels:
            - tag_name: "k8s.namespace.label.%s"
              key: "*"
          labels:
            - tag_name: "k8s.pod.label.%s"
              key: "*"

      ## The memory_limiter processor is used to prevent out of memory situations on the collector.
      memory_limiter:
        ## check_interval is the time between measurements of memory usage for the
        ## purposes of avoiding going over the limits. Defaults to zero, so no
        ## checks will be performed. Values below 1 second are not recommended since
        ## it can result in unnecessary CPU consumption.
        check_interval: 5s

        ## Maximum amount of memory, in MiB, targeted to be allocated by the process heap.
        ## Note that typically the total memory usage of process will be about 50MiB higher
        ## than this value.
        limit_mib: 1900

      ## The batch processor accepts spans and places them into batches grouped by node and resource
      batch:
        ## Number of spans after which a batch will be sent regardless of time
        send_batch_size: 256
        ## Time duration after which a batch will be sent regardless of size
        timeout: 5s
    extensions:
      health_check: {}
      memory_ballast:
        ## Memory Ballast size should be max 1/3 to 1/2 of memory.
        size_mib: 250
      pprof: {}
    exporters:
      otlphttp/traces:
        endpoint: 'http://exporters.otlptraces.endpoint.replace:4318'
      otlphttp/metrics:
        endpoint: 'http://exporters.otlpmetrics.endpoint.replace:4318'
    service:
      extensions: [health_check, memory_ballast, pprof]
      pipelines:
        traces:
          receivers: [jaeger, opencensus, otlp, otlp/deprecated, zipkin]
          processors: [memory_limiter, k8s_tagger, batch]
          exporters: [otlphttp/traces]
        metrics:
          receivers: [otlp, otlp/deprecated]
          processors: [memory_limiter, k8s_tagger, batch]
          exporters: [otlphttp/metrics]

## Configure otelcol
otelcol:
  sourceMetadata:
    ## Set the _sourceName metadata field in Sumo Logic.
    sourceName: "%{k8s.namespace.name}.%{k8s.pod.pod_name}.%{k8s.container.name}"
    ## Set the _sourceCategory metadata field in Sumo Logic.
    sourceCategory: "%{k8s.namespace.name}/%{k8s.pod.pod_name}"
    ## Set the prefix, for _sourceCategory metadata.
    sourceCategoryPrefix: "kubernetes/"
    ## Used to replace - with another character.
    sourceCategoryReplaceDash: "/"

    ## A regular expression for containers.
    ## Matching containers will be excluded from Sumo. The logs will still be sent to FluentD.
    excludeContainerRegex: ""
    ## A regular expression for hosts.
    ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
    excludeHostRegex: ""
    ## A regular expression for namespaces.
    ## Matching namespaces will be excluded from Sumo. The logs will still be sent to FluentD.
    excludeNamespaceRegex: ""
    ## A regular expression for pods.
    ## Matching pods will be excluded from Sumo. The logs will still be sent to FluentD.
    excludePodRegex: ""

  deployment:
    nodeSelector: {}
    tolerations: []
    replicas: 1
    resources:
      limits:
        memory: 2Gi
        cpu: 1000m
      requests:
        memory: 384Mi
        cpu: 200m
    ## Option to define priorityClassName to assign a priority class to pods.
    priorityClassName:

    ## Add custom labels only to otelcol deployment.
    podLabels: {}
    ## Add custom annotations only to otelcol deployment.
    podAnnotations: {}
    image:
      repository: "public.ecr.aws/sumologic/sumologic-otel-collector"
      tag: "0.54.0-sumo-0"
      pullPolicy: IfNotPresent

    ## Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #   - name: VALUE_FROM_SECRET
    #     valueFrom:
    #       secretKeyRef:
    #         name: secret_name
    #         key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

  ## To enable collecting all logs, set to false
  logLevelFilter: false
  ## Metrics from Collector
  metrics:
    enabled: true
  ## Collector configuration
  config:
    receivers:
      jaeger:
        protocols:
          thrift_compact:
            endpoint: "0.0.0.0:6831"
          thrift_binary:
            endpoint: "0.0.0.0:6832"
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_http:
            endpoint: "0.0.0.0:14268"
      opencensus:
        endpoint: "0.0.0.0:55678"
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
      otlp/deprecated:
        protocols:
          http:
            endpoint: "0.0.0.0:55681"
      zipkin:
        endpoint: "0.0.0.0:9411"
    processors:
      ## Source processor adds Sumo Logic related metadata
      source:
        annotation_prefix: "k8s.pod.annotation."
        collector: "processors.source.collector.replace"
        exclude:
          k8s.container.name: "processors.source.exclude_container_regex.replace"
          k8s.host.name: "processors.source.exclude_host_regex.replace"
          k8s.namespace.name: "processors.source.exclude_namespace_regex.replace"
          k8s.pod.name: "processors.source.exclude_pod_regex.replace"
        pod_key: "k8s.pod.name"
        pod_name_key: "k8s.pod.pod_name"
        pod_template_hash_key: "k8s.pod.label.pod-template-hash"
        source_category: "processors.source.category.replace"
        source_category_prefix: "processors.source.category_prefix.replace"
        source_category_replace_dash: "processors.source.category_replace_dash.replace"
        source_host: "%{k8s.pod.hostname}"
        source_name: "processors.source.name.replace"

      ## Resource processor sets the associted cluster attribute
      resource:
        attributes:
          - key: k8s.cluster.name
            value: "processors.resource.cluster.replace"
            action: upsert

      ## The memory_limiter processor is used to prevent out of memory situations on the collector.
      memory_limiter:
        ## check_interval is the time between measurements of memory usage for the
        ## purposes of avoiding going over the limits. Defaults to zero, so no
        ## checks will be performed. Values below 1 second are not recommended since
        ## it can result in unnecessary CPU consumption.
        check_interval: 5s

        ## Maximum amount of memory, in MiB, targeted to be allocated by the process heap.
        ## Note that typically the total memory usage of process will be about 50MiB higher
        ## than this value.
        limit_mib: 1900

      ## Smart cascading filtering rules with preset limits.
      cascading_filter:
        ## Max number of traces for which decisions are kept in memory
        num_traces: 200000

      ## The batch processor accepts spans and places them into batches grouped by node and resource
      batch:
        ## Number of spans after which a batch will be sent regardless of time
        send_batch_size: 256
        ## Never more than this many spans are being sent in a batch
        send_batch_max_size: 512
        ## Time duration after which a batch will be sent regardless of size
        timeout: 5s

      ## Add System metadata - needed for Sumologic exporter source_host
      resourcedetection:
        detectors: [system]
        timeout: 10s
        override: false

    extensions:
      health_check: {}
      memory_ballast:
        ## Memory Ballast size should be max 1/3 to 1/2 of memory.
        size_mib: 683
      pprof: {}
    exporters:
      ## Following generates verbose logs with span content, useful to verify what
      ## metadata is being tagged. To enable, uncomment and add "logging" to exporters below.
      ## There are two levels that could be used: `debug` and `info` with the former
      ## being much more verbose and including (sampled) spans content
      # logging:
      #   loglevel: debug
      otlphttp:
        traces_endpoint: ${SUMO_ENDPOINT_DEFAULT_TRACES_SOURCE}
        compression: gzip
      sumologic:
        endpoint: ${SUMO_ENDPOINT_DEFAULT_METRICS_SOURCE}
        ## Compression encoding format, either empty string (""), gzip or deflate (default gzip).
        ## Empty string means no compression
        compress_encoding: gzip
        ## Max HTTP request body size in bytes before compression (if applied). By default 1_048_576 (1MB) is used.
        max_request_body_size: 1_048_576  # 1MB
        ## Format to use when sending logs to Sumo. (default json) (possible values: json, text)
        log_format: text
        ## Format of the metrics to be sent (default is prometheus) (possible values: carbon2, prometheus)
        ## carbon2 and graphite are going to be supported soon.
        metric_format: prometheus
        ## Desired source category. Useful if you want to override the source category configured for the source.
        source_category: "exporters.sumologic.source_category.replace"
        ## Desired source name. Useful if you want to override the source name configured for the source.
        source_name: "exporters.sumologic.source_name.replace"
        ## Desired host name. Useful if you want to override the source host configured for the source.
        source_host: "%{k8s.pod.hostname}"
        ## Timeout for every attempt to send data to Sumo Logic backend. Maximum connection timeout is 55s.
        timeout: 5s
        retry_on_failure:
          enabled: true
          ## Time to wait after the first failure before retrying
          initial_interval: 5s
          ## Upper bound on backoff
          max_interval: 30s
          ## Maximum amount of time spent trying to send a batch
          max_elapsed_time: 120s
        sending_queue:
          enabled: false
          ## Number of consumers that dequeue batches
          num_consumers: 10
          ## Maximum number of batches kept in memory before data
          ## User should calculate this as num_seconds * requests_per_second where:
          ## num_seconds is the number of seconds to buffer in case of a backend outage
          ## requests_per_second is the average number of requests per seconds.
          queue_size: 5000
    service:
      extensions: [health_check, memory_ballast, pprof]
      pipelines:
        traces:
          receivers: [jaeger, opencensus, otlp, otlp/deprecated, zipkin]
          processors: [memory_limiter, source, resource, cascading_filter, batch]
          exporters: [otlphttp]
        metrics:
          receivers: [otlp, otlp/deprecated]
          processors: [memory_limiter, resourcedetection, source, resource, batch]
          exporters: [sumologic]

metadata:
  ## Configure image for Opentelemetry Collector (for logs and metrics)
  image:
    repository: public.ecr.aws/sumologic/sumologic-otel-collector
    tag: 0.57.2-sumo-1
    pullPolicy: IfNotPresent

  securityContext:
    ## ToDo: Verify following comment
    ## The group ID of all processes in the statefulset containers. By default this needs to be otelcol(999).
    fsGroup: 999

  ## Add custom labels to all otelcol sts pods(logs and metrics)
  podLabels: {}

  ## Add custom annotations to all otelcol sts pods(logs and metrics)
  podAnnotations: {}

  ## Add custom labels to all otelcol svc (logs and metrics)
  serviceLabels: {}

  ## Configure persistence for Opentelemetry Collector
  persistence:
    enabled: true
    # storageClass: "-"
    accessMode: ReadWriteOnce
    size: 10Gi
    ## Add custom labels to all otelcol statefulset PVC (logs and metrics)
    pvcLabels: {}

  ## Configure metrics pipeline.
  ## This section affects only otelcol provider.
  metrics:
    enabled: true
    logLevel: info
    config:
      receivers:
        ## Configuration for Telegraf Receiver
        ## ref: https://github.com/SumoLogic/sumologic-otel-collector/tree/main/pkg/receiver/telegrafreceiver
        telegraf:
          agent_config: |
            [agent]
              interval = "30s"
              flush_interval = "30s"
              omit_hostname = true

            [[inputs.http_listener_v2]]
              # wait longer than prometheus
              read_timeout = "30s"
              write_timeout = "30s"
              service_address = ":9888"
              data_format = "prometheusremotewrite"
              path_tag = true
              paths = [
                "/prometheus.metrics",
                "/prometheus.metrics.apiserver",
                "/prometheus.metrics.applications.activemq",
                "/prometheus.metrics.applications.apache",
                "/prometheus.metrics.applications.cassandra",
                "/prometheus.metrics.applications.couchbase",
                "/prometheus.metrics.applications.elasticsearch",
                "/prometheus.metrics.applications.haproxy",
                "/prometheus.metrics.applications.jmx",
                "/prometheus.metrics.applications.kafka",
                "/prometheus.metrics.applications.memcached",
                "/prometheus.metrics.applications.mongodb",
                "/prometheus.metrics.applications.mysql",
                "/prometheus.metrics.applications.nginx",
                "/prometheus.metrics.applications.nginx-ingress",
                "/prometheus.metrics.applications.postgresql",
                "/prometheus.metrics.applications.rabbitmq",
                "/prometheus.metrics.applications.redis",
                "/prometheus.metrics.applications.sqlserver",
                "/prometheus.metrics.applications.squidproxy",
                "/prometheus.metrics.applications.tomcat",
                "/prometheus.metrics.applications.varnish",
                "/prometheus.metrics.container",
                "/prometheus.metrics.controller-manager",
                "/prometheus.metrics.control-plane.coredns",
                "/prometheus.metrics.control-plane.kube-etcd",
                "/prometheus.metrics.kubelet",
                "/prometheus.metrics.node",
                "/prometheus.metrics.operator.rule",
                "/prometheus.metrics.scheduler",
                "/prometheus.metrics.state"
              ]
      extensions:
        health_check: {}
        ## Configuration for File Storage extension
        ## ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/release/v0.37.x/extension/storage/filestorage
        file_storage:
          directory: /var/lib/storage/otc
          timeout: 10s
          compaction:
            on_start: true
            on_rebound: true
            # Can't be /tmp yet, see https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/13449
            directory: /var/lib/storage/otc
        pprof: {}
      exporters:
        ## Configuration for Sumo Logic Exporter
        ## ref: https://github.com/SumoLogic/sumologic-otel-collector/blob/main/pkg/exporter/sumologicexporter
        sumologic/default:
          metric_format: prometheus
          endpoint: ${SUMO_ENDPOINT_DEFAULT_METRICS_SOURCE}
          ## Configuration for sending queue
          ## ref: https://github.com/open-telemetry/opentelemetry-collector/tree/release/v0.37.x/exporter/exporterhelper#configuration
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            ## setting queue_size a high number, so we always use maximum space of the storage
            ## minimal alert non-triggering queue size (if only one exporter is being used): 10GB/16MB = 640
            queue_size: 10_000
          max_request_body_size: 16_777_216  # 16 MB before compression
          ## set timeout to 30s due to big requests
          timeout: 30s
        sumologic/apiserver:
          metric_format: prometheus
          endpoint: ${SUMO_ENDPOINT_APISERVER_METRICS_SOURCE}
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000
          max_request_body_size: 16_777_216  # 16 MB before compression
          ## set timeout to 30s due to big requests
          timeout: 30s
        sumologic/control_plane:
          metric_format: prometheus
          endpoint: ${SUMO_ENDPOINT_CONTROL_PLANE_METRICS_SOURCE}
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000
          max_request_body_size: 16_777_216  # 16 MB before compression
          ## set timeout to 30s due to big requests
          timeout: 30s
        sumologic/controller:
          metric_format: prometheus
          endpoint: ${SUMO_ENDPOINT_CONTROLLER_METRICS_SOURCE}
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000
          max_request_body_size: 16_777_216  # 16 MB before compression
          ## set timeout to 30s due to big requests
          timeout: 30s
        sumologic/kubelet:
          metric_format: prometheus
          endpoint: ${SUMO_ENDPOINT_KUBELET_METRICS_SOURCE}
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000
          max_request_body_size: 16_777_216  # 16 MB before compression
          ## set timeout to 30s due to big requests
          timeout: 30s
        sumologic/node:
          metric_format: prometheus
          endpoint: ${SUMO_ENDPOINT_NODE_METRICS_SOURCE}
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000
          max_request_body_size: 16_777_216  # 16 MB before compression
          ## set timeout to 30s due to big requests
          timeout: 30s
        sumologic/scheduler:
          metric_format: prometheus
          endpoint: ${SUMO_ENDPOINT_SCHEDULER_METRICS_SOURCE}
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000
          max_request_body_size: 16_777_216  # 16 MB before compression
          ## set timeout to 30s due to big requests
          timeout: 30s
        sumologic/state:
          metric_format: prometheus
          endpoint: ${SUMO_ENDPOINT_STATE_METRICS_SOURCE}
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000
          max_request_body_size: 16_777_216  # 16 MB before compression
          ## set timeout to 30s due to big requests
          timeout: 30s
      processors:
        ## Configuration for Metrics Transform Processor
        ## ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/release/v0.37.x/processor/metricstransformprocessor
        metricstransform:
          transforms:
            ## rename all prometheus_remote_write_$name metrics to $name
            include: ^prometheus_remote_write_(.*)$$
            match_type: regexp
            action: update
            new_name: $$1
        ## NOTE: Drop these for now and and when proper configuration options
        ## are exposed and source processor is configured then send them
        ## as headers.
        ## ref: https://github.com/SumoLogic/sumologic-otel-collector/issues/265
        resource/delete_source_metadata:
          attributes:
            - key: _sourceCategory
              action: delete
            - key: _sourceHost
              action: delete
            - key: _sourceName
              action: delete
        ## Configuration for Resource Processor
        ## ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/release/v0.37.x/processor/resourceprocessor
        resource:
          attributes:
            - key: k8s.namespace.name
              from_attribute: namespace
              action: upsert
            - key: namespace  # remove namespace to avoid duplication when attribute translation is enabled
              action: delete
            - key: k8s.pod.name
              from_attribute: pod
              action: upsert
            - key: pod  # remove pod to avoid duplication when attribute translation is enabled
              action: delete
            - key: k8s.container.name  # add container in OpenTelemetry convention to unify configuration for Source processor
              from_attribute: container
              action: upsert
            - key: container  # remove container to avoid duplication when attribute translation is enabled
              action: delete
            - key: prometheus_service
              from_attribute: service
              action: upsert
            - key: service
              action: delete
            - key: _origin  # add "_origin" metadata to metrics to keep the same format as for metrics from Fluentd
              value: kubernetes
              action: upsert
            - key: cluster
              value: '{{ .Values.sumologic.clusterName | quote }}'
              action: upsert
        resource/remove_k8s_pod_pod_name:
          attributes:
            - action: delete
              key: k8s.pod.pod_name
        ## NOTE: below listed rules could be simplified if routingprocessor
        ## supports regex matching. At this point we could group route entries
        ## going to the same set of exporters.
        routing:
          ## NOTE: add a feature to routingprocessor to drop the routing
          ## attribute to prevent it being sent to the exporters.
          from_attribute: http_listener_v2_path
          drop_resource_routing_attribute: true
          attribute_source: resource
          default_exporters:
            - sumologic/default
          table:
            ## apiserver metrics
            - value: /prometheus.metrics.apiserver
              exporters:
                - sumologic/apiserver
            ## container metrics
            - value: /prometheus.metrics.container
              exporters:
                - sumologic/kubelet
            ## control-plane metrics
            - value: /prometheus.metrics.control-plane.coredns
              exporters:
                - sumologic/control_plane
            - value: /prometheus.metrics.control-plane.kube-etcd
              exporters:
                - sumologic/control_plane
            ## controller metrics
            - value: /prometheus.metrics.controller-manager
              exporters:
                - sumologic/controller
            ## kubelet metrics
            - value: /prometheus.metrics.kubelet
              exporters:
                - sumologic/kubelet
            ## node metrics
            - value: /prometheus.metrics.node
              exporters:
                - sumologic/node
            ## scheduler metrics
            - value: /prometheus.metrics.scheduler
              exporters:
                - sumologic/scheduler
            ## state metrics
            - value: /prometheus.metrics.state
              exporters:
                - sumologic/state

        ## Configuration for Memory Limiter Processor
        ## The memory_limiter processor is used to prevent out of memory situations on the collector.
        ## ref: https://github.com/SumoLogic/opentelemetry-collector/tree/main/processor/memorylimiter
        memory_limiter:
          ## check_interval is the time between measurements of memory usage for the
          ## purposes of avoiding going over the limits. Defaults to zero, so no
          ## checks will be performed. Values below 1 second are not recommended since
          ## it can result in unnecessary CPU consumption.
          check_interval: 5s

          ## Maximum amount of memory, in %, targeted to be allocated by the process heap.
          limit_percentage: 75
          ## Spike limit (calculated from available memory). Must be less than limit_percentage.
          spike_limit_percentage: 20

        sumologic_schema:
          add_cloud_namespace: false

        ## Configuration for Batch Processor
        ## The batch processor accepts spans and places them into batches grouped by node and resource
        ## ref: https://github.com/SumoLogic/opentelemetry-collector/blob/main/processor/batchprocessor
        batch:
          ## Number of spans after which a batch will be sent regardless of time
          send_batch_size: 1_024
          ## Time duration after which a batch will be sent regardless of size
          timeout: 1s
        ## Configuration for Kubernetes Processor
        ## ref: https://github.com/SumoLogic/sumologic-otel-collector/tree/main/pkg/processor/k8sprocessor
        k8s_tagger:
          ## Has to be false to enrich metadata
          passthrough: false
          owner_lookup_enabled: true  # To enable fetching additional metadata using `owner` relationship
          extract:
            metadata:
              ## extract the following well-known metadata fields
              - daemonSetName
              - deploymentName
              - nodeName
              - replicaSetName
              - serviceName
              - statefulSetName
            labels:
              - tag_name: "pod_labels_%s"
                key: "*"
            delimiter: "_"
          pod_association:
            - from: build_hostname  # Pods are identified by Pod name and namespace
        ## Configuration for Source Processor
        ## Source processor adds Sumo Logic related metadata
        ## ref: https://github.com/SumoLogic/sumologic-otel-collector/tree/main/pkg/processor/sourceprocessor
        source:
          collector: '{{ .Values.sumologic.collectorName | default .Values.sumologic.clusterName | quote }}'
      service:
        telemetry:
          logs:
            level: '{{ .Values.metadata.metrics.logLevel }}'
        extensions:
          - health_check
          - file_storage
          - pprof
        pipelines:
          metrics:
            receivers:
              - telegraf
            processors:
              - memory_limiter
              - metricstransform
              - resource
              - k8s_tagger
              - source
              - resource/remove_k8s_pod_pod_name
              - resource/delete_source_metadata
              - sumologic_schema
              - batch
              - routing
            exporters:
              - sumologic/default
              - sumologic/apiserver
              - sumologic/control_plane
              - sumologic/controller
              - sumologic/kubelet
              - sumologic/node
              - sumologic/scheduler
              - sumologic/state

    statefulset:
      nodeSelector: {}
      tolerations: []
      topologySpreadConstraints: []
      affinity: {}
      ## Acceptable values for podAntiAffinity:
      ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
      ## hard: specifies rules that must be met for a pod to be scheduled onto a node
      podAntiAffinity: "soft"
      replicaCount: 3
      resources:
        limits:
          memory: 1Gi
          cpu: 1000m
        requests:
          memory: 768Mi
          cpu: 500m
      ## Option to define priorityClassName to assign a priority class to pods.
      priorityClassName:

      ## Add custom labels only to metrics sts pods
      podLabels: {}
      ## Add custom annotations only to metrics sts pods
      podAnnotations: {}

      ## Set securityContext for containers running in pods in metrics statefulset.
      containers:
        otelcol:
          securityContext: {}
          livenessProbe:
            initialDelaySeconds: 15
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 3
          startupProbe:
            periodSeconds: 3
            failureThreshold: 60

      ## Extra Environment Values - allows yaml definitions
      # extraEnvVars:
      #   - name: VALUE_FROM_SECRET
      #     valueFrom:
      #       secretKeyRef:
      #         name: secret_name
      #         key: secret_key

      # extraVolumes:
      #   - name: es-certs
      #     secret:
      #       defaultMode: 420
      #       secretName: es-certs
      # extraVolumeMounts:
      #   - name: es-certs
      #     mountPath: /certs
      #     readOnly: true

    ## Option to turn autoscaling on for metrics and specify params for HPA.
    ## Autoscaling needs metrics-server to access cpu metrics.
    autoscaling:
      enabled: false
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilizationPercentage: 100
      # targetMemoryUtilizationPercentage: 50

    ## Option to specify PodDisrutionBudgets
    ## You can specify only one of maxUnavailable and minAvailable in a single PodDisruptionBudget
    podDisruptionBudget:
      minAvailable: 2
    ## To use maxUnavailable, set minAvailable to null and uncomment the below:
    #   maxUnavailable: 1

  ## Configure logs pipeline.
  ## This section affects only otelcol provider.
  logs:
    enabled: true
    logLevel: info
    config:
      receivers:
        fluentforward:
          endpoint: 0.0.0.0:24321
        otlp:
          protocols:
            http:
              endpoint: 0.0.0.0:4318
      extensions:
        health_check: {}
        ## Configuration for File Storage extension
        ## ref: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/release/v0.37.x/extension/storage/filestorage
        file_storage:
          directory: /var/lib/storage/otc
          timeout: 10s
          compaction:
            on_start: true
            on_rebound: true
            # Can't be /tmp yet, see https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/13449
            directory: /var/lib/storage/otc
        pprof: {}
      exporters:
        sumologic/containers:
          log_format: json
          json_logs:
            add_timestamp: true
            timestamp_key: timestamp
          endpoint: ${SUMO_ENDPOINT_DEFAULT_LOGS_SOURCE}
          source_name: "%{_sourceName}"
          source_category: "%{_sourceCategory}"
          source_host: "%{_sourceHost}"
          ## Configuration for sending queue
          ## ref: https://github.com/open-telemetry/opentelemetry-collector/tree/release/v0.37.x/exporter/exporterhelper#configuration
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000
        sumologic/systemd:
          log_format: json
          json_logs:
            add_timestamp: true
            timestamp_key: timestamp
            ## use flatten_body, but OTLP won't require any flattening
            ## fluent based logs will be all send as record attributes
            ## otellogs based logs will be all send as body attributes
            flatten_body: true
          endpoint: ${SUMO_ENDPOINT_DEFAULT_LOGS_SOURCE}
          source_name: "%{_sourceName}"
          source_category: "%{_sourceCategory}"
          source_host: "%{_sourceHost}"
          ## Configuration for sending queue
          ## ref: https://github.com/open-telemetry/opentelemetry-collector/tree/release/v0.37.x/exporter/exporterhelper#configuration
          sending_queue:
            enabled: true
            persistent_storage_enabled: '{{ .Values.metadata.persistence.enabled }}'
            num_consumers: 10
            queue_size: 10_000

      processors:
        ## Common processors
        attributes/remove_fluent_tag:
          actions:
            - action: delete
              key: fluent.tag
        ## The memory_limiter processor is used to prevent out of memory situations on the collector.
        memory_limiter:
          ## check_interval is the time between measurements of memory usage for the
          ## purposes of avoiding going over the limits. Defaults to zero, so no
          ## checks will be performed. Values below 1 second are not recommended since
          ## it can result in unnecessary CPU consumption.
          check_interval: 5s

          ## Maximum amount of memory, in %, targeted to be allocated by the process heap.
          limit_percentage: 75
          ## Spike limit (calculated from available memory). Must be less than limit_percentage.
          spike_limit_percentage: 20

        ## The batch processor accepts spans and places them into batches grouped by node and resource
        batch:
          ## Number of spans after which a batch will be sent regardless of time
          send_batch_size: 1_024
          ## Time duration after which a batch will be sent regardless of size
          timeout: 1s

        ## Containers related processors
        filter/include_fluent_tag_containers:
          logs:
            include:
              match_type: regexp
              record_attributes:
                - key: fluent.tag
                  value: containers\..+
        filter/include_containers:
          logs:
            include:
              match_type: regexp
              record_attributes:
                - key: k8s.container.name
                  value: .+
        attributes/containers:
          actions:
            - action: extract
              key: fluent.tag
              pattern: ^containers\.var\.log\.containers\.(?P<k8s_pod_name>[^_]+)_(?P<k8s_namespace>[^_]+)_(?P<k8s_container_name>.+)-(?P<container_id>[a-f0-9]{64})\.log$
            - action: insert
              key: k8s.container.id
              from_attribute: container_id
            - action: delete
              key: container_id
            - action: insert
              key: k8s.pod.name
              from_attribute: k8s_pod_name
            - action: delete
              key: k8s_pod_name
            - action: insert
              key: k8s.namespace.name
              from_attribute: k8s_namespace
            - action: delete
              key: k8s_namespace
            - action: insert
              key: k8s.container.name
              from_attribute: k8s_container_name
            - action: delete
              key: k8s_container_name
        resource/containers_copy_node_to_host:
          attributes:
            - action: upsert
              key: k8s.pod.hostname
              from_attribute: k8s.node.name
        resource/drop_annotations:
          attributes:
            - pattern: ^pod_annotations_.*
              action: delete
        resource/add_cluster:
          attributes:
            - key: cluster
              value: '{{ .Values.sumologic.clusterName | quote }}'
              action: upsert
        groupbyattrs/containers:
          keys:
            - k8s.container.id
            - k8s.container.name
            - k8s.namespace.name
            - k8s.pod.name
            - _collector
        k8s_tagger:
          ## Has to be false to enrich metadata
          passthrough: false
          owner_lookup_enabled: true  # To enable fetching additional metadata using `owner` relationship
          extract:
            metadata:
              ## extract the following well-known metadata fields
              - containerId
              - containerName
              - daemonSetName
              - deploymentName
              - hostName
              - namespace
              - nodeName
              - podId
              - podName
              - replicaSetName
              - serviceName
              - statefulSetName
            annotations:
              - tag_name: "pod_annotations_%s"
                key: "*"
            namespace_labels:
              - tag_name: "namespace_labels_%s"
                key: "*"
            labels:
              - tag_name: "pod_labels_%s"
                key: "*"
            delimiter: "_"
          pod_association:
            - from: build_hostname
        source/containers:
          collector: '{{ .Values.sumologic.collectorName | default .Values.sumologic.clusterName | quote }}'
          source_host: "%{k8s.pod.hostname}"
          source_name: "%{k8s.namespace.name}.%{k8s.pod.name}.%{k8s.container.name}"
          source_category: "%{k8s.namespace.name}/%{k8s.pod.pod_name}"
          source_category_prefix: '{{ .Values.fluentd.logs.containers.sourceCategoryPrefix | quote }}'
          source_category_replace_dash: '{{ .Values.fluentd.logs.containers.sourceCategoryReplaceDash | quote }}'
          exclude:
            k8s.namespace.name: '{{ include "fluentd.excludeNamespaces" . }}'
            k8s.pod.name: '{{ .Values.fluentd.logs.containers.excludePodRegex | quote }}'
            k8s.container.name: '{{ .Values.fluentd.logs.containers.excludeContainerRegex | quote }}'
            k8s.pod.hostname: '{{ .Values.fluentd.logs.containers.excludeHostRegex | quote }}'
          annotation_prefix: "pod_annotations_"
          pod_template_hash_key: "pod_labels_pod-template-hash"
          pod_name_key: "k8s.pod.pod_name"
          pod_key: "k8s.pod.name"
          container_annotations:
            enabled: '{{ .Values.fluentd.logs.containers.perContainerAnnotationsEnabled }}'
            prefixes: '{{ toJson .Values.fluentd.logs.containers.perContainerAnnotationPrefixes }}'

        ## Systemd related processors
        filter/include_fluent_tag_host:
          logs:
            include:
              match_type: regexp
              record_attributes:
                - key: fluent.tag
                  value: host\..+
        attributes/extract_systemd_source_fields:
          actions:
            - action: extract
              key: fluent.tag
              pattern: ^host\.(?P<_sourceName>[a-zA-z0-9]+)\..+$
            - action: insert
              from_attribute: _HOSTNAME
              key: _sourceHost
        filter/include_systemd:
          logs:
            include:
              match_type: regexp
              record_attributes:
                - key: _SYSTEMD_UNIT
                  value: .+
        filter/exclude_kubelet:
          logs:
            exclude:
              match_type: strict
              record_attributes:
                - key: _SYSTEMD_UNIT
                  value: kubelet.service
        filter/exclude_systemd_syslog:
          logs:
            exclude:
              match_type: regexp
              record_attributes:
                - key: SYSLOG_FACILITY
                  value: '{{ .Values.fluentd.logs.systemd.excludeFacilityRegex | default "$^" | quote }}'
        filter/exclude_systemd_hostname:
          logs:
            exclude:
              match_type: regexp
              record_attributes:
                - key: _HOSTNAME
                  value: '{{ .Values.fluentd.logs.systemd.excludeHostRegex | default "$^" | quote }}'
        filter/exclude_systemd_priority:
          logs:
            exclude:
              match_type: regexp
              record_attributes:
                - key: PRIORITY
                  value: '{{ .Values.fluentd.logs.systemd.excludePriorityRegex | default "$^" | quote }}'
        filter/exclude_systemd_unit:
          logs:
            exclude:
              match_type: regexp
              record_attributes:
                - key: _SYSTEMD_UNIT
                  value: '{{ .Values.fluentd.logs.systemd.excludeUnitRegex | default "$^" | quote }}'
        filter/exclude_kubelet_syslog:
          logs:
            exclude:
              match_type: regexp
              record_attributes:
                - key: SYSLOG_FACILITY
                  value: '{{ .Values.fluentd.logs.kubelet.excludeFacilityRegex | default "$^" | quote }}'
        filter/exclude_kubelet_hostname:
          logs:
            exclude:
              match_type: regexp
              record_attributes:
                - key: _HOSTNAME
                  value: '{{ .Values.fluentd.logs.kubelet.excludeHostRegex | default "$^" | quote }}'
        filter/exclude_kubelet_priority:
          logs:
            exclude:
              match_type: regexp
              record_attributes:
                - key: PRIORITY
                  value: '{{ .Values.fluentd.logs.kubelet.excludePriorityRegex | default "$^" | quote }}'
        filter/exclude_kubelet_unit:
          logs:
            exclude:
              match_type: regexp
              record_attributes:
                - key: _SYSTEMD_UNIT
                  value: '{{ .Values.fluentd.logs.kubelet.excludeUnitRegex | default "$^" | quote }}'

        groupbyattrs/systemd:
          keys:
            - _sourceName
            - _sourceHost
            - _collector
        source/systemd:
          collector: '{{ .Values.sumologic.collectorName | default .Values.sumologic.clusterName | quote }}'
          source_host: "%{_sourceHost}"
          source_name: "%{_sourceName}"
          source_category: '{{ .Values.fluentd.logs.systemd.sourceCategory | quote }}'
          source_category_prefix: '{{ .Values.fluentd.logs.systemd.sourceCategoryPrefix | quote }}'
          source_category_replace_dash: '{{ .Values.fluentd.logs.systemd.sourceCategoryReplaceDash | quote }}'
        ## Remove all attributes, so body won't by nested by SumoLogic receiver in case of using otlp format
        transform/remove_attributes:
          logs:
            queries:
              - limit(attributes, 0)

        ## kubelet related processors
        filter/include_kubelet:
          logs:
            include:
              match_type: strict
              record_attributes:
                - key: _SYSTEMD_UNIT
                  value: kubelet.service
        source/kubelet:
          collector: '{{ .Values.sumologic.collectorName | default .Values.sumologic.clusterName | quote }}'
          source_host: "%{_sourceHost}"
          source_name: '{{ .Values.fluentd.logs.kubelet.sourceName | quote }}'
          source_category: '{{ .Values.fluentd.logs.kubelet.sourceCategory | quote }}'
          source_category_prefix: '{{ .Values.fluentd.logs.kubelet.sourceCategoryPrefix | quote }}'
          source_category_replace_dash: '{{ .Values.fluentd.logs.kubelet.sourceCategoryReplaceDash | quote }}'

        sumologic_schema:
          add_cloud_namespace: false

      service:
        telemetry:
          logs:
            level: '{{ .Values.metadata.logs.logLevel }}'
        extensions:
          - health_check
          - file_storage
          - pprof
        pipelines:
          logs/fluent/containers:
            receivers:
              - fluentforward
            processors:
              - memory_limiter
              - filter/include_fluent_tag_containers
              - attributes/containers
              - groupbyattrs/containers
              - k8s_tagger
              - resource/add_cluster
              - source/containers
              - resource/drop_annotations
              - attributes/remove_fluent_tag
              - resource/containers_copy_node_to_host
              - sumologic_schema
              - batch
            exporters:
              - sumologic/containers
          ## Uncomment this only if you're enabling the Otelcol Log Collector via otellogs.enabled
          ## This is commented due to k8s_tagger memory footprint
          ## This is the same pipeline like for logs/fluent/containers with the following modifications:
          ## - filter/include_fluent_tag_containers and attributes/remove_fluent_tag are being removed
          ##   as only containers log are being provided to otlp receiver
          ## - attributes/containers functionality is being replaced by otellogs operators
          # logs/otlp/containers:
          #   receivers:
          #     - otlp
          #   processors:
          #     - memory_limiter
          #     - filter/include_containers
          #     - groupbyattrs/containers
          #     - k8s_tagger
          #     - resource/add_cluster
          #     - source/containers
          #     - resource/drop_annotations
          #     - resource/containers_copy_node_to_host
          #     - sumologic_schema
          #     - batch
          #   exporters:
          #     - sumologic/containers
          logs/fluent/systemd:
            receivers:
              - fluentforward
            processors:
              - memory_limiter
              - filter/include_fluent_tag_host
              - filter/include_systemd
              - filter/exclude_kubelet
              - filter/exclude_systemd_syslog
              - filter/exclude_systemd_hostname
              - filter/exclude_systemd_priority
              - filter/exclude_systemd_unit
              - attributes/extract_systemd_source_fields
              - attributes/remove_fluent_tag
              - groupbyattrs/systemd
              - resource/add_cluster
              - source/systemd
              - batch
            exporters:
              - sumologic/systemd
          logs/fluent/kubelet:
            receivers:
              - fluentforward
            processors:
              - memory_limiter
              - filter/include_fluent_tag_host
              - filter/include_kubelet
              - filter/exclude_kubelet_syslog
              - filter/exclude_kubelet_hostname
              - filter/exclude_kubelet_priority
              - filter/exclude_kubelet_unit
              - attributes/extract_systemd_source_fields
              - attributes/remove_fluent_tag
              - groupbyattrs/systemd
              - resource/add_cluster
              - source/kubelet
              - batch
            exporters:
              - sumologic/systemd
          ## This is the same pipeline like logs/fluent/systemd, but with the following changes:
          ## - otlp receiver instead of fluentforward
          ## - added transform/remove_attributes processor
          logs/otlp/systemd:
            receivers:
              - otlp
            processors:
              - memory_limiter
              - filter/include_fluent_tag_host
              - filter/include_systemd
              - filter/exclude_kubelet
              - filter/exclude_systemd_syslog
              - filter/exclude_systemd_hostname
              - filter/exclude_systemd_priority
              - filter/exclude_systemd_unit
              - attributes/extract_systemd_source_fields
              - attributes/remove_fluent_tag
              - groupbyattrs/systemd
              - resource/add_cluster
              - source/systemd
              - transform/remove_attributes
              - batch
            exporters:
              - sumologic/systemd
          ## This is the same pipeline like logs/fluent/kubelet, but with the following changes:
          ## - otlp receiver instead of fluentforward
          ## - added transform/remove_attributes processor
          logs/otlp/kubelet:
            receivers:
              - otlp
            processors:
              - memory_limiter
              - filter/include_fluent_tag_host
              - filter/include_kubelet
              - filter/exclude_kubelet_syslog
              - filter/exclude_kubelet_hostname
              - filter/exclude_kubelet_priority
              - filter/exclude_kubelet_unit
              - attributes/extract_systemd_source_fields
              - attributes/remove_fluent_tag
              - groupbyattrs/systemd
              - resource/add_cluster
              - source/kubelet
              - transform/remove_attributes
              - batch
            exporters:
              - sumologic/systemd
    statefulset:
      nodeSelector: {}
      tolerations: []
      topologySpreadConstraints: []
      affinity: {}
      ## Acceptable values for podAntiAffinity:
      ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
      ## hard: specifies rules that must be met for a pod to be scheduled onto a node
      podAntiAffinity: "soft"
      replicaCount: 3
      resources:
        limits:
          memory: 1Gi
          cpu: 1000m
        requests:
          memory: 768Mi
          cpu: 500m
      ## Option to define priorityClassName to assign a priority class to pods.
      priorityClassName:

      ## Add custom labels only to logs sts pods
      podLabels: {}
      ## Add custom annotations only to logs sts pods
      podAnnotations: {}

      ## Set securityContext for containers running in pods in logs statefulset.
      containers:
        otelcol:
          securityContext: {}
          livenessProbe:
            initialDelaySeconds: 15
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 3
          startupProbe:
            periodSeconds: 3
            failureThreshold: 60

      ## Extra Environment Values - allows yaml definitions
      # extraEnvVars:
      #   - name: VALUE_FROM_SECRET
      #     valueFrom:
      #       secretKeyRef:
      #         name: secret_name
      #         key: secret_key

      # extraVolumes:
      #   - name: es-certs
      #     secret:
      #       defaultMode: 420
      #       secretName: es-certs
      # extraVolumeMounts:
      #   - name: es-certs
      #     mountPath: /certs
      #     readOnly: true

    ## Option to turn autoscaling on for logs and specify params for HPA.
    ## Autoscaling needs metrics-server to access cpu metrics.
    autoscaling:
      enabled: false
      minReplicas: 3
      maxReplicas: 10
      targetCPUUtilizationPercentage: 100
      # targetMemoryUtilizationPercentage: 50

    ## Option to specify PodDisrutionBudgets
    ## You can specify only one of maxUnavailable and minAvailable in a single PodDisruptionBudget
    podDisruptionBudget:
      minAvailable: 2
      ## To use maxUnavailable, set minAvailable to null and uncomment the below:
      # maxUnavailable: 1

## Configure optional OpenTelemetry Gateway in Agent mode
## ref: docs/opentelemetry_collector.md#load-balancing-using-the-gateway
otelgateway:
  enabled: true
  deployment:
    replicas: 1
    nodeSelector: {}
    tolerations: []
    resources:
      limits:
        memory: 2Gi
        cpu: 1000m
      requests:
        memory: 196Mi
        cpu: 50m
    ## Add custom labels only to otelagent daemonset.
    podLabels: {}
    ## Add custom annotations only to otelagent daemonset.
    podAnnotations: {}
    image:
      repository: "public.ecr.aws/sumologic/sumologic-otel-collector"
      tag: "0.54.0-sumo-0"
      pullPolicy: IfNotPresent
    livenessProbe:
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 3
    readinessProbe:
      periodSeconds: 10
      timeoutSeconds: 3
      failureThreshold: 3
    startupProbe:
      periodSeconds: 5
      timeoutSeconds: 3
      failureThreshold: 60

    ## Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #   - name: VALUE_FROM_SECRET
    #     valueFrom:
    #       secretKeyRef:
    #         name: secret_name
    #         key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

  config:
    receivers:
      jaeger:
        protocols:
          thrift_compact:
            endpoint: "0.0.0.0:6831"
          thrift_binary:
            endpoint: "0.0.0.0:6832"
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_http:
            endpoint: "0.0.0.0:14268"
      opencensus:
        endpoint: "0.0.0.0:55678"
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
      otlp/deprecated:
        protocols:
          http:
            endpoint: "0.0.0.0:55681"
      zipkin:
        endpoint: "0.0.0.0:9411"
    processors:
      ## The memory_limiter processor is used to prevent out of memory situations on the collector.
      memory_limiter:
        ## check_interval is the time between measurements of memory usage for the
        ## purposes of avoiding going over the limits. Defaults to zero, so no
        ## checks will be performed. Values below 1 second are not recommended since
        ## it can result in unnecessary CPU consumption.
        check_interval: 5s

        ## Maximum amount of memory, in %, targeted to be allocated by the process heap.
        limit_percentage: 75
        ## Spike limit (calculated from available memory). Must be less than limit_percentage.
        spike_limit_percentage: 20

      ## The batch processor accepts spans and places them into batches grouped by node and resource
      batch:
        ## Number of spans after which a batch will be sent regardless of time
        send_batch_size: 256
        ## Time duration after which a batch will be sent regardless of size
        timeout: 5s
    extensions:
      health_check: {}
      memory_ballast:
        ## Memory Ballast size should be max 1/3 to 1/2 of memory.
        size_mib: 250
      pprof: {}
    exporters:
      loadbalancing:
        protocol:
          otlp:
            timeout: 10s
            tls:
              insecure: true
        resolver:
          dns:
            hostname: '{{ include "traces.otelgateway.exporter.loadbalancing.endpoint" . }}'
            port: 4317
      otlp:
        endpoint: '{{ (printf "%s:4317" (include "traces.otelgateway.exporter.endpoint" .)) }}'
        tls:
          ## This disables TLS when communicating with the gateway (within the same cluster)
          insecure: true
    service:
      extensions: [health_check, memory_ballast, pprof]
      pipelines:
        traces:
          receivers: [jaeger, opencensus, otlp, otlp/deprecated, zipkin]
          processors: [memory_limiter, batch]
          exporters: [loadbalancing]
        metrics:
          receivers: [otlp, otlp/deprecated]
          processors: [memory_limiter, batch]
          exporters: [otlp]

## Configuration of the OpenTelemetry Collector that collects Kubernetes events.
## It is used when the `sumologic.events.provider` is set to `otelcol`.
## See https://github.com/SumoLogic/sumologic-kubernetes-collection/deploy/docs/collecting-kubernetes-events.md.
otelevents:

  ## Configure persistence for Opentelemetry Collector
  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: 10Gi
    ## Add custom labels to otelcol event statefulset PVC
    pvcLabels: {}

  ## Configure image for Opentelemetry Collector
  image:
    repository: public.ecr.aws/sumologic/sumologic-otel-collector
    tag: 0.56.0-sumo-0
    pullPolicy: IfNotPresent

  logLevel: info

  ## Customize the Opentelemetry Collector configuration beyond the exposed options
  config:
    ## Directly override the OT configuration. The value of this key should be a dictionary, that will
    ## be directly merged with the generated configuration, overriding existing values.
    ## For example:
    # override:
    #   processors:
    #     batch:
    #       send_batch_size: 512
    ## will change the batch size of the pipeline.
    ##
    ## WARNING: This field is not subject to backwards-compatibility guarantees offered by the rest
    ## of this chart. It involves implementation details that may change even in minor versions.
    ## Use with caution, and consider opening an issue, so your customization can be added in a safer way.
    override: {}

  statefulset:
    nodeSelector: {}
    tolerations: []
    topologySpreadConstraints: []
    affinity: {}
    ## Acceptable values for podAntiAffinity:
    ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
    ## hard: specifies rules that must be met for a pod to be scheduled onto a node
    podAntiAffinity: "soft"
    resources:
      limits:
        memory: 2Gi
        cpu: 2000m
      requests:
        memory: 500Mi
        cpu: 200m
    ## Option to define priorityClassName to assign a priority class to pods.
    priorityClassName:

    ## Add custom labels only to events sts pods
    podLabels: {}
    ## Add custom annotations only to events sts pods
    podAnnotations: {}

    securityContext:
      ## ToDo: Verify following comment
      ## The group ID of all processes in the statefulset containers. By default this needs to be otelcol(999).
      fsGroup: 999

    ## Set securityContext for containers running in pods in events statefulset.
    containers:
      otelcol:
        securityContext: {}
        livenessProbe:
          initialDelaySeconds: 15
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        startupProbe:
          periodSeconds: 3
          failureThreshold: 60

    ## Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #   - name: VALUE_FROM_SECRET
    #     valueFrom:
    #       secretKeyRef:
    #         name: secret_name
    #         key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

## Configure log collection with Otelcol
## This is currently highly experimental and not recommended unless you really know what you're doing.
## In particular, this configuration currently only covers container logs, and only supports docker-shim.
## Unlike the fluent-bit configuration, it also reads logs from /var/log/pods instead of /var/log/containers
## which is consistent with CRI, but may possibly cause issues on older K8s versions.
## This is an alpha feature, and may change in the near future.
otellogs:

  ## Metrics from Collector
  metrics:
    enabled: true

  ## Add custom labels to otelcol svc
  serviceLabels: {}

  ## Configure image for Opentelemetry Collector
  image:
    repository: "public.ecr.aws/sumologic/sumologic-otel-collector"
    tag: "0.54.0-sumo-0"
    pullPolicy: IfNotPresent

  logLevel: info

  config:
    override: {}

  daemonset:
    ## Set securityContext for containers running in pods in log collector daemonset
    securityContext:
    ## In order to reliably read logs from mounted node logging paths, we need to run as root
      fsGroup: 0
      runAsUser: 0
      runAsGroup: 0

    ## Add custom labels to all otelcol daemonset pods
    podLabels: {}

    ## Add custom annotations to all otelcol daemonset pods
    podAnnotations: {}

    resources:
      limits:
        memory: 1Gi
        cpu: 1000m
      requests:
        memory: 32Mi
        cpu: 100m
    ## Option to define priorityClassName to assign a priority class to pods.
    ## If not set then temaplates/priorityclass.yaml is used.
    priorityClassName:

    ## Set securityContext for containers running in pods in log collector daemonset
    containers:
      otelcol:
        securityContext: {}

    nodeSelector: {}
    tolerations: []
    affinity: {}

    ## Extra Environment Values - allows yaml definitions
    # extraEnvVars:
    #   - name: VALUE_FROM_SECRET
    #     valueFrom:
    #       secretKeyRef:
    #         name: secret_name
    #         key: secret_key

    # extraVolumes:
    #   - name: es-certs
    #     secret:
    #       defaultMode: 420
    #       secretName: es-certs
    # extraVolumeMounts:
    #   - name: es-certs
    #     mountPath: /certs
    #     readOnly: true

## Configure telegraf-operator
## ref: https://github.com/influxdata/helm-charts/blob/master/charts/telegraf-operator/values.yaml
telegraf-operator:
  enabled: false
  image:
    sidecarImage: public.ecr.aws/sumologic/telegraf:1.21.2
  replicaCount: 1
  classes:
    secretName: "telegraf-operator-classes"
    default: "sumologic-prometheus"
    data:
      sumologic-prometheus: |
        [[outputs.prometheus_client]]
          ## Configuration details:
          ## https://github.com/influxdata/telegraf/tree/master/plugins/outputs/prometheus_client#configuration
          listen = ":9273"
          metric_version = 2
  # imagePullSecrets: []

## Configure falco
## This is an experimental configuration and shouldn't be used in production environment
## https://github.com/falcosecurity/charts/tree/master/falco
falco:
  enabled: false
  image:
    registry: public.ecr.aws
    repository: sumologic/falco
    # pullSecrets: []

  ## Add kernel-devel package through MachineConfig, required to enable building of missing falco modules (only for OpenShift)
  addKernelDevel: true
  ## Add initContainers to Falco pod
  extraInitContainers:
    ## Add initContainer to wait until kernel-devel is installed on host
    - name: init-falco
      image: public.ecr.aws/docker/library/busybox
      command:
        - 'sh'
        - '-c'
        - |
          while [ -f /host/etc/redhat-release ] && [ -z "$(ls /host/usr/src/kernels)" ] ; do
          echo "waiting for kernel headers to be installed"
          sleep 3
          done
      volumeMounts:
        - mountPath: /host/usr
          name: usr-fs
          readOnly: true
        - mountPath: /host/etc
          name: etc-fs
          readOnly: true
  ## Enable eBPF support for Falco instead of falco-probe kernel module.
  ## Set to true for GKE, for details see:
  ## https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/main/deploy/docs/Troubleshoot_Collection.md#falco-and-google-kubernetes-engine-gke
  # ebpf:
  #   enabled: true
  falco:
    jsonOutput: true
    ## The location of the rules file(s). This can contain one or more paths to
    ## separate rules files.
    ## Explicitly add missing /etc/falco/rules.available/application_rules.yaml
    ## before https://github.com/falcosecurity/charts/issues/230 gets resolved.
    rulesFile:
      - /etc/falco/falco_rules.yaml
      - /etc/falco/falco_rules.local.yaml
      - /etc/falco/k8s_audit_rules.yaml
      - /etc/falco/rules.d
      - /etc/falco/rules.available/application_rules.yaml

  customRules:
    ## Mark the following as known k8s api callers:
    ## * fluentd and its plugins from sumologic/kubernetes-fluentd image
    ## * prometheus
    ## * prometheus operator
    ## * telegraf operator
    ## * grafana sidecar
    rules_user_known_k8s_api_callers.yaml: |-
      - macro: user_known_contact_k8s_api_server_activities
        condition: >
          (container.image.repository = "sumologic/kubernetes-fluentd") or
          (container.image.repository = "quay.io/prometheus/prometheus") or
          (container.image.repository = "quay.io/coreos/prometheus-operator") or
          (container.image.repository = "quay.io/influxdb/telegraf-operator") or
          (container.image.repository = "kiwigrid/k8s-sidecar")
    rules_user_sensitive_mount_containers.yaml: |-
      - macro: user_sensitive_mount_containers
        condition: >
          (container.image.repository = "falcosecurity/falco") or
          (container.image.repository = "quay.io/prometheus/node-exporter")
    ## NOTE: kube-proxy not exact matching because of regional ecr e.g.
    ## 602401143452.dkr.ecr.us-west-1.amazonaws.com/eks/kube-proxy
    rules_user_privileged_containers.yaml: |-
      - macro: user_privileged_containers
        condition: >
          (container.image.repository endswith ".amazonaws.com/eks/kube-proxy")

## Configure Tailing Sidecar Operator
## ref: https://github.com/SumoLogic/tailing-sidecar/blob/main/helm/tailing-sidecar-operator/values.yaml
tailing-sidecar-operator:
  enabled: false

  ## creation of Security Context Constraints in Openshift
  scc:
    create: false

## Configure OpenTelemetry Operator - Instrumentation
## ref: https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-operator
opentelemetry-operator:
  enabled: false

  ## Specific for Sumo Logic chart - Instrumentation resource creation
  instrumentationJobImage:
    image:
      repository: sumologic/kubernetes-tools
      tag: 2.13.0

  createDefaultInstrumentation: false
  instrumentationNamespaces: ""

  ## Specific for OpenTelemetry Operator chart values
  admissionWebhooks:
    failurePolicy: Fail
    enabled: true

    certManager:
      enabled: false
      issuerRef: {}

  manager:
    env:
