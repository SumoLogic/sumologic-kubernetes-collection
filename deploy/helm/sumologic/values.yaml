image:
  repository: sumologic/kubernetes-fluentd
  tag: 0.17.0
  pullPolicy: IfNotPresent

nameOverride: ""

sumologic:
  ## Setup

  # If enabled, a pre-install hook will create Collector and Sources in Sumo Logic
  setupEnabled: true

  # If enabled, accessId and accessKey will be sourced from Secret Name given
  # Be sure to include at least the following env variables in your secret
  # (1) SUMOLOGIC_ACCESSID, (2) SUMOLOGIC_ACCESSKEY
  #envFromSecret: sumo-api-secret

  # Sumo access ID
  #accessId: ""

  # Sumo access key
  #accessKey: ""

  # Sumo API endpoint; Leave blank for automatic endpoint discovery and redirection
  # ref: https://help.sumologic.com/APIs/General-API-Information/Sumo-Logic-Endpoints-and-Firewall-Security
  endpoint: ""

  # Collector name
  #collectorName: ""

  # Cluster name
  clusterName: "kubernetes"

  setup:
    clusterRole:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "1"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    clusterRoleBinding:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "2"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    configMap:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "2"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    job:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "3"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    serviceAccount:
      annotations:
        helm.sh/hook: pre-install,pre-upgrade
        helm.sh/hook-weight: "0"
        helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded

  # Traces configuration
  # This is experimental feature and may be unavailable for your account
  traces:
    enabled: false
    # Use filter stdout plugin for tracing data
    # Warning: It will produce additional fluentd logs (one per trace)
    fluentd_stdout: false
    # Sumologic endpoint for traces
    endpoint: ''
    # How many spans per request should be send to receiver
    spans_per_request: 100

fluentd:
  statefulset:
    nodeSelector: {}
    tolerations: {}
    affinity: {}
    ## Acceptable values for podAntiAffinity:
    ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
    ## hard: specifies rules that must be met for a pod to be scheduled onto a node
    podAntiAffinity: "soft"
    replicaCount: 3
    resources:
      limits:
        memory: 1Gi
        cpu: 1
      requests:
        memory: 768Mi
        cpu: 0.5

  eventsStatefulset:
    nodeSelector: {}
    tolerations: {}
    resources:
      limits:
        memory: 256Mi
        cpu: "100m"
      requests:
        memory: 256Mi
        cpu: "100m"

  rawConfig: |-
    @include common.conf
    @include metrics.conf
    @include logs.conf

  ## Sets the fluentd log level. The default log level, if not specified, is info.
  ## Sumo will only ingest the error log level and some specific warnings, the info logs can be seen in kubectl logs.
  ## ref: https://docs.fluentd.org/deployment/logging
  logLevel: "info"

  ## Verify SumoLogic HTTPS certificates
  verifySsl: true
  ## Proxy URI for sumologic output plugin
  proxyUri: ""

  ## Option to turn autoscaling on for fluentd and specify params for HPA.
  ## Autoscaling needs metrics-server to access cpu metrics.
  autoscaling:
    enabled: false
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 50

  securityContext:
    ## The group ID of all processes in the statefulset containers. By default this needs to be fluent(999).
    fsGroup: 999

  ## Persist data to a persistent volume; When enabled, fluentd uses the file buffer instead of memory buffer.
  persistence:
    enabled: false

    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, Azure & OpenStack)
    ##
    # storageClass: "-"
    # annotations: {}
    accessMode: ReadWriteOnce
    size: 10Gi

  buffer:
    ## Option to specify the Fluentd buffer as file/memory.
    ## If fluentd.persistence.enabled is true, this will be ignored.
    type: "memory"

    ## How frequently to push logs to SumoLogic
    ## ref: https://github.com/SumoLogic/fluentd-kubernetes-sumologic#options
    flushInterval: "5s"
    ## Increase number of http threads to Sumo. May be required in heavy logging/high DPM clusters
    numThreads: 8
    chunkLimitSize: "1m"
    queueChunkLimitSize: 128
    totalLimitSize: "128m"

    ## File paths to buffer to, if Fluentd buffer type is specified as file above.
    ## Each sumologic output plugin buffers to its own unique file.
    filePaths:
      logs:
        containers: /fluentd/buffer/logs.containers
        kubelet: /fluentd/buffer/logs.kubelet
        systemd: /fluentd/buffer/logs.systemd
        default: /fluentd/buffer/logs.default
      metrics:
        apiserver: /fluentd/buffer/metrics.apiserver
        kubelet: /fluentd/buffer/metrics.kubelet
        container: /fluentd/buffer/metrics.container
        controller: /fluentd/buffer/metrics.controller
        scheduler: /fluentd/buffer/metrics.scheduler
        state: /fluentd/buffer/metrics.state
        node: /fluentd/buffer/metrics.node
        default: /fluentd/buffer/metrics.default
      events: /fluentd/buffer/events

    ## Additional config for buffer settings
    extraConf: |-

  metadata:
    ## Option to control the enabling of metadata filter plugin cache_size.
    ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
    cacheSize: "10000"
    ## Option to control the enabling of metadata filter plugin cache_ttl (in seconds).
    ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
    cacheTtl: "3600"
    ## Option to control the interval at which metadata cache is asynchronously refreshed (in seconds).
    cacheRefresh: "1800"

  logs:
    ## Configuration for sumologic output plugin
    ## ref: https://github.com/SumoLogic/fluentd-output-sumologic
    ## ref: https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/master/deploy/helm/sumologic/conf/logs/logs.output.conf
    output:
      ## Format to post logs into Sumo: fields, json, json_merge, or text.
      ## NOTE: for logs metadata, fields is required.
      logFormat: fields
      ## Option to control adding timestamp to logs.
      addTimestamp: true
      ## Field name when add_timestamp is on.
      timestampKey: "timestamp"
      ## Additional config parameters for sumologic output plugin
      extraConf: |-

    ## Additional config for custom log pipelines
    ## ref: TODO: documentation for custom logs pipelines
    extraLogs: |-

    ## Container log configuration
    containers:
      outputConf: |-
        @include logs.output.conf
      ## Set the _sourceName metadata field in Sumo Logic.
      sourceName: "%{namespace}.%{pod}.%{container}"
      ## Set the _sourceHost metadata field in Sumo Logic.
      sourceHost: ""
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "%{namespace}/%{pod_name}"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for containers.
      ## Matching containers will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeContainerRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for namespaces.
      ## Matching namespaces will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeNamespaceRegex: ""
      ## A regular expression for pods.
      ## Matching pods will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePodRegex: ""

      ## ref: https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter#configuration
      k8sMetadataFilter:
        ## Option to control the enabling of metadata filter plugin watch.
        watch: "true"
        ## path to CA file for Kubernetes server certificate validation
        caFile: ""
        ## Validate SSL certificates
        verifySsl: true
        ## Path to a client cert file to authenticate to the API server
        clientCert: ""
        ## Path to a client key file to authenticate to the API server
        clientKey: ""
        ## Path to a file containing the bearer token to use for authentication
        bearerTokenFile: ""

      ## To use additional filter plugins
      extraFilterPluginConf: |-

    ## Kubelet log configuration
    kubelet:
      outputConf: |-
        @include logs.output.conf
      ## Set the _sourceName metadata field in Sumo Logic.
      sourceName: "k8s_kubelet"
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "kubelet"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for facility.
      ## Matching facility will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeFacilityRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for priority.
      ## Matching priority will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePriorityRegex: ""
      ## A regular expression for unit.
      ## Matching unit will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeUnitRegex: ""

    ## Systemd log configuration
    systemd:
      outputConf: |-
        @include logs.output.conf
      ## Set the _sourceCategory metadata field in Sumo Logic.
      sourceCategory: "system"
      ## Set the prefix, for _sourceCategory metadata.
      sourceCategoryPrefix: "kubernetes/"
      ## Used to replace - with another character.
      sourceCategoryReplaceDash: "/"

      ## A regular expression for facility.
      ## Matching facility will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeFacilityRegex: ""
      ## A regular expression for hosts.
      ## Matching hosts will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeHostRegex: ""
      ## A regular expression for priority.
      ## Matching priority will be excluded from Sumo. The logs will still be sent to FluentD.
      excludePriorityRegex: ""
      ## A regular expression for unit.
      ## Matching unit will be excluded from Sumo. The logs will still be sent to FluentD.
      excludeUnitRegex: ""

    ## Default log configuration (catch-all)
    default:
      outputConf: |-
        @include logs.output.conf

  metrics:
    ## Configuration for sumologic output plugin
    ## ref: https://github.com/SumoLogic/fluentd-output-sumologic
    ## ref: https://github.com/SumoLogic/sumologic-kubernetes-collection/blob/master/deploy/helm/sumologic/conf/metrics/metrics.output.conf
    outputConf: |-
      @include metrics.output.conf

    ## To use additional filter plugins
    extraFilterPluginConf: |-

    ## To use additional output plugins
    extraOutputPluginConf: |-

  events:
    # If enabled, collect K8s events
    enabled: true
    # Source category for the Events source. Default: "{clusterName}/events"
    sourceCategory: ""
    ## Override Kubernetes resource types you want to get events for from different Kubernetes
    ## API versions. The key represents the name of the resource type and the value represents
    ## the API version.
    # watchResourceEventsOverrides:
    #   pods: "v1"
    #   events: "events.k8s.io/v1beta1"

## Configure metrics-server
## ref: https://github.com/helm/charts/blob/master/stable/metrics-server/values.yaml
metrics-server:
  ## Set the enabled flag to true for enabling metrics-server.
  ## This is required before enabling fluentd autoscaling unless you have an existing metrics-server in the cluster.
  enabled: false
  args:
    - --kubelet-insecure-tls
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname

## Configure fluent-bit
## ref: https://github.com/helm/charts/blob/master/stable/fluent-bit/values.yaml
fluent-bit:
  ## Resource limits for fluent-bit
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 10m
    #   memory: 8Mi

  enabled: true
  service:
    flush: 5
  metrics:
    enabled: true
  env:
    - name: CHART
      valueFrom:
        configMapKeyRef:
          name: sumologic-configmap
          key: release
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace

  backend:
    type: forward
    forward:
      # NOTE: Requires trailing "." for fully-qualified name resolution
      host: ${CHART}.${NAMESPACE}.svc.cluster.local.
      port: 24321
      tls: "off"
      tls_verify: "on"
      tls_debug: 1
      shared_key:

  trackOffsets: true

  tolerations:
    - effect: NoSchedule
      operator: Exists

  input:
    systemd:
      enabled: true

  parsers:
    enabled: true
    # This regex matches the first line of a multiline log starting with a date of the format :  "2019-11-17 07:14:12" or "2019-11-17T07:14:12"
    regex:
      - name: multi_line
        regex: (?<log>^{"log":"\d{4}-\d{1,2}-\d{1,2}.\d{2}:\d{2}:\d{2}.*)

  rawConfig: |-
    @INCLUDE fluent-bit-service.conf

    [INPUT]
      Name             tail
      Path             /var/log/containers/*.log
      Multiline        On
      Parser_Firstline multi_line
      Tag              containers.*
      Refresh_Interval 1
      Rotate_Wait      60
      Mem_Buf_Limit    5MB
      Skip_Long_Lines  On
      DB               /tail-db/tail-containers-state.db
      DB.Sync          Normal
    [INPUT]
      Name            systemd
      Tag             host.*
      Systemd_Filter  _SYSTEMD_UNIT=addon-config.service
      Systemd_Filter  _SYSTEMD_UNIT=addon-run.service
      Systemd_Filter  _SYSTEMD_UNIT=cfn-etcd-environment.service
      Systemd_Filter  _SYSTEMD_UNIT=cfn-signal.service
      Systemd_Filter  _SYSTEMD_UNIT=clean-ca-certificates.service
      Systemd_Filter  _SYSTEMD_UNIT=containerd.service
      Systemd_Filter  _SYSTEMD_UNIT=coreos-metadata.service
      Systemd_Filter  _SYSTEMD_UNIT=coreos-setup-environment.service
      Systemd_Filter  _SYSTEMD_UNIT=coreos-tmpfiles.service
      Systemd_Filter  _SYSTEMD_UNIT=dbus.service
      Systemd_Filter  _SYSTEMD_UNIT=docker.service
      Systemd_Filter  _SYSTEMD_UNIT=efs.service
      Systemd_Filter  _SYSTEMD_UNIT=etcd-member.service
      Systemd_Filter  _SYSTEMD_UNIT=etcd.service
      Systemd_Filter  _SYSTEMD_UNIT=etcd2.service
      Systemd_Filter  _SYSTEMD_UNIT=etcd3.service
      Systemd_Filter  _SYSTEMD_UNIT=etcdadm-check.service
      Systemd_Filter  _SYSTEMD_UNIT=etcdadm-reconfigure.service
      Systemd_Filter  _SYSTEMD_UNIT=etcdadm-save.service
      Systemd_Filter  _SYSTEMD_UNIT=etcdadm-update-status.service
      Systemd_Filter  _SYSTEMD_UNIT=flanneld.service
      Systemd_Filter  _SYSTEMD_UNIT=format-etcd2-volume.service
      Systemd_Filter  _SYSTEMD_UNIT=kube-node-taint-and-uncordon.service
      Systemd_Filter  _SYSTEMD_UNIT=kubelet.service
      Systemd_Filter  _SYSTEMD_UNIT=ldconfig.service
      Systemd_Filter  _SYSTEMD_UNIT=locksmithd.service
      Systemd_Filter  _SYSTEMD_UNIT=logrotate.service
      Systemd_Filter  _SYSTEMD_UNIT=lvm2-monitor.service
      Systemd_Filter  _SYSTEMD_UNIT=mdmon.service
      Systemd_Filter  _SYSTEMD_UNIT=nfs-idmapd.service
      Systemd_Filter  _SYSTEMD_UNIT=nfs-mountd.service
      Systemd_Filter  _SYSTEMD_UNIT=nfs-server.service
      Systemd_Filter  _SYSTEMD_UNIT=nfs-utils.service
      Systemd_Filter  _SYSTEMD_UNIT=node-problem-detector.service
      Systemd_Filter  _SYSTEMD_UNIT=ntp.service
      Systemd_Filter  _SYSTEMD_UNIT=oem-cloudinit.service
      Systemd_Filter  _SYSTEMD_UNIT=rkt-gc.service
      Systemd_Filter  _SYSTEMD_UNIT=rkt-metadata.service
      Systemd_Filter  _SYSTEMD_UNIT=rpc-idmapd.service
      Systemd_Filter  _SYSTEMD_UNIT=rpc-mountd.service
      Systemd_Filter  _SYSTEMD_UNIT=rpc-statd.service
      Systemd_Filter  _SYSTEMD_UNIT=rpcbind.service
      Systemd_Filter  _SYSTEMD_UNIT=set-aws-environment.service
      Systemd_Filter  _SYSTEMD_UNIT=system-cloudinit.service
      Systemd_Filter  _SYSTEMD_UNIT=systemd-timesyncd.service
      Systemd_Filter  _SYSTEMD_UNIT=update-ca-certificates.service
      Systemd_Filter  _SYSTEMD_UNIT=user-cloudinit.service
      Systemd_Filter  _SYSTEMD_UNIT=var-lib-etcd2.service
      Max_Entries     1000
      Read_From_Tail  true

    @INCLUDE fluent-bit-output.conf

## Configure prometheus-operator
## ref: https://github.com/helm/charts/blob/master/stable/prometheus-operator/values.yaml
grafana:
  enabled: false
prometheus-operator:
  # Ensure we use pre 1.14 recording rules consistently as current content depends on them.
  kubeTargetVersionOverride: 1.13.0-0
  ## Set the enabled flag to false for either of the below two purposes:
  ## 1. Not install prometheus operator helm chart as a dependency along with this helm chart
  ## 2. Disable metrics collection altogether
  enabled: true
  alertmanager:
    enabled: false
  grafana:
    enabled: false
    defaultDashboardsEnabled: false
  prometheusOperator:
    ## Resource limits for prometheus operator
    resources: {}
    admissionWebhooks:
      enabled: false
    tlsProxy:
      enabled: false
  prometheus:
    additionalServiceMonitors:
      - name: collection-sumologic
        additionalLabels:
          app: collection-sumologic
        endpoints:
        - port: metrics
        namespaceSelector:
          matchNames:
          - sumologic
        selector:
          matchLabels:
            app: collection-sumologic
      - name: collection-sumologic-events
        additionalLabels:
          app: collection-sumologic-events
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - sumologic
        selector:
          matchLabels:
            app: collection-sumologic-events
      - name: collection-fluent-bit
        additionalLabels:
          app: collection-fluent-bit
        endpoints:
          - port: metrics
            path: /api/v1/metrics/prometheus
        namespaceSelector:
          matchNames:
            - sumologic
        selector:
          matchLabels:
            app: fluent-bit
      - name: collection-sumologic-otelcol
        additionalLabels:
          app: collection-sumologic-otelcol
        endpoints:
          - port: metrics
        namespaceSelector:
          matchNames:
            - sumologic
        selector:
          matchLabels:
            app: collection-sumologic-otelcol
    prometheusSpec:
      ## Define resources requests and limits for single Pods.
      resources: {}
      # requests:
      #   memory: 400Mi
      thanos:
        version: v0.10.0
      containers:
      - name: "prometheus-config-reloader"
        env:
        - name: CHART
          valueFrom:
            configMapKeyRef:
              name: sumologic-configmap
              key: release
        - name: NAMESPACE
          valueFrom:
            configMapKeyRef:
              name: sumologic-configmap
              key: fluentdNamespace

      remoteWrite:
        # kube state metrics
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.state
          writeRelabelConfigs:
            - action: keep
              regex: kube-state-metrics;(?:kube_statefulset_status_observed_generation|kube_statefulset_status_replicas|kube_statefulset_replicas|kube_statefulset_metadata_generation|kube_daemonset_status_current_number_scheduled|kube_daemonset_status_desired_number_scheduled|kube_daemonset_status_number_misscheduled|kube_daemonset_status_number_unavailable|kube_daemonset_metadata_generation|kube_deployment_metadata_generation|kube_deployment_spec_paused|kube_deployment_spec_replicas|kube_deployment_spec_strategy_rollingupdate_max_unavailable|kube_deployment_status_replicas_available|kube_deployment_status_observed_generation|kube_deployment_status_replicas_unavailable|kube_node_info|kube_node_spec_unschedulable|kube_node_status_allocatable|kube_node_status_capacity|kube_node_status_condition|kube_pod_container_info|kube_pod_container_resource_requests|kube_pod_container_resource_limits|kube_pod_container_status_ready|kube_pod_container_status_terminated_reason|kube_pod_container_status_waiting_reason|kube_pod_container_status_restarts_total|kube_pod_status_phase)
              sourceLabels: [job, __name__]
        # controller manager metrics
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.controller-manager
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;cloudprovider_.*_api_request_duration_seconds.*
              sourceLabels: [job, __name__]
        # scheduler metrics
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.scheduler
          writeRelabelConfigs:
            - action: keep
              regex: kube-scheduler;scheduler_(?:e2e_scheduling|binding|scheduling_algorithm)_latency_microseconds.*
              sourceLabels: [job, __name__]
        # api server metrics
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.apiserver
          writeRelabelConfigs:
            - action: keep
              regex: apiserver;(?:apiserver_request_(?:count|total)|apiserver_request_(?:latencies|duration_seconds).*|etcd_request_cache_get_(?:latencies_summary|duration_seconds).*|etcd_request_cache_add_(?:latencies_summary|duration_seconds).*|etcd_helper_cache_hit_(?:count|total)|etcd_helper_cache_miss_(?:count|total))
              sourceLabels: [job, __name__]
        # kubelet metrics
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.kubelet
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;(?:kubelet_docker_operations_errors.*|kubelet_docker_operations_(?:latency_micro|duration_)seconds.*|kubelet_running_container_count|kubelet_running_pod_count|kubelet_runtime_operations_(?:latency_micro|duration_)seconds.*)
              sourceLabels: [job, __name__]
        # cadvisor container metrics
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.container
          writeRelabelConfigs:
            - action: labelmap
              regex: container_name
              replacement: container
            - action: drop
              regex: POD
              sourceLabels: [container]
            - action: keep
              regex: kubelet;.+;(?:container_cpu_load_average_10s|container_cpu_system_seconds_total|container_cpu_usage_seconds_total|container_cpu_cfs_throttled_seconds_total|container_memory_usage_bytes|container_memory_swap|container_memory_working_set_bytes|container_spec_memory_limit_bytes|container_spec_memory_swap_limit_bytes|container_spec_memory_reservation_limit_bytes|container_spec_cpu_quota|container_spec_cpu_period|container_fs_usage_bytes|container_fs_limit_bytes|container_fs_reads_bytes_total|container_fs_writes_bytes_total|)
              sourceLabels: [job,container,__name__]
        # cadvisor aggregate container metrics
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.container
          writeRelabelConfigs:
            - action: keep
              regex: kubelet;(?:container_network_receive_bytes_total|container_network_transmit_bytes_total|container_network_receive_errors_total|container_network_transmit_errors_total|container_network_receive_packets_dropped_total|container_network_transmit_packets_dropped_total)
              sourceLabels: [job,__name__]
        # node exporter metrics
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.node
          writeRelabelConfigs:
            - action: keep
              regex: node-exporter;(?:node_load1|node_load5|node_load15|node_cpu_seconds_total|node_memory_MemAvailable_bytes|node_memory_MemTotal_bytes|node_memory_Buffers_bytes|node_memory_SwapCached_bytes|node_memory_Cached_bytes|node_memory_MemFree_bytes|node_memory_SwapFree_bytes|node_ipvs_incoming_bytes_total|node_ipvs_outgoing_bytes_total|node_ipvs_incoming_packets_total|node_ipvs_outgoing_packets_total|node_disk_reads_completed_total|node_disk_writes_completed_total|node_disk_read_bytes_total|node_disk_written_bytes_total|node_filesystem_avail_bytes|node_filesystem_free_bytes|node_filesystem_size_bytes|node_filesystem_files)
              sourceLabels: [job, __name__]
        # prometheus operator rules
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics.operator.rule
          writeRelabelConfigs:
            - action: keep
              regex: 'cluster_quantile:apiserver_request_latencies:histogram_quantile|instance:node_cpu:rate:sum|instance:node_filesystem_usage:sum|instance:node_network_receive_bytes:rate:sum|instance:node_network_transmit_bytes:rate:sum|instance:node_cpu:ratio|cluster:node_cpu:sum_rate5m|cluster:node_cpu:ratio|cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile|cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile|cluster_quantile:scheduler_binding_latency:histogram_quantile|node_namespace_pod:kube_pod_info:|:kube_pod_info_node_count:|node:node_num_cpu:sum|:node_cpu_utilisation:avg1m|node:node_cpu_utilisation:avg1m|node:cluster_cpu_utilisation:ratio|:node_cpu_saturation_load1:|node:node_cpu_saturation_load1:|:node_memory_utilisation:|:node_memory_MemFreeCachedBuffers_bytes:sum|:node_memory_MemTotal_bytes:sum|node:node_memory_bytes_available:sum|node:node_memory_bytes_total:sum|node:node_memory_utilisation:ratio|node:cluster_memory_utilisation:ratio|:node_memory_swap_io_bytes:sum_rate|node:node_memory_utilisation:|node:node_memory_utilisation_2:|node:node_memory_swap_io_bytes:sum_rate|:node_disk_utilisation:avg_irate|node:node_disk_utilisation:avg_irate|:node_disk_saturation:avg_irate|node:node_disk_saturation:avg_irate|node:node_filesystem_usage:|node:node_filesystem_avail:|:node_net_utilisation:sum_irate|node:node_net_utilisation:sum_irate|:node_net_saturation:sum_irate|node:node_net_saturation:sum_irate|node:node_inodes_total:|node:node_inodes_free:'
              sourceLabels: [__name__]
        # health
        - url: http://$(CHART).$(NAMESPACE).svc.cluster.local:9888/prometheus.metrics
          writeRelabelConfigs:
            - action: keep
              regex: (?:up|prometheus_remote_storage_.*|fluentd_.*|fluentbit.*|otelcol.*)
              sourceLabels: [__name__]

## Configure falco
## ref: https://github.com/helm/charts/blob/master/stable/falco/values.yaml
falco:
  ## Set the enabled flag to false to disable falco.
  enabled: true
  #ebpf:
  #  enabled: true
  falco:
    jsonOutput: true

##Configure otelcol
otelcol:
  deployment:
    nodeSelector: {}
    tolerations: {}
    replicas: 1
    resources:
      limits:
        memory: 2Gi
        cpu: 1
      requests:
        memory: 384Mi
        cpu: "200m"
    # Memory Ballast size should be max 1/3 to 1/2 of memory.
    memBallastSizeMib: "683"
    image:
      name: "sumologic/opentelemetry-collector"
      tag: "0.2.7.0"
      pullPolicy: IfNotPresent
  config:
    receivers:
      jaeger:
        protocols:
          thrift_compact:
            endpoint: "0.0.0.0:6831"
          thrift_binary:
            endpoint: "0.0.0.0:6832"
          grpc:
            endpoint: "0.0.0.0:14250"
          thrift_tchannel:
            endpoint: "0.0.0.0:14267"
          thrift_http:
            endpoint: "0.0.0.0:14268"
      opencensus:
        endpoint: "0.0.0.0:55678"
      zipkin:
        endpoint: "0.0.0.0:9411"
    processors:
      # Tags spans with K8S metadata, basing on the context IP
      k8s_tagger:
        # When true, only IP is assigned and passed (so it could be tagged on another collector)
        passthrough: false
        # Extracted fields and assigned names
        extract:
          metadata:
            # extract the following well-known metadata fields
            - containerId
            - containerName
            - cluster
            - daemonSetName
            - deployment
            - hostName
            - namespace
            - node
            - owners
            - podId
            - podName
            - replicaSetName
            - serviceName
            - startTime
            - statefulSetName
          tags:
            containerId: container_id
            containerName: container
            cluster: cluster
            daemonSetName: daemonset
            deployment: deployment
            hostName: hostname
            namespace: namespace
            node: node
            podId: pod_id
            podName: pod
            replicaSetName: replicaset
            serviceName: service_name
            startTime: start_time
            statefulSetName: statefulset
          annotations:
            - tag_name: pod_annotation_%s
              key: "*"
          labels:
            - tag_name: pod_label_%s
              key: "*"

      # The memory_limiter processor is used to prevent out of memory situations on the collector.
      memory_limiter:
        # check_interval is the time between measurements of memory usage for the
        # purposes of avoiding going over the limits. Defaults to zero, so no
        # checks will be performed. Values below 1 second are not recommended since
        # it can result in unnecessary CPU consumption.
        check_interval: 5s

        # Maximum amount of memory, in MiB, targeted to be allocated by the process heap.
        # Note that typically the total memory usage of process will be about 50MiB higher
        # than this value.
        limit_mib: 1900

      # The queued_retry processor uses a bounded queue to relay batches from the receiver or previous
      # processor to the next processor.
      queued_retry:
        # Number of workers that dequeue batches
        num_workers: 16
        # Maximum number of batches kept in memory before data is dropped
        queue_size: 10000
        # Whether to retry on failure or give up and drop
        retry_on_failure: true

      # The batch processor accepts spans and places them into batches grouped by node and resource
      batch:
        # Number of spans after which a batch will be sent regardless of time
        send_batch_size: 256
        # Number of tickers that loop over batch buckets
        num_tickers: 10
        # Time duration after which a batch will be sent regardless of size
        timeout: 5s
    extensions:
      health_check: {}
    exporters:
      zipkin:
        url: "exporters.zipkin.url_replace"
      # Following generates verbose logs with span content, useful to verify what
      # metadata is being tagged. To enable, uncomment and add "logging" to exporters below.
      # There are two levels that could be used: `debug` and `info` with the former
      # being much more verbose and including (sampled) spans content
      # logging:
      #   loglevel: debug
    service:
      extensions: [health_check]
      pipelines:
        traces:
          receivers: [jaeger, zipkin, opencensus]
          processors: [memory_limiter, k8s_tagger, batch, queued_retry]
          exporters: [zipkin]
