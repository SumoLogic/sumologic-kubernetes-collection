## Configuration for Batch Processor
## The batch processor accepts spans and places them into batches grouped by node and resource
batch:
  ## Maximum number of spans sent at once
  send_batch_max_size: 2_048
  ## Number of spans after which a batch will be sent regardless of time
  send_batch_size: 1_024
  ## Time duration after which a batch will be sent regardless of size
  timeout: 1s

# Prometheus receiver puts all labels in record-level attributes, and we need them in resource
groupbyattrs:
  keys:
    - container
    - namespace
    - pod
    - service

## The Kubernetes sprocessor automatically tags logs, metrics and traces with Kubernetes metadata like pod name, namespace name etc.
k8s_tagger:
  extract:
    delimiter: "_"
    labels:
      - key: "*"
        tag_name: "pod_labels_%s"
    metadata:
      ## extract the following well-known metadata fields
      - daemonSetName
      - deploymentName
      - nodeName
      - replicaSetName
      - serviceName
      - statefulSetName
  owner_lookup_enabled: true  # To enable fetching additional metadata using `owner` relationship
  ## Has to be false to enrich metadata
  passthrough: false
  pod_association:
    - from: build_hostname  # Pods are identified by Pod name and namespace

## Configuration for Memory Limiter Processor
## The memory_limiter processor is used to prevent out of memory situations on the collector.
memory_limiter:
  ## check_interval is the time between measurements of memory usage for the
  ## purposes of avoiding going over the limits. Defaults to zero, so no
  ## checks will be performed. Values below 1 second are not recommended since
  ## it can result in unnecessary CPU consumption.
  check_interval: 5s
  ## Maximum amount of memory, in %, targeted to be allocated by the process heap.
  limit_percentage: 75
  ## Spike limit (calculated from available memory). Must be less than limit_percentage.
  spike_limit_percentage: 20

## Configuration for Metrics Transform Processor
metricstransform:
  transforms:
    ## rename all prometheus_remote_write_$name metrics to $name
    action: update
    include: ^prometheus_remote_write_(.*)$$
    match_type: regexp
    new_name: $$1

## Configuration for Resource Processor
resource:
  attributes:
    - action: upsert
      from_attribute: namespace
      key: k8s.namespace.name
    - action: delete
      key: namespace  # remove namespace to avoid duplication when attribute translation is enabled
    - action: upsert
      from_attribute: pod
      key: k8s.pod.name
    - action: delete
      key: pod  # remove pod to avoid duplication when attribute translation is enabled
    - action: upsert
      from_attribute: container
      key: k8s.container.name  # add container in OpenTelemetry convention to unify configuration for Source processor
    - action: delete
      key: container  # remove container to avoid duplication when attribute translation is enabled
    - action: upsert
      from_attribute: service
      key: prometheus_service
    - action: delete
      key: service
    - action: upsert
      from_attribute: service.name
      key: job
    - action: delete  # we don't want service.name to be set, as the schema processor translates it to "service"
      key: service.name
    - action: upsert
      key: _origin  # add "_origin" metadata to metrics to keep the same format as for metrics from Fluentd
      value: kubernetes
    - action: upsert
      key: cluster
      value: {{ .Values.sumologic.clusterName | quote }}

## NOTE: Drop these for now and and when proper configuration options
## are exposed and source processor is configured then send them
## as headers.
resource/delete_source_metadata:
  attributes:
    - action: delete
      key: _sourceCategory
    - action: delete
      key: _sourceHost
    - action: delete
      key: _sourceName
resource/remove_k8s_pod_pod_name:
  attributes:
    - action: delete
      key: k8s.pod.pod_name

transform/prepare_routing:
  error_mode: ignore
  metric_statements:
    - context: metric
      statements:
        - set(resource.attributes["http_listener_v2_path"], "/prometheus.metrics.apiserver") where resource.attributes["job"] == "apiserver"
        - set(resource.attributes["http_listener_v2_path"], "/prometheus.metrics.control-plane.coredns") where resource.attributes["job"] == "coredns"
        - set(resource.attributes["http_listener_v2_path"], "/prometheus.metrics.control-plane.kube-etcd") where resource.attributes["job"] == "kube-etcd"
        - set(resource.attributes["http_listener_v2_path"], "/prometheus.metrics.controller-manager") where resource.attributes["job"] == "kubelet" and IsMatch(name, "^cloudprovider_.*")
        - set(resource.attributes["http_listener_v2_path"], "/prometheus.metrics.kubelet") where resource.attributes["job"] == "kubelet" and not IsMatch(name, "^cloudprovider_.*")
        - set(resource.attributes["http_listener_v2_path"], "/prometheus.metrics.node") where resource.attributes["job"] == "node-exporter"
        - set(resource.attributes["http_listener_v2_path"], "/prometheus.metrics.scheduler") where resource.attributes["job"] == "kube-scheduler"
        - set(resource.attributes["http_listener_v2_path"], "/prometheus.metrics.state") where resource.attributes["job"] == "kube-state-metrics"

## NOTE: below listed rules could be simplified if routingprocessor
## supports regex matching. At this point we could group route entries
## going to the same set of exporters.
routing:
  attribute_source: resource
  default_exporters:
    - sumologic/default
  drop_resource_routing_attribute: true
  from_attribute: http_listener_v2_path
  table:
    ## apiserver metrics
    - exporters:
        - sumologic/apiserver
      value: /prometheus.metrics.apiserver
    ## control-plane metrics
    - exporters:
        - sumologic/control_plane
      value: /prometheus.metrics.control-plane.coredns
    - exporters:
        - sumologic/control_plane
      value: /prometheus.metrics.control-plane.kube-etcd
    ## controller metrics
    - exporters:
        - sumologic/controller
      value: /prometheus.metrics.controller-manager
    ## kubelet metrics
    - exporters:
        - sumologic/kubelet
      value: /prometheus.metrics.kubelet
    ## node metrics
    - exporters:
        - sumologic/node
      value: /prometheus.metrics.node
    ## scheduler metrics
    - exporters:
        - sumologic/scheduler
      value: /prometheus.metrics.scheduler
    ## state metrics
    - exporters:
        - sumologic/state
      value: /prometheus.metrics.state

## Configuration for Source Processor
## Source processor adds Sumo Logic related metadata
source:
  collector: {{ .Values.sumologic.collectorName | default .Values.sumologic.clusterName | quote }}

## The Sumo Logic Schema processor modifies the metadata on logs, metrics and traces sent to Sumo Logic
## so that the Sumo Logic apps can make full use of the ingested data.
sumologic_schema:
  add_cloud_namespace: false

{{- if .Values.sumologic.metrics.otelcol.extraProcessors }}
{{- range $processor := .Values.sumologic.metrics.otelcol.extraProcessors }}
{{ toYaml $processor }}
{{- end }}
{{- end }}
