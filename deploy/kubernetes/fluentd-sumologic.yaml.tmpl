# This file is auto-generated.
---
# Source: sumologic/templates/logs-pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: collection-sumologic-fluentd-logs-pdb
spec:
  selector:
    matchLabels:
      app: collection-sumologic-fluentd-logs
  minAvailable: 2
  
---
# Source: sumologic/templates/metrics-pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: collection-sumologic-fluentd-metrics-pdb
spec:
  selector:
    matchLabels:
      app: collection-sumologic-fluentd-metrics
  minAvailable: 2
  
---
# Source: sumologic/templates/chart-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: sumologic-configmap
data:
  fluentdLogs: collection-sumologic-fluentd-logs
  fluentdMetrics: collection-sumologic-fluentd-metrics
  fluentdNamespace: $NAMESPACE
---
# Source: sumologic/templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: collection-sumologic-fluentd-logs
  labels:
    app: collection-sumologic-fluentd-logs
    
data:
  fluent.conf: |-
    @include common.conf
    @include logs.conf
  buffer.output.conf: |-
    compress "gzip"
    flush_interval "5s"
    flush_thread_count "8"
    chunk_limit_size "1m"
    total_limit_size "128m"
    queued_chunks_limit_size "128"
    overflow_action drop_oldest_chunk
    retry_max_interval "10m"
    retry_forever "true"
    
  common.conf: |-
    # Prevent fluentd from handling records containing its own logs.
    <match fluentd.**>
      @type null
    </match>
    # expose the Fluentd metrics to Prometheus
    <source>
      @type prometheus
      metrics_path /metrics
      port 24231
    </source>
    <source>
      @type prometheus_output_monitor
    </source>
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>
    <system>
      log_level info
    </system>
  
  logs.conf: |-
    <source>
      @type forward
      port 24321
      bind 0.0.0.0
      
    </source>
    @include logs.source.containers.conf
    @include logs.source.systemd.conf
    
    @include logs.source.default.conf
  logs.enhance.k8s.metadata.filter.conf: |-
    cache_size  "10000"
    cache_ttl  "7200"
    cache_refresh "3600"
    cache_refresh_variation "900"
    in_namespace_path '$.kubernetes.namespace_name'
    in_pod_path '$.kubernetes.pod_name'
    core_api_versions v1
    api_groups apps/v1,extensions/v1beta1
    data_type logs
  logs.kubernetes.metadata.filter.conf: |-
    annotation_match ["sumologic\.com.*"]
    de_dot false
    watch "true"
    ca_file ""
    verify_ssl "true"
    client_cert ""
    client_key ""
    bearer_token_file ""
    cache_size "10000"
    cache_ttl "7200"
    tag_to_kubernetes_name_regexp '.+?\.containers\.(?<pod_name>[^_]+)_(?<namespace>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$'
  logs.kubernetes.sumologic.filter.conf: |-
    source_name "%{namespace}.%{pod}.%{container}"
    source_host 
    log_format "fields"
    source_category "%{namespace}/%{pod_name}"
    source_category_prefix "kubernetes/"
    source_category_replace_dash "/"
    exclude_pod_regex ""
    exclude_container_regex ""
    exclude_host_regex ""
  logs.output.conf: |-
    data_type logs
    log_key log
    endpoint "#{ENV['SUMO_ENDPOINT_DEFAULT_LOGS_SOURCE']}"
    verify_ssl "true"
    log_format "fields"
    add_timestamp "true"
    timestamp_key "timestamp"
    proxy_uri ""
    
  logs.source.containers.conf: |
    
    
    <filter containers.**>
      @type record_transformer
      enable_ruby
      renew_record true
      <record>
        log    ${record["log"].split(/[\n\t]+/).map! {|item| JSON.parse(item)["log"]}.any? ? record["log"].split(/[\n\t]+/).map! {|item| JSON.parse(item)["log"]}.join("") : record["log"] rescue record["log"]}
        stream ${[record["log"].split(/[\n\t]+/)[0]].map! {|item| JSON.parse(item)["stream"]}.any? ? [record["log"].split(/[\n\t]+/)[0]].map! {|item| JSON.parse(item)["stream"]}.join("") : record["stream"] rescue record["stream"]}
        time   ${[record["log"].split(/[\n\t]+/)[0]].map! {|item| JSON.parse(item)["time"]}.any? ? [record["log"].split(/[\n\t]+/)[0]].map! {|item| JSON.parse(item)["time"]}.join("") : record["time"] rescue record["time"]}
      </record>
    </filter>
    # match all  container logs and label them @NORMAL
    <match containers.**>
      @type relabel
      @label @NORMAL
    </match>
    <label @NORMAL>
    
      # only match fluentd logs based on fluentd container log file name.
      # by default, this is <filter **collection-sumologic-fluentd**>
      <filter **collection-sumologic-fluentd**>
        # only ingest fluentd logs of levels: {error, fatal} and warning messages if buffer is full
        @type grep
        <regexp>
          key log
          pattern /\[error\]|\[fatal\]|drop_oldest_chunk|retry succeeded/
        </regexp>
      </filter>
  
    
     <filter **collection-sumologic-otelcol**>
       @type grep
       <regexp>
         key log
         # Select only known error/warning/fatal/panic levels or logs coming from one of the source known to provide useful data
         pattern /\"level\":\"(error|warning|fatal|panic|dpanic)\"|\"caller\":\"(builder|service|kube|static)/
       </regexp>
     </filter>
  
      # third-party kubernetes metadata  filter plugin
      <filter containers.**>
        @type kubernetes_metadata
        @log_level error
        @include logs.kubernetes.metadata.filter.conf
      </filter>
      # sumologic kubernetes metadata enrichment filter plugin
      <filter containers.**>
        @type enhance_k8s_metadata
        @log_level error
        @include logs.enhance.k8s.metadata.filter.conf
      </filter>
      
      # kubernetes sumologic filter plugin
      <filter containers.**>
        @type kubernetes_sumologic
        @include logs.kubernetes.sumologic.filter.conf
        
        exclude_namespace_regex ""
      </filter>
      
      <match containers.**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.logs
          sumo_client "k8s_1.3.0"
          @log_level error
          @include logs.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
    </label>
  logs.source.default.conf: |
    
    <filter **>
      @type grep
      <exclude>
        key message
        pattern /disable filter chain optimization/
      </exclude>
    </filter>
    <filter **>
      @type kubernetes_sumologic
      source_name "k8s_default"
      source_category "default"
      source_category_prefix "kubernetes/"
      source_category_replace_dash "/"
      exclude_facility_regex ""
      exclude_host_regex ""
      exclude_priority_regex ""
      exclude_unit_regex ""
    </filter>
    <match **>
      @type copy
      <store>
        @type sumologic
        @id sumologic.endpoint.logs.default
        sumo_client "k8s_1.3.0"
        @include logs.output.conf
        <buffer>
          @type memory
          @include buffer.output.conf
        </buffer>
      </store>
    </match>
  logs.source.systemd.conf: |-
    
    <match host.kubelet.**>
      @type relabel
      @label @KUBELET
    </match>
    <label @KUBELET>
      <filter host.kubelet.**>
        @type kubernetes_sumologic
        source_category "kubelet"
        source_name "k8s_kubelet"
        source_category_prefix "kubernetes/"
        source_category_replace_dash "/"
        exclude_facility_regex ""
        exclude_host_regex ""
        exclude_priority_regex ""
        exclude_unit_regex ""
      </filter>
      
      <match **>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.logs.kubelet
          sumo_client "k8s_1.3.0"
          @include logs.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
    </label>
    
    <match host.**>
      @type relabel
      @label @SYSTEMD
    </match>
    <label @SYSTEMD>
      <filter host.**>
        @type kubernetes_sumologic
        source_name "k8s_systemd"
        source_category "system"
        source_category_prefix "kubernetes/"
        source_category_replace_dash "/"
        exclude_facility_regex ""
        exclude_host_regex ""
        exclude_priority_regex ""
        exclude_unit_regex ""
      </filter>
      <filter host.**>
        @type record_modifier
        <record>
          _sumo_metadata ${record["_sumo_metadata"][:source] = tag_parts[1]; record["_sumo_metadata"]}
        </record>
      </filter>
      
      <match **>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.logs.systemd
          sumo_client "k8s_1.3.0"
          @include logs.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
    </label>
  
---
# Source: sumologic/templates/events-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: collection-sumologic-fluentd-events
  labels:
    app: collection-sumologic-fluentd-events
    
data:
  fluent.conf: |-
    @include events.conf
  events.conf: |-
    <source>
      @type events
      deploy_namespace $NAMESPACE
    </source>
    # Prevent fluentd from handling records containing its own logs.
    <match fluentd.**>
      @type null
    </match>
    # expose the Fluentd metrics to Prometheus
    <source>
      @type prometheus
      metrics_path /metrics
      port 24231
    </source>
    <source>
      @type prometheus_output_monitor
    </source>
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>
    <match kubernetes.**>
      @type copy
      <store>
        @type sumologic
        @id sumologic.endpoint.events
        sumo_client "k8s_1.3.0"
        endpoint "#{ENV['SUMO_ENDPOINT_DEFAULT_EVENTS_SOURCE']}"
        data_type logs
        disable_cookies true
        verify_ssl "true"
        proxy_uri ""
        <buffer>
          @type memory
          @include buffer.output.conf
        </buffer>
      </store>
    </match>
    <system>
      log_level info
    </system>
  
  buffer.output.conf: |-
    compress "gzip"
    flush_interval "5s"
    flush_thread_count "8"
    chunk_limit_size "1m"
    total_limit_size "128m"
    queued_chunks_limit_size "128"
    overflow_action drop_oldest_chunk
    retry_max_interval "10m"
    retry_forever "true"
    
  
---
# Source: sumologic/templates/metrics-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: collection-sumologic-fluentd-metrics
  labels:
    app: collection-sumologic-fluentd-metrics
    
data:
  fluent.conf: |-
    @include common.conf
    @include metrics.conf
  buffer.output.conf: |-
    compress "gzip"
    flush_interval "5s"
    flush_thread_count "8"
    chunk_limit_size "1m"
    total_limit_size "128m"
    queued_chunks_limit_size "128"
    overflow_action drop_oldest_chunk
    retry_max_interval "10m"
    retry_forever "true"
    
  common.conf: |-
    # Prevent fluentd from handling records containing its own logs.
    <match fluentd.**>
      @type null
    </match>
    # expose the Fluentd metrics to Prometheus
    <source>
      @type prometheus
      metrics_path /metrics
      port 24231
    </source>
    <source>
      @type prometheus_output_monitor
    </source>
    <source>
      @type http
      port 9880
      bind 0.0.0.0
    </source>
    <system>
      log_level info
    </system>
  
  metrics.conf: |-
    <source>
      @type http
      port 9888
      <parse>
        @type protobuf
      </parse>
    </source>
    <match prometheus.metrics**>
      @type datapoint
      @label @DATAPOINT
    </match>
    <label @DATAPOINT>
      <filter prometheus.metrics**> # NOTE: Remove this filter if you are running Kubernetes 1.13 or below.
        @type grep
        <exclude>
          key @metric
          pattern /^apiserver_request_count|^apiserver_request_latencies_summary|^kubelet_runtime_operations_latency_microseconds|^kubelet_docker_operations_latency_microseconds|^kubelet_docker_operations_errors$/
        </exclude>
      </filter>
      <filter prometheus.metrics**>
        @type record_modifier
        <record>
          cluster kubernetes
        </record>
      </filter>
      <filter prometheus.metrics**>
        @type enhance_k8s_metadata
        cache_size  "10000"
        cache_ttl  "7200"
        cache_refresh "3600"
        cache_refresh_variation "900"
        core_api_versions v1
        api_groups apps/v1,extensions/v1beta1
      </filter>
      
      <filter prometheus.metrics**>
        @type prometheus_format
        relabel container_name:container,pod_name:pod
      </filter>
      
      <match prometheus.metrics.apiserver**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics.apiserver
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_APISERVER_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
      <match prometheus.metrics.container**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics.container
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_KUBELET_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
      <match prometheus.metrics.control-plane**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics.control.plane
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_CONTROL_PLANE_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
      <match prometheus.metrics.controller-manager**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics.kube.controller.manager
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_CONTROLLER_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
      <match prometheus.metrics.kubelet**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics.kubelet
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_KUBELET_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
      <match prometheus.metrics.node**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics.node.exporter
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_NODE_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
      <match prometheus.metrics.scheduler**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics.kube.scheduler
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_SCHEDULER_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
      <match prometheus.metrics.state**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics.kube.state
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_STATE_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
      <match prometheus.metrics**>
        @type copy
        <store>
          @type sumologic
          @id sumologic.endpoint.metrics
          sumo_client "k8s_1.3.0"
          endpoint "#{ENV['SUMO_ENDPOINT_DEFAULT_METRICS_SOURCE']}"
        @include metrics.output.conf
          <buffer>
            @type memory
            @include buffer.output.conf
          </buffer>
        </store>
      </match>
      
    </label>
  metrics.output.conf: |-
    data_type metrics
    metric_data_format prometheus
    disable_cookies true
    proxy_uri ""
  
---
# Source: sumologic/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: collection-sumologic
  labels:
    app: collection-sumologic
    
---
# Source: sumologic/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: collection-sumologic
  labels:
    app: collection-sumologic
    
rules:
- apiGroups: ["", "apps", "extensions", "events.k8s.io"]
  resources:
  - configmaps
  - daemonsets
  - deployments
  - endpoints
  - events
  - namespaces
  - nodes
  - pods
  - replicasets
  - services
  - statefulsets
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
    - configmaps
  verbs: ["create", "patch"]

---
# Source: sumologic/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: collection-sumologic
  labels:
    app: collection-sumologic
    
subjects:
- kind: ServiceAccount
  namespace: $NAMESPACE
  name: collection-sumologic
roleRef:
  kind: ClusterRole
  name: collection-sumologic
  apiGroup: rbac.authorization.k8s.io
---
# Source: sumologic/templates/events-service-headless.yaml

apiVersion: v1
kind: Service
metadata:
  name: collection-sumologic-fluentd-events-headless
  labels:
    app: collection-sumologic-fluentd-events-headless
    
spec:
  selector:
    app: collection-sumologic-fluentd-events
  clusterIP: None
  ports:
  - name: metrics
    port: 24231
    targetPort: 24231
    protocol: TCP
---
# Source: sumologic/templates/events-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: collection-sumologic-fluentd-events
  labels:
    app: collection-sumologic-fluentd-events
    sumologic.com/app: fluentd-events
    sumologic.com/component: events
    
spec:
  selector:
    app: collection-sumologic-fluentd-events
  ports:
  - name: metrics
    port: 24231
    targetPort: 24231
    protocol: TCP
---
# Source: sumologic/templates/metrics-service-headless.yaml

apiVersion: v1
kind: Service
metadata:
  name: collection-sumologic-fluentd-metrics-headless
  labels:
    app: collection-sumologic-fluentd-metrics-headless
    
spec:
  selector:
    app: collection-sumologic-fluentd-metrics
  clusterIP: None
  ports:
  - name: prom-write
    port: 9888
    targetPort: 9888
    protocol: TCP
  - name: metrics
    port: 24231
    targetPort: 24231
    protocol: TCP

---
# Source: sumologic/templates/metrics-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: collection-sumologic-fluentd-metrics
  labels:
    app: collection-sumologic-fluentd-metrics
    sumologic.com/app: fluentd-metrics
    sumologic.com/component: metrics
    
spec:
  selector:
    app: collection-sumologic-fluentd-metrics
  ports:
  - name: prom-write
    port: 9888
    targetPort: 9888
    protocol: TCP
  - name: metrics
    port: 24231
    targetPort: 24231
    protocol: TCP

---
# Source: sumologic/templates/service-headless.yaml

apiVersion: v1
kind: Service
metadata:
  name: collection-sumologic-fluentd-logs-headless
  labels:
    app: collection-sumologic-fluentd-logs-headless
    
spec:
  selector:
    app: collection-sumologic-fluentd-logs
  clusterIP: None
  ports:
  - name: fluent-bit
    port: 24321
    targetPort: 24321
    protocol: TCP
  - name: metrics
    port: 24231
    targetPort: 24231
    protocol: TCP

---
# Source: sumologic/templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: collection-sumologic-fluentd-logs
  labels:
    app: collection-sumologic-fluentd-logs
    sumologic.com/app: fluentd-logs
    sumologic.com/component: logs
    
spec:
  selector:
    app: collection-sumologic-fluentd-logs
  ports:
  - name: fluent-bit
    port: 24321
    targetPort: 24321
    protocol: TCP
  - name: metrics
    port: 24231
    targetPort: 24231
    protocol: TCP

---
# Source: sumologic/templates/events-statefulset.yaml

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: collection-sumologic-fluentd-events
  labels:
    app: collection-sumologic-fluentd-events
    
spec:
  selector:
    matchLabels:
      app: collection-sumologic-fluentd-events
  serviceName: collection-sumologic-fluentd-events-headless
  podManagementPolicy: "Parallel"
  template:
    metadata:
      annotations:
      labels:
        app: collection-sumologic-fluentd-events
        
    spec:
      serviceAccountName: collection-sumologic
      volumes:
      - name: config-volume
        configMap:
          name: collection-sumologic-fluentd-events
      securityContext:
        fsGroup: 999
        
      containers:
      - name: fluentd-events
        image: sumologic/kubernetes-fluentd:1.3.0
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 200m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi
          
        volumeMounts:
        - name: config-volume
          mountPath: /fluentd/etc/
        livenessProbe:
          httpGet:
            path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
            port: 9880
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 3
        readinessProbe:
          httpGet:
            path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
            port: 9880
          initialDelaySeconds: 30
          periodSeconds: 5
        env:
        - name: SUMO_ENDPOINT_DEFAULT_EVENTS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-events
---
# Source: sumologic/templates/metrics-statefulset.yaml

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: collection-sumologic-fluentd-metrics
  labels:
    app: collection-sumologic-fluentd-metrics
    
spec:
  selector:
    matchLabels:
      app: collection-sumologic-fluentd-metrics
  serviceName: collection-sumologic-fluentd-metrics-headless
  podManagementPolicy: "Parallel"
  replicas: 3
  template:
    metadata:
      annotations:
      labels:
        app: collection-sumologic-fluentd-metrics
        
    spec:
      serviceAccountName: collection-sumologic
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - collection-sumologic-fluentd-logs
                  - collection-sumologic-fluentd-metrics
                - key: app
                  operator: In
                  values:
                  - prometheus-operator-prometheus
              topologyKey: "kubernetes.io/hostname"
      volumes:
      - name: config-volume
        configMap:
          name: collection-sumologic-fluentd-metrics
      securityContext:
        fsGroup: 999
        
      containers:
      - name: fluentd
        image: sumologic/kubernetes-fluentd:1.3.0
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 1000m
            memory: 1Gi
          requests:
            cpu: 500m
            memory: 768Mi
          
        ports:
        - name: prom-write
          containerPort: 9888
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
            port: 9880
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 3
        readinessProbe:
          httpGet:
            path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
            port: 9880
          initialDelaySeconds: 30
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /fluentd/etc/
        env:
        - name: SUMO_ENDPOINT_APISERVER_METRICS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-metrics-apiserver
        - name: SUMO_ENDPOINT_CONTROL_PLANE_METRICS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-control_plane_metrics_source
        - name: SUMO_ENDPOINT_CONTROLLER_METRICS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-metrics-kube-controller-manager
        - name: SUMO_ENDPOINT_DEFAULT_METRICS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-metrics
        - name: SUMO_ENDPOINT_KUBELET_METRICS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-metrics-kubelet
        - name: SUMO_ENDPOINT_NODE_METRICS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-metrics-node-exporter
        - name: SUMO_ENDPOINT_SCHEDULER_METRICS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-metrics-kube-scheduler
        - name: SUMO_ENDPOINT_STATE_METRICS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-metrics-kube-state
        - name: ADDITIONAL_PLUGINS
          value: ""

---
# Source: sumologic/templates/statefulset.yaml

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: collection-sumologic-fluentd-logs
  labels:
    app: collection-sumologic-fluentd-logs
    
spec:
  selector:
    matchLabels:
      app: collection-sumologic-fluentd-logs
  serviceName: collection-sumologic-fluentd-logs-headless
  podManagementPolicy: "Parallel"
  replicas: 3
  template:
    metadata:
      annotations:
      labels:
        app: collection-sumologic-fluentd-logs
        
    spec:
      serviceAccountName: collection-sumologic
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - collection-sumologic-fluentd-logs
                  - collection-sumologic-fluentd-metrics
                - key: app
                  operator: In
                  values:
                  - prometheus-operator-prometheus
              topologyKey: "kubernetes.io/hostname"
      volumes:
      - name: config-volume
        configMap:
          name: collection-sumologic-fluentd-logs
      securityContext:
        fsGroup: 999
        
      containers:
      - name: fluentd
        image: sumologic/kubernetes-fluentd:1.3.0
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 1000m
            memory: 1Gi
          requests:
            cpu: 500m
            memory: 768Mi
          
        ports:
        - name: fluent-bit
          containerPort: 24321
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
            port: 9880
          initialDelaySeconds: 300
          periodSeconds: 30
          timeoutSeconds: 3
        readinessProbe:
          httpGet:
            path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
            port: 9880
          initialDelaySeconds: 30
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /fluentd/etc/
        env:
        - name: SUMO_ENDPOINT_DEFAULT_LOGS_SOURCE
          valueFrom:
            secretKeyRef:
              name: sumologic
              key: endpoint-logs
        - name: ADDITIONAL_PLUGINS
          value: ""

---
# Source: sumologic/templates/hpa.yaml

---
# Source: sumologic/templates/metrics-hpa.yaml

---
# Source: sumologic/templates/otelcol-configmap.yaml

---
# Source: sumologic/templates/otelcol-deployment.yaml


---
# Source: sumologic/templates/otelcol-service.yaml


---
# Source: sumologic/templates/psp.yaml


---
# Source: sumologic/templates/scc.yaml


